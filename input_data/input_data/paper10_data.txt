
\noindent The unification of low-level perception and high-level reasoning is a long-standing problem in artificial intelligence, which has the potential to not only bring the areas of logic and learning closer together but also demonstrate how abstract concepts might emerge from sensory data. Precisely because deep learning methods dominate perception-based learning, including vision, speech, and linguistic grammar, there is fast-growing literature on how to integrate symbolic reasoning and deep learning. Efforts have ranged from providing a truth-theory to deep learning ~{serafini2016logic,garcez2012neural}, neural architectures that enable differential computation for symbolic constraints~{semanticLossXu2018,bovsnjak2017programming,rocktaschel2017end,santoro2017simple},
and embeddings for graph and relational data~{yang2014joint,lin2015learning,niepert2016learning,dumancic2018auto}. 
Approaches such as DeepProbLog~{manhaeve2018deepproblog}, on the other hand, treat deep learning as an external computation and integrate its predictions as an external predicate in a probabilistic logic programming framework. Broadly, efforts seem to fall into three camps: those focused on \textit{semantic characterizations} (i.e., define a logic whose formulas capture deep learning), \textit{constrained learning} (i.e., integrate symbolic constraints in deep learning), and \textit{hybrid methods} (allow neural computations and symbolic reasoning to co-exist separately, to enjoy the strengths of both worlds). 

In this paper, we identify another dimension to this inquiry: \textit{what do the hidden layers really capture, and how can we reason about that logically?} In particular, we consider autoencoders (AEs)~{goodfellow2016deep,kingma2013auto,rezende2014stochastic}. As a variant of neural networks, AE frameworks are perhaps the most popular for dimensionality reduction, but its inner workings are entirely opaque and mysterious. Basically, given an encoder \encoder, one first applies it to input data $x$ to obtain a feature layer (\fl) and then attempts to recover $x$ from \fl~using a decoder \decoder. Constraints on \fl~can lead to massive reductions on the dimensionality and identify salient features for applications such as anomaly detection. (See, for example,~{dumancic2018auto} that is a purely logical approach inspired by autoencoding principles.) Thus, we ask the question: \textit{can we inject a logical language onto the \fl~to perform Boolean reasoning over the \fl's variables?}

The exact choice of the language would depend on what we intend to do with the logic. A purely discrete representation such as propositional logic may not be very interesting or insightful about what the \fl~really captures, especially in cases where there may be probabilities assigned to image labels. In that regard, there has been an interesting development in knowledge representation over the last few years. As a special case of probabilistic logical models~{de2015probabilistic,getoor2007statistical} tractable probabilistic models have emerged as an extension to data structures such as binary decision diagrams (BDDs). In particular probabilistic sentential decision diagram (PSDDs)~{kisa2014probabilistic}, for example, are a complete and canonical representation of a probabilistic distribution defined over the models of a propositional theory. By imposing certain properties on the propositional representation, such as decomposability and determinism, probabilistic queries can be answered in polynomial time in the size of the data structure by way of model counting. Its parameters can be learned efficiently from data, which allows us to view the representation through a generative lens over a logical base. We discuss below how these features are put to use, but more generally, we see the work as a step in re-purposing deep learning in logical space to contributing to the emergence of high-level reasoning from a low-level system. Our contributions are orthogonal in many regards to the existing literature on neuro-symbolic systems and thus we imagine there would be space for looking at other kinds of integration with the existing literature.

Interestingly, we take note of multiple approaches for visually inspecting and interpreting NNs in the literature~{szegedy2013intriguing,yosinski2015understanding}, with a special focus on understanding convolutions of deep networks after training~{simonyan2013deep,zeiler2014visualizing}. While many of these methods yield various analysis of what happens in a given NN, including saliency maps~{simonyan2013deep}, they differ in thrust significantly from our contributions. For example, optimization methods are usually used to infer and decode regions of interest in a specific layer of a pre-trained network. In contrast to this, our logical approach uses a symbolic framework to make sense of the NN over a generative model. As such we do not infer the meaning of individual variables, although it is possible to visualize them, but rather compute conditional probabilities over those variables.

It should also be noted that recent circuit models attempt to  tackle vision problems too (e.g., ~{poon2011sum,gens2012discriminative,liang2019learning}), and so it is possible to realize the entire image classification pipeline using these tractable probabilistic models (however usually as a discriminative model). We think this is a very exciting development. What our work attempts to do, however, is to inspect state-of-the-art deep learning architectures (especially models like AEs that are very powerful for dimensionality reduction) via such symbolic generative models, in case such architectures are already in place, or are tackling problems still to be addressed using a pure circuit scheme. In that regard, as mentioned, our work is to be seen as attempting to re-purpose the latent space in a logical manner.

Our approach offers the following capabilities. We learn a PSDD over a discretized \fl, which yields a joint distribution over the individual variables, including image labels, of the \fl. This allows us, among other things, to visualize these individual variables by conditional sampling. In particular, this enables us to generate example images for a class to get a sense of what was learned. Moreover, the modular structure of the proposed model makes it possible to learn relations over multiple images at a time. Finally, because of the logical structure that we impose, noisy labels can also be handled.
We also discuss how we can evaluate the learned representation over well-known datasets, and also discuss both reconstructability (i.e., generative capabilities) and classification accuracy. 
%new paragraph (not sure if this is not too defensive already)


At the outset, from an engineering (as opposed to mathematical) viewpoint, 
it should be noted that, at this point, since circuit software packages have not enjoyed the same amount of maturity as deep learning packages,  the reported accuracy is not as competitive as state-of-the-art systems. Nonetheless, although 
% that due to setting a high bar on interpretability in our model, the accuracy reported is sub-par to other state-of-the-art systems when it comes to the classification task. However, while our recorded 
this reported accuracy is lower, the model has considerably more functionality, including the ability to sample prototypical images for each of the learned classes, ultimately aiding us in understanding what has been learned by the model (i.e., visually showing us what the model thinks a given class represents).







@label

The unification of low-level perception and high-level reasoning is a long-standing problem in artificial intelligence, which has the potential to not only bring the areas of logic and learning closer together but also demonstrate how abstract concepts might emerge from sensory data
@label
  Precisely because deep learning methods dominate perception-based learning, including vision, speech, and linguistic grammar, there is fast-growing literature on how to integrate symbolic reasoning and deep learning
@label
 Broadly, efforts seem to fall into three camps: those focused on defining a logic whose formulas capture deep learning, ones that integrate symbolic constraints in deep learning, and others that allow neural computations and symbolic reasoning to co-exist separately, to enjoy the strengths of both worlds
@label
 In this paper, we identify another dimension to this inquiry: what do the hidden layers really capture, and how can we reason about that logically? In particular, we consider autoencoders that are widely used for dimensionality reduction and inject a symbolic generative framework onto the feature layer
@label
 This allows us, among other things, to generate example images for a class to get a sense of what was learned
@label
 Moreover, the modular structure of the proposed model makes it possible to learn relations over multiple images at a time, as well as handle noisy labels
@label
 Our empirical evaluations show the promise of this inquiry
@label


