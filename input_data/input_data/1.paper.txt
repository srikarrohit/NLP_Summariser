Modern classical planners usually rely on heuristic forward search. Much research effort in recent years has
been devoted to the development of advanced domainindependent heuristic functions (e.g. (Hoffmann and Nebel
2001; Helmert 2006; Richter and Westphal 2010; Domshlak, Hoffmann, and Katz 2015)). In contrast, the search
algorithms at the core of many successful planners have
largely remained simple variations of best-first search, such
as greedy best-first search (Helmert 2006) or weighted A*
(Richter and Westphal 2010). In recent years, some work
has sought to combine best-first search with additional exploratory mechanisms, such as randomized order of node
expansion (Valenzano et al. 2014; Asai and Fukunaga 2017),
random walks and local search (Xie, Muller, and Holte ¨
2014) or novelty search (Lipovetzky and Geffner 2017). The
motivation behind these approaches is to enable the search to
escape local minima and plateaus of the heuristic function.
Introducing even a single auxiliary exploration mechanism necessarily comes with a number of nontrivial design
choices, such as when to switch between the main and auxiliary search approach. In addition, many of the exploration
mechanisms come with a number of parameters of their own,
such as the length and number of random walks to perform.
The values of the parameters are typically selected by human
experts. Furthermore, they stay fixed throughout the process
of solving the problem.
The work introduced in this paper aims at automating the
design of multi-technique search algorithms. To achieve it,
we first construct a parametrized search algorithm which
combines multiple search techniques in a flexible manner.
Depending on the values of the parameters, the search algorithm can take form of BFS, iterated local search or random
search, among others. Rather than choosing fixed values of
the parameters, we introduce a trainable model mapping the
current state of the search to an assignment over the parameters. We then train the model using the cross-entropy method
(CEM) to obtain search policies tailored to specific problem
distributions. To the best of our knowledge no existing work
has focused on a similar problem, except a very recent one
(Gomoluch, Alrajeh, and Russo 2019). That approach, however, is limited to selection from a discrete set of several
fixed search routines. We discuss it in more detail in the following section.
We implement our approach within the Fast Downward planning system (Helmert 2006) and evaluate it using five domains from the International Planning Competition (IPC). The learned search policies outperform baselines
built around the component techniques, as well as a fixed
hand-crafted combination all the techniques available to the
parametrized planner

@label
Heuristic forward search is currently the dominant paradigm
in classical planning. Forward search algorithms typically
rely on a single, relatively simple variation of best-first search
and remain fixed throughout the process of solving a planning problem.
@label
Existing work combining multiple search techniques usually aims at supporting best-first search with an
additional exploratory mechanism, triggered using a handcrafted criterion. 
@label
A notable exception is very recent work
which combines various search techniques using a trainable
policy. 
@label
It is, however, confined to a discrete action space comprising several fixed subroutines.
In this paper, we introduce a parametrized search algorithm
template which combines various search techniques within
a single routine. 
@label
The template’s parameter space defines an
infinite space of search algorithms, including, among others,
BFS, local and random search. 
@label
We further introduce a neural
architecture for designating the values of the search parameters given the state of the search. This enables expressing neural search policies that change the values of the parameters as
the search progresses.
@label
The policies can be learned automatically, with the objective of maximizing the planner’s performance on a given distribution of planning problems. We
consider a training setting based on a stochastic optimization
algorithm known as the cross-entropy method (CEM). 
@label
Experimental evaluation of our approach shows that it is capable of
finding effective distribution-specific search policies, outperforming the relevant baselines.