

Automated Theorem Provers (ATPs)~{DBLP:books/el/RobinsonV01} can be in principle
used to attempt the proof of any provable mathematical conjecture. 
% (except undecidable ones). 
The standard ATP approaches have
so far relied primarily on fast implementation of manually designed search procedures and heuristics.
However, using machine learning for guidance in the vast action spaces of the ATP calculi is a
natural choice that has been recently shown to significantly improve over the 
unguided systems~{KaliszykUMO18,JakubuvU19}.

The common procedure of a first-order ATP system -- saturation-style
or tableaux -- is the following. The ATP starts with a set of first
order axioms and a conjecture. The conjecture is negated and the
formulas are Skolemized and clausified.
% converted to a quantifier-free form using Skolemization and then clausified.
The objective is then to derive a contradiction from the set of clauses,
typically using some form of resolution and related inference rules.
The Skolemization as well as introduction of new definitions during
the clausification results in the introduction of many new function and predicate
symbols.

When guiding the proving process by statistical machine learning, 
the state of the prover and the
formulas, literals, and clauses, are typically encoded  to vectors of
real numbers. This has been so far mostly done with hand-crafted features resulting
in large sparse vectors~{DBLP:conf/ijcai/KaliszykUV15,hammers4qed,abs-1108-3446,UrbanVS11,KaliszykU15,JakubuvU17a}, possibly reducing their dimension afterwards~{ChvalovskyJ0U19}. 
Several experiments with neural
networks have been made recently, in particular based on 1D convolutions,
RNNs {GollerK96}, TreeRNNs~{ChvalovskyJ0U19}, and GraphNNs {DuvenaudMABHAA15}. Most of the approaches, however, cannot
capture well the idea of a variable occurring multiple times in the
formula and to abstract from the names of the variables.
%while its name actually being irrelevant. 
These issues were first 
addressed in FormulaNet~{DBLP:conf/nips/WangTWD17} but even that
architecture relies on knowing the names of function and predicate symbols. This makes it 
unsuitable for handling the large number of problem-specific function and predicate
symbols introduced during the clausification.\footnote{The ratio of such symbols in real-world clausal datasets is around 40\%, see Section~\ref{symbol-experiments}.}
 %, when introducing definitions, 
The same holds for large datasets of ATP problems
where symbol names are not used consistently, such as the TPTP library~{Sutcliffe10}.


In this paper, we make further steps towards the abstraction of
mathematical clauses, formulas and proof states. We present a network that is invariant not only
under renaming of variables, but also under renaming of arbitrary function and predicate
symbols. It is also invariant under replacement of the symbols by their negated versions.
This is achieved by a novel conversion of the input formulas into a hypergraph, followed by a 
particularly designed 
% form of 
graph neural network (GNN) capable of maintaining the invariance under negation. 
We experimentally demonstrate in three case studies
that the network works well on data 
coming from automated theorem proving tasks.


The paper is structured as follows. We first formally describe our network architecture in
Section~\ref{architecture}, and discuss its invariance properties
in Section~\ref{invariance}. We describe an experiment using the
network for guiding \lc in Section~\ref{leancop-example}, and
two experiments done on a fixed dataset in
Section~\ref{deepmath-experiments}. Section~\ref{results} contains the
results of these three experiments.

@label

Automated reasoning and theorem proving have recently become major challenges
for machine learning
@label
 In other domains, representations that are able
to abstract over unimportant transformations, such as abstraction over
translations and rotations in vision, are becoming more common
@label

Standard methods of embedding mathematical formulas for learning theorem proving
are however yet unable to handle many important transformations
@label
 In particular, embedding previously unseen
labels, that often arise in definitional encodings and in Skolemization, has been very weak so far
@label
 Similar problems appear when transferring knowledge between known symbols
@label


We propose a novel encoding of formulas that extends existing graph neural
network models
@label
 This encoding represents symbols  only by nodes
in the graph, without giving the network any knowledge of the original
labels
@label
 We provide additional links between such nodes that allow the
network to recover the meaning and therefore correctly embed such nodes
irrespective of the given labels
@label
 We test the proposed encoding in an
automated theorem prover
% reasoning system 
based on the tableaux connection calculus,
and show that it improves on the best characterizations used
so far
@label
  The encoding is further evaluated on the premise selection task and a newly introduced symbol guessing task,
and shown to correctly predict 65\% of the symbol names
@label

% show that the network can 
%One of the main questions in the rapidly arising field of 
%machine learning in automated theorem proving is how to embed a
%mathematical formula, that is, how to assign a vector of real numbers
%to it
@label
 The standard methods often cannot handle well labels in the
%formulas unseen before, which can easily appear
%from skolemization or definitional encoding, and also their
%capabilities of transferring knowledge between known symbol are
%mostly limited
@label
 We propose an encoding based on
%graph neural networks, where symbols are represented
%only by nodes in the graph, without giving the network
%any knowledge of the original labels
@label

%We tested the network on a tableaux connection prover
%on Mizar40 dataset, with significantly better results
%in the first iteration
@label


