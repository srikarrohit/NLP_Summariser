

A system can defend itself against adversarial machine learners by randomly switching the feature mask it is using, by detecting unusual distributions of system inputs and reacting defensively to them, or by using other methods. Authorship attribution can be discovered using adversarial machine learning to learn the patterns between the character unigrams and authors. An adversarial machine learner can learn the mappings between system inputs and outputs and learn the Neural Network (NN) or Support Vector Machine (SVM). 

A defensive system can detect the system inputs from an adversarial machine learner by comparing the normal probability distribution of the inputs versus the current probability distribution
% and conditional probabilities
% of the feature masks
 of 
% and 
the inputs and detecting a difference between them. If a difference is detected between the normal inputs to the system and the current inputs to the system based on the probability, then it can be assumed that the current inputs may come from an adversarial machine learner which 
has learned 
%is attempting to learn 
the input-output mapping to model the NN. 
Also, if it is discovered that the inputs are along a certain decision boundary that the system has, then the defending system can become wary of the inputs and suspect them of being from an adversarial machine learning attacker system. 
Since the adversarial machine learner can develop a NN or SVM to model the system, given the input-output mappings, the adversarial machine learning system can then use knowledge of the decision boundaries in the model to inform creating inputs that will ``trick" the system that the adversarial machine learning system is attacking. 

Generative Adversarial Networks (GANs) are a type of adversarial artificial intelligence. The discriminator operates based on a neural network that has decision boundaries and makes decisions based on inputs. The generative network, the adversary, learns the distribution in order to ``fake out" the discriminator. Generative Adversarial Networks learn the distribution, create synthetic data, and can then trick the discriminator system and its decision boundaries. Examples of GANs tricking systems are ``deep fakes", where an attacker creates an input to trick the decision boundaries of the original system. The analysis of variance, such as an F-test, ANOVA test, or Student t-Test, can be done. Then, the P-value and T-values can be analyzed to make sure that the guessed value is correct. 

The seven phases in the Machine Learning Model Kill Chain are the: 
\begin{enumerate}
\item Reconnaissance (Recon) Phase
\item Weaponization Phase
\item Delivery Phase
\item Exploitation
\item Installation
\item Command and Control
\item Action
\end{enumerate} 

% The three phases in the Machine Learning Model Kill Chain are the Reconnaissance (Recon) Phase, the Weaponization Phase, and the Delivery Phase. 
During the Reconnaissance (Recon) Phase, the Machine Learning (ML) models are determined. The ML models are used to protect the defending system from the type of attacks which are to be launched by the attacking system. Then, during the Weaponization Phase, the results of probes are used in an effort to develop an attack on the defending system by learning the defending system's decision boundary. During the Decision Phase, the defending system's decision boundary is attacked by the attacking adversarial machine learning system. During the Exploitation Phase, the adversarial machine learning system gathers deeper information about the defending system's underlying model. The attacking system learns how the defending system's model will be tuned, how fast new rules can be formed, and how threats are ranked. During the Installation Phase, new rules, or features, that will allow future attacks to happen, are set up. During the Command and Control Phase, a hidden command and control channel is set up to allow for expansion of the attack. Finally, during the Action Phase, the attackers act on their main objective{Nyugen 2017}. 

\begin{math}30\end{math} feature masks are developed, and the system switches between the \begin{math}30\end{math} feature masks to get roughly the same results. If an attacker knows the coding language that the system is written in (Python, in this case), the random generator, and the seed, then the attacker can figure out the \begin{math}30\end{math} feature masks. 

If the number of features is reduced, the accuracy will increase. Feature reduction is done \begin{math}30\end{math} times to develop the \begin{math}30\end{math} feature masks. The cycle of attack by the Adversarial Author (Attacking System) and defense by the Authorship Attribution System (AAS) (Defending System) is shown in Figure~\ref{figure_AdvAuthor_AAS}. 

 \begin{figure}%[H]
 \begin{center}
 \setlength{\unitlength}{0.012500in}
 \includegraphics[width=40mm, scale=1.5]{AdvAuthor_AAS.png}
 \end{center}
 \caption{Cycle of Adversarial Author (Attacking System) vs. Authorship Attribution System (Defending System)}
 \label{figure_AdvAuthor_AAS}
 \end{figure}

Acceptable ranges of drops in accuracy by the AAS whilst under attack are less than \begin{math}11\end{math} percent. The baseline accuracy of the AAS is between \begin{math}60\end{math} to \begin{math}80\end{math} percent, depending on which algorithm is being used. Therefore, an \begin{math}11\end{math} percent drop in accuracy on \begin{math}60\end{math} to \begin{math}80\end{math} percent accuracy would result in \begin{math}49\end{math} to \begin{math}69\end{math} percent accuracy. %would result in \begin{math}53.4\end{math} to \begin{math}71.2\end{math} percent accuracy. %would result in \begin{math}49\end{math} to \begin{math}69\end{math} percent accuracy. %


%
%
%%%type of Authorship attribution uses machine learning to recognize script samples as adversarial or friendly. %Artificial Intelligence and Machine Learning Security are applications for adversarial machine learning algorithms, while Artificial Intelligence in Cybersecurity is another application. 
%%Generative Adversarial Networks (GANs) are a type of adversarial artificial intelligence. The discriminator operates based on a neural network that has decision boundaries and makes decisions based on inputs. The generative network, the adversary, learns the distribution in order to "fake out" the discriminator. Generative Adversarial Networks learn the distribution, create synthetic data, and can then trick the discriminator system and its decision boundaries. Examples of GANs tricking systems are "deep fakes", where an attacker creates an input to trick the decision boundaries of the original system. The analysis of variance, such as an F-test, ANOVA test, or Student t-Test, can be done. Then, the P-value and T-values can be analyzed to make sure that the guessed value is correct. 
%
%%There are three phases in the Machine Learning Model Kill Chain: Recon Phase, Weaponization, and Delivery. The Recon Phase consists of determining which ML models to use to protect the type of attacks which are desired to be launched.  %The information that is known about the overall system is ... 
%%%The models are (not) open-source. %The kinds of Social Engineering available are ... 
%%The Weaponization Phase consists of using the results of probes in an effort to develop an attack on the system by learning the system's decision boundary. The Decision Phase consists of attacking the decision boundary. 
%%\begin{math}30\end{math} feature masks are developed, and the system switches between the \begin{math}30\end{math} feature masks to get roughly the same results. If an attacker knows the coding language that the system is written in (Python, in this case), the. random generator, and the seed, then the attacker can figure out the \begin{math}30\end{math} feature masks. 
%
Term Frequency-Inverse Document Frequency (TFIDF) is a numerical statistic that is intended to reflect how important a letter is in a character unigram. TFIDF is used as a weighting factor in searches, and the value increases proportionally to the number of times a letter appears in the character unigram and is offset by the number of character unigrams in the set that contain the letter. 

When the simple (unweighted) Euclidean distance is used, normalization provides an equal weighting to all features, whereas some features would have more importance than others when normalization is not used. Normalization of the training data is done by subtracting the mean and dividing the mean and standard deviation of the training data. 

Standardization provides a way to scale the test data with the mean and standard deviation of the training data. 

It is important for a defensive system to establish a sense of normalcy for inputs. A measure of normalcy is created by the algorithm knowing the distribution of normal inputs. If the distribution of the inputs has changed, a notification is made by the defensive algorithm to alert the system that there is something that has changed and the machine learning algorithm is receiving abnormal inputs. This violates the stationarity assumption, since the inputs are not normal and expected inputs. The inputs with abnormal distributions can trick the decision boundary and create bad results from the machine learning algorithm. 

In machine learning, Naïve Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. They are among the simplest Bayesian network models.

Naïve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-Likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.

In the statistics and computer science literature, Naïve Bayes models are known under a variety of names, including simple Bayes and independent Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but Naïve Bayes is not (necessarily) a Bayesian method. 

The discussion so far has derived the independent feature model, that is, the Naïve Bayes probability model. The Naïve Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable; this is known as the Maximum A-Posteriori (MAP) decision rule. 

When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution. For example, suppose the training data contains a continuous attribute, \begin{math}x\end{math}. The data is first segmented by the class, and then compute the mean and variance of \begin{math}x\end{math} in each class.

The algorithms are sorted into equivalence classes based on whether there are statistically significant differences between the algorithms. The ANOVA and Student t-Tests are used to determine statistical significance and to compare the algorithms. 


%%There are \begin{math}\end{math} equivalence classes. The LSVM with Normalization, Standardization, and TFIDF and the RBFNN with Normalization are in the same equivalence class, while the FFNN with Standardization and the RBFNN with Standardization and TFIDF are in a different equivalence class. 
%
%
%%GEFeS is done for the RBFSVM, LSVM, and FFNN. The multi-objective optimization problem is not 

@label

An Adversarial System to attack and an Authorship Attribution System (AAS) to defend itself against the attacks are analyzed
@label
 
Defending a system against attacks from an adversarial machine learner can be done by randomly switching between models for the system, by detecting and reacting to changes in the distribution of normal inputs, or by using other methods
@label
 Adversarial machine learning is used to identify a system that is being used to map system inputs to outputs
@label
 
%A GEFeS is developed to evolve \begin{math}30\end{math} feature masks for the three baseline machine learning techniques
@label
 A unigram feature extractor is developed
@label
 
%%The improvement of using feature selection over the baselines is 
@label
%explained
@label
 
%Over the \begin{math}30\end{math} runs for each GEFeS, the average and standard deviation of the features being presented (this is known as the feature consistency) is 
@label
 
%The most consistent features are 
@label
 
Three types of machine learners are using for the model that is being attacked
@label
 The machine learners that are used to model the system being attacked are a Radial Basis Function Support Vector Machine, a Linear Support Vector Machine, and a Feedforward Neural Network
@label
 The feature masks are evolved using accuracy as the fitness measure
@label
 The system defends itself against adversarial machine learning attacks by identifying inputs that do not match the probability distribution of normal inputs
@label
 The system also defends itself against adversarial attacks by randomly switching between the feature masks being used to map system inputs to outputs
@label
 


