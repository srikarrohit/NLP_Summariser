  Modern classical planners usually rely on heuristic forward search. Much research effort in rec nt years has been devoted to the development of advanced domain-independent heuristic functions (e.g. Hoffmann2001 Helmert2006 Richter2010 Domshlak2015). In contrast  the search algorithms at the core of many successfu  planners have largely remained simple variations of best-first search  such as greedy best-fir t search Helmert2006 or weighted A* Richter2010. In recent years  some work has sought to combine best-first search with additional exploratory mechanisms  such as randomized order of node expansion Valenzano2014 Asai2017  random walks and local search Xie2014 or novelty search Lipovetzky2017. The motivation behind these approaches is to enable the search to escape local minima and plateaus of the heuristic function.  Introducing even a single auxiliary exploration mechanism necessarily comes with a number of nontrivial design choices  such as when to switch between the main and auxiliary search approach. In addition  many of the exploration mechanisms come with a number of parameters of their own  such as the length and number of random walks to perform. The values of the parameters are typically selected by human experts. Furthermore  they stay fixed through ut the process of solving the problem.  The work introduced in this paper aims at automating the design of multi-technique search algorithms. To achieve it  we first construct a parametrized search algorithm which combines multiple search techniques in a flexible manner. Depending on the values of the parameters  the search algorithm can take form of BFS  iterated local search or random search  among others. Rather than choosing fixed values of the parameters  we introduce a tr inable model mapping the current state of the search to an assignment over the parameters. We then train the model using the cross-entropy method (CEM) to obtain search policies tailored to specific problem distributions. To the best of our knowledge no existing work has focused on a similar problem  except a very recent one Gomoluch2019. That approach  however  is limited to selection from a discrete set of several fixed search routines. We discuss it in more detail in the following section.  We implement our approach within the Fast Downward planning system Helmert2006 and evaluate it using five domains from the Internati nal Planning Competition (IPC). The learned search policies outperform baselines built around the component techniques  as well as a fixed hand-crafted combination all the techniques available to the parametrized planner.      We have introduced a parametrized search algorithm flexibly combining multiple search techniques in a way determined by the values of its parameters. We further constructed a simple neural policy model for designating the values of the algorithm's parameters given a high-level representation of the current state of the search. Using the cross-entropy method we have trained the model on problems from five different planning domains. The empirical evaluation shows that the learner is able to discover effective distribution-specific search policies.  Besides the strengths of the approach  the empir cal evaluation has also shown some of its limitations. While the current approach has the capacity to ignore the value of a heuristic it finds m sleading  it can neither replace it nor stop computing it for the all encountered states  even if the value is not going to be used. One possible solution would be to make to choice of heuristic subject to the learning process  for example by allowing the algorithm to work with multiple open lists. It is also be possible to extend the algorithm with other state-of-the-art techniques from classical planning  such as preferred operators or novelty-based search Lipovetzky2017.  Another direction of future work is the representation of the planner's state. Capturing the relevant information in a domain-independent manner is a huge challenge and the key to improve the expressiveness of the learned policies.  aaaireferences    

@label
Heuristic forward search is currently the dominant paradigm in classical planning
@label
Forward search algorithms typically rely on a single  relatively simple variation of best-first search and remain fixed throughout the process of solving a planning problem
@label
Existing work combining multiple search techniques usually aims at supporting best-first search with an additional exploratory mechanism  triggered using a handcrafted criterion
@label
A notable exception is very recent work which combines various search techniques using a trainable policy
@label
It is  however  confined to a discrete action space comprising several fixed subroutines
@label
In this paper  we introduce a parametrized search algorithm template which combines various search techniques within a single routine
@label
The template's parameter space defines an infinite space of search algorithms  including  among others  BFS  local and random search
@label
We further introduce a neural architecture for designating the values of the search parameters given the state of the search
@label
This enables expressing neural search policies that change the values of the parameters as the search progresses
@label
The policies can be learned automatically  with the objective of maximizing the planner's performance on a given distribution of planning problems
@label
We consider a training setting based on a stochastic optimization algorithm known as the cross-entropy method(CEM)
@label
Experimental evaluation of our approach shows that it is capable of finding effective distribution-specific search policies  outperforming the relevant baselines



