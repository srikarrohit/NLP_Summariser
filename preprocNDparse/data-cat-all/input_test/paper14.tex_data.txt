  The deployment of automated and autonomous vehicles presents us with transformational opportunities for road transport. To date, the number of companies workin  on this technology is substantive, and growing cbsreport.  Opportunities reach beyond single-vehi le automation: by enabling groups of vehicles to jointly agree on maneuvers and navigation strategies, real-time coordination promises to improve ov ral  traffic throughput, road capacity, and passenger safety dressler:2014,ferreira2010self. However, drivi g in multivehicle and multi-lane settings still remains a challenging research problem, due to unpredictable vehicle interactions (e.g., non-cooperative cars, unreliable communication), hard workspace limitations (e.g., lane topographies), and constrained platform dynamics (e.g., steering kinematics, driver comfort).    Le rningbased methods, such as deep reinforcement learning, have proven effective at designing robot control policies for an increasing number of tasks in single-vehicle systems, for applications such as navigation khan2019learning, flight molchanovSimto2019, and locomotion tan2018sim.  Leveraging such methods for learning autonomous driving policies is emerging as a particularly promising approach pan2017virtual, shalev2016safe, kuderer2015learning. Yet, the process of safely learning autonomous driving involves unique challenges, since the decision models often used in robotics do not lend themselves naturally to the multi-vehicle domain, due to the unpredictable behaviour of other agents. The unapologetic nature of the trial-and-error process in reinforcement learning compounds the difficulty of ensuring functional safety.   These adversities call for learning that first takes place in simulation, before transferring to the real world miglino1995evolving, shah2018airsim.  This transfer, often referred to as sim2real, is challenging due to discrepancies between conditions in simulation and the real world (such as vehicle dynamics and sensor data) peng2018sim, james2019sim, chebotar2019closing. Despite substantial advances in this field, the problem of executing immature policies directly  n an autonomous vehicle still raises considerable safety concerns. These concerns are exacerbated when multiple autonomous vehicle  share the same workspace, risking collisions and un-reparable damage  Simultaneously, the act of colliding---or nearly-co lidingis essential to the learning process, enabling future policy roll-outs to incorporate these critical experiences. How are w  to provide safe multi-vehicle learning experiences, without forgoing the realism of high-fidelity training data? There is a dearth of work that addresses this challenge.       Our goal in this paper is t  develop a safe and efficient framework that allows us to learn driving policies for autonomous  ehicles operating in a shared workspace, where collision-freeness cann t be guaranteed. Towards this end, we learn an end-to-end policy for vehicle navigation on a multi-lane tr ck that is shared with other moving vehicles and static obstacles. The learning is based on a model-free method embedded in a distributed training mechanism that we tailor for mixed-reality compatibility. Key to our learning procedure is a sim2real approach that uses real-world online policy adaptation in a mixed-reality setup, where obstacles (vehicles and objects) exist in the virtual domain. This allows us to perform safe learning by simulating (and learning from) collisions between the learning agent(s) and other objects in virtual reality.  We apply our framework to a multi-vehicle setup consisting  f one real vehicle, and se eral simulated vehicles (as shown in Figure ||SYMBOLTOKEN|| Experiments show that a significant performance improvement can be obtained after just a few runs in mixed-reality, reducing the number of collisions and increasing reward collection. To the best of our knowledge, this is the first demonstration of mixed-reality reinforcement learning for multi-vehicle applications.       
