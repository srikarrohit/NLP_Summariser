paper9.tex_data.txt	Evaluating Com onsense in Pre-trained Language Models	{"Introduction": ["\r\nConte tualized representations trained over large-scale text data have given remarkable improvements to a wide range of NLP tasks, including natural language inference Bowman2015ALA, question answering rajpurkar-etal-2018-know and reading comprehension Lai2017RACELR. Giving new state-of-the-art results that approach or surpass human performance on several benchmark datasets, it is an interesti g question what types of knowledge are learned in pre-trained contextualized representations in order to better understand how they benefit the NLP problems above. There has been work investigating the nature of syntactic liu-etal-2019-linguistic, semantic liu-etal-2019-linguistic and word sense kim-etal-2019-probing knowledge contained in such contextualized representations, in particular BERT devlin-etal-2019-bert, showing that suc  knowledge can be effectively learned via language model (LM) pre-training over large scale data.\r\n\r\nCommonsense knowledge spans \"a huge portion of human experience, encompassing\r\nknowledge about the spatial, physical, social, temporal, and\r\npsychological aspects of typical everyday life. \" Liu2004ConceptNetA. Intuitively, such knowledge is at least as useful as semantic and syntactic knowledge in natural language inference, reading comprehension and coreference resolution. For example, the word \"it\" in the sentence \"the dog cannot cross the street because it is too X\" can refer to three different entities when the word \"X\" is \"timid\", \"wide\" and \"dark\", respectively, and resolving such ambiguity can require that a system has relevant commonsense knowledge beyond the sentence level. However, relatively little work has been conducted on systematically evaluating the nature of commonsense knowledge learned in contextualized representations.\r\n\r\n\r\nExample of reframed test instances corresponding to each of our test task. The key word is bolded in token-level tasks. \u2227, , \u2190 and \u2192 are used for showing the logic flows and replaced by natural language in actual test data.\r\n\r\n\r\nWe fill this gap by evaluating five state-of-the-art contextualized embedding models on seven commonsense benchmarks. The models include off-the-shelf embeddings[https://github.com/huggingface/transformers] from GPT rad-2018, GPT2 radford2019language, BERT devlin-etal-2019-bert, XLNet zhilin-19 and RoBERTa liu2019roberta, and the benchmarks include Conjunction Acceptability, Sense Making wang-etal-2019-make, Winograd Schema Challenge Levesque:2012:WSC:3031843.3031909, SWAG zellers-etal-2018-swag, HellaSwag zellers-etal-2019-hellaswag, Sense Making with Reasoning wang-etal-2019-make, and Argument Reasoning Comprehension habernal-etal-2018-argument. We evaluate commonsense knowledge contained in the above models by unifying the form of all the datasets and comparing LM perplexities on positive and negative samples (i.e., sentences that make sense and those that do not make sense, respectively). Commonsense contained in our data covers a wide range of subjects, from physical world knowledge to social conventions, from scientific domains to daily life scenes. We further categorize them by the difficulty level, namely the nu ber of inference steps nece sary in making sense.\r\n\r\nWe reframe the datasets in order to conduct both word- and sentence-level testing. For word-level testing, negative samples are drawn by replacing words from positive samples. We are concerned about nouns, verbs, adjectives, adverbs, pronouns and conjunctions, which reflect different aspects of commonsense. For example, while verbs such as \"buy, throw, sell ...\" are relatively more a sociated with event knowledge, conjunctions such as \"because, but, so ...\" are more associated with logical reasoning. For senten elevel testing, negative examples are drawn by replacing a full subsentences (such as a clause) with irrelevant o  conflicting contents. Sentence-level tests concern more about commonsense inference.\r\n\r\nFrom the results  e have four salient observations. First, the pre-trained models give  onsistently better performances than random baselines, which demonstrates that language model pre-training is usef l for learning commonsense knowledge. Second, models based on bi-directional contexts such as BERT, XLNet and RoBERTa are stronger in learning commonsense knowledge compared to those based on uni-directional contexts, such as GPT and GPT2. Third, more commonsense knowledge can be learned from larger training sets, which conforms well to the intuition. Fourth, the models have a certain degree of  ommonsense reasoning ability. However, as the number of necessary inference steps increase, the model performances drop, which shows that commonsense is still a big challenge that is not completely solved by pre-trained contextualized language models (LMs).\r\n\r\nFinally, we further test the robustness of the five models by making dual test samples. Here a dual test sample is built by adding, deleting and replacing words in a test sample, or swapping two words in the sample, thereby resulting in a closely related test case. In theory, a model equipped with relevant commonsense should give consistent predictions on a pair of dual test cases. However, we find that none of the models are able to reach such consistency  Instead, the models are confused by the modification, tending to give the same predictions over a pair of dual samples despite they may have different gold labels. This further reveals that commonsense contained in the pre-trained models may remain in a surface level, without deep semantic comprehension. We publicly release our datasets, named commonsense ability tests (CATs), and the test script at GitHub. [https://github.com/XuhuiZhou/CATS]\r\n\r\n\r\n", {}], "Tasks for Evaluating Commonsense": ["\r\nCommonsense ability can be broadly divided to two categories. First, a model with commonsense ability should have basic knowledge about the world, for example, water always goes down. Second, it should have the ability to reason over commonsense knowledge, such as water always goes down because there is gravity on the earth and if you are injured, you should go to the hospital. To comprehensively test different mod ls' commonsense ability, we synthesize six challenging tasks by taking positive and negative samples from existing benchmarks, and further introduce a new task called Conjunction Acceptability (CA).\r\n\r\nWe reframe all the tasks into sentence-scoring tasks by substitution or concatenation. For example, we create positive and negative samples by replacing a pronoun in the sentence of a WSC question with the candidates to obtain a test instance as Table ||SYMBOLTOKEN|| A model is asked to score the sentences and we pick the sentence with the highest score as its prediction in a test instance. Below we introduce the data sources and reframed tasks in detail (the correct answer is bolded).\r\n\r\n\r\n\r\n\r\n\r\n", {"Sense Making (SM)": ["\r\nIntroduced by wang- tal2019-make (2019), this task tests whether a model can differentiate sense-making and non-sense-making statements. Given a pair of statements (i.e a test instance), it requires the model to choose the more sensible statement. One example is: I work 8 hours a day / I work 25 hours a day. This task conforms to our evaluation schema without a change. More examples are shown in the SM section of Table ||SYMBOLTOKEN|| The statements typically differ only in one key word which covers nouns, verbs, adjectives, and adverbs.\r\n\r\n\r\n", {}], "Winograd Schema Challenge (WSC)": ["\r\nThe Winograd Schema Challenge (WSC) dataset Levesque:2012:WSC:3031843.3031909 consists 273 instances of the pronoun resolution problem. Each instance contains a sentence with a pronoun referring to one of nouns; the original question is to pick the correct noun. For our task, we transform the test  s shown in Table ||SYMBOLTOKEN|| More examples are shown in the WSC section  f Table ||SYMBOLTOKEN|| WSC is recognized as one of the most difficult commonsense datasets.\r\n\r\n\r\n", {}], "Conjunction Acceptability (CA)": [" \r\n As stated by lobue-yates-2011-types (2011), logic-based commonsense knowledge is an important part of world knowledge in addition to content-based knowledge. We aim to probe a model's ability to understand the logic relations in the language by extracting 18  positive samples from the WSC dataset and replacing the conjunction manually with another conjunction to obtain a negative sample. We pair the positive and negative samples to obtain a test instance. For example, The lawyer asked the witness a question, and the witness was reluctant to answer it / The lawyer asked the witness a question, but the witness was reluctant to an wer it. More examples are shown in the CA section of Table ||SYMBOLTOKEN|| This task using \"because\", \"before\", \"when\", \"but\", \"and\" to correspond to the Cause and Effect, Preconditions, Simultaneous Conditions, Contradiction, and Addition logic relations, respectively. It is complementary to the other token-level tasks which focus more on content-based knowledge.\r\n\r\n\r\n", {}], "SWAG": ["\r\nSWAG zellers-etal-2018-swag is a dataset with multiple choices questions about grounded situations. It questions models' understanding towards the relationship between two physical scenes. With the help of adversarial filtering (AF), zellers-etal-2018-swag created a sufficiently large amount of questions automatically. For example, given On stage, a woman takes a seat at the piano. She, the question is to choose the following candidates: A. sits on a bench as her sister plays with the doll B. smiles with someone as the music plays C.is in the crowd, watching the dancers D. nervously sets her fingers on the keys. We obtain a positive or negative sample by concatenating the context and a candidate together (e.g On stage, a woman takes a seat at the piano. She nervously sets her fingers on the keys). There are one positive sample and three negative samples in a SWAG test instance. More examples are shown in the SWAG section of Table ||SYMBOLTOKEN|| By forcing the model to predict the next action, it requires inductive reasoning and temporal reasoning.\r\n\r\n\r\n", {}], "HellaSwag": ["\r\nHellaSwag zellers-etal-2019-hellaswag is an argumented version of SWAG with the same data format as SWAG, more inference steps and higher data quality. While HellaSwag also includes the dataset from WikiHow, we choose only the instances coming from ActivityNet to make the results comparable to the original SWAG dataset.\r\n\r\n\r\n", {}], "Sense Making with Reasoning (SMR)": ["\r\nSense Making with Reasoning focuses on identifying the reason behind a statement wang-etal-2019-make against commonsense. A model needs to understand that a specific statement (e.g can is usually made of gold) is against commonsense and to make a choice for the reason behind from three candidates (e.g gold is too bright to make cans, gold is too soft to make cans and gold is too expensive to make cans). We make a posit ve or negative sample by concatenating the statement and candidate reason together. For each test instance in SMR, there is a positive sample and two negative samples. More examples are shown in the SMR section of Table ||SYMBOLTOKEN|| This task is intuitively difficult since it requires a model to have deeper knowledge of with higher-l vel inference, which belongs to abductive reasoning.\r\n\r\n\r\n", {}], "Argument Reasoning Comprehension Task (ARCT)": ["\r\nSimilar to SMR, habernal-etal-2018-argument (2018) propose the ARCT dataset to test a model's abductive reasoning ability. Its domain lies in social topics such as search engine and LGBT rights, which is different from the daily-routine scenarios. For example, given a reason R: I find the idea that it is a sin to be born or live a life at all to be preposterous and a claim C: Christians have created a harmful atmosphere for gays, this task is to pick the correct warrant W from two candidates: A. being gay isn't considered a sin B. being gay is considered a sin, where R \u2227 W \u2192 C. We make a positive or negative sample by concatenating the reason, candidate warrant and claim together (e.g I find the idea that it is a sin to be born or live a life at all to be preposterous and since being gay is considered a sin, Christians have created a harmful atmosphere for gays). A test instance in ARCT contains a pair of positive and negative samples. More examples are shown in the ARCT section of Table ||SYMBOLTOKEN|| We further break this task into two variants, where ARCT1 represents the original dataset, ARCT2 represents an argumented dataset by adding negation to original instances to alleviate the statistical cues in the dataset niven-kao-2019-probing.\r\n\r\nWe integrated the above test sets into a commonsense ability tes  (CATs) benchmark, released for future research.\r\n\r\n\r\n", {}]}], "Pre-trained Models": ["\r\nWe take six contextualized representation models that give the state-of-the-art pe formances on NLP benchmarks such as GLUE wang-etal-2018-glue and SQuAD rajpurkar-etal-2018-know. Off-the-shelf models are taken. Below we give the detailed settings.\r\n\r\nGPTrad-2018 is a uni-directional transformer LM trained on 800M tokens of BookCorpus Zhu2015AligningBA. Given a text sequence x ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ..., ||SYMBOLTOKEN|| GPT works in a way similar to conventional auto-regressive (AR) ||SYMBOLTOKEN|| ||MATHEQUATION|| ||SYMBOLTOKEN|| ||MATHEQUATION|| ..., ||SYMBOLTOKEN|| The model has dimension of hidden states H ||SYMBOLTOKEN|| 768, attentio  head numbers ||SYMBOLTOKEN|| number of layers ||SYMBOLTOKEN|| and total parameter size ||SYMBOLTOKEN|| works similarly as GPT with a few modifications on the hyperparameters. In particular, GPT2 optimizes the layer normalization, expands the vocabulary size to 50,257, increases the context size from 512 to 1024 tokens, and optimizes with a larger batchsize of 512. In addition, GPT2 is pre-trained on WebText, which was created from scraping web pages. The dataset roughly contains 8 million documents (40 GB). We study GPT2-base and GPT2-medium, with model size ||SYMBOLTOKEN|| A=12, ||SYMBOLTOKEN|| P=117M and ||SYMBOLTOKEN|| A=16, ||SYMBOLTOKEN|| P=345M, respectively, where the definitions of H, L and A are the same as for GPT.\r\n\r\nBERTdevlin-etal-2019-bert jointly trains on a masked language modeling task and a next senten e prediction task (NSP). The model is trained on the BookCorpus and English Wikipedia, a total of approximately 3300M tokens. BERT is designed with the following ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| m_t ||S MBOLTOKEN|| ,\r\n\r\nwhere x\u0303 is a corrupted version of text sequence x, and x\u0305 is  masked tokens. ||SYMBOLTOKEN|| if token ||SYMBOLTOKEN|| belongs to x\u0305.\r\n\r\nHere we consider B RTbase and BERT-large, with ||SYMBOLTOKEN|| A=12, ||SYMBOLTOKEN|| P=117M and ||SYMBOLTOKEN|| A=16, ||SYMBOLTOKEN|| P=340M, respectively, where the definitions of H, L a d A are the same as for GPT.\r\n\r\nXLNetzhilin-19 is trained with a permutation-based language modeling objective to capture bidirectional contexts while retain the benefits of AR models. Specifically, t ey let ||SYMBOLTOKEN|| be the set of all possible permutations of the length-T sequence x ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ..., ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ] ,\r\n\r\nwhere ||SYMBOLTOKEN|| and ||SYMB LTOKEN|| are the t-th element and the first  1 elements of a permutation ||SYMBOLTOKEN|| respectively. In this way, XLNet ensures that any specific token ||SYMBOLTOKEN|| in x has seen all the tokens before or after it.\r\n\r\nWe consider XLNet-base and XLNet-large, whose model sizes are ||SY BOLTOKEN|| A=12, ||SYMBOLT KEN|| P=117M and ||SYMBOLTOKEN|| A=16, ||SYMBOLTOKEN|| P=340M, respectively, where the definitions of H, L and A are the same as for GPT. Note that XLNet-base is trained with the same data as BERT, while XLNet-large is trained with a larger dataset that consists of 32.98B subword pieces coming from Wiki, BookCorpus, Giga5, ClueWeb, and Common Crawl.\r\n\r\nRoBERTaliu2019roberta has the same architecture as BERT but is trained with dynamic maskin , FULLSENTENCES without NSP loss, a larger batch-size and a larger vocabulary size. Given the optimized design choice, one key difference of RoBERTa with other models is its large training dataset, which consists of BookCorpus, CC-NEWS, OpenWebText, and STORIES. With a total 160GB text, RoBERTa has access to more potential knowledge than the other models.\r\n\r\n\r\n", {}], "Experimental Design": ["\r\nThe CAT datasets are applicable to any model that has a method to score a sentence. They fit with the pre-trained models above, which are by nature language models. We derive the score of a sentence below  ith unidirectional-context LMs and bi-directional-context LMs, respect vely.\r\n\r\n\r\nAccuracy for each pre-trained contextualizer on each test set. The rightmost column shows the average of accuracy scor  of each model.\r\n\r\n\r\nFormally, suppose the sentence S of n wo ds S ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| w_k-1,  ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| We define the score of a sentence as:\r\n\r\n \r\nwhere the denominator n is for alleviating the influence of the sentence length to models' prediction, especially in sentence-level tasks. For a uni-directional model, ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ..., ||SYMBOLTOKEN|| The numerator becomes ||SYMBOLTOKEN|| which is factorized from ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| w_k, ||SYMBOLTOKEN|| This is essentially a LM. For a bi-directional model, the ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| which represents the S with the k-th word being removed. In particular, the k-th word can be removed with being replaced by a special token `[MASK]' in BERT. The numerator ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| can also be factorized from ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| w_k, ||SYMBOLTOKEN|| under the assumption that ||SYMBOLTOKEN|| is independent of the successive words (i.e. ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ..., ||SYMBOLTOKEN|| which is the bi-directional-context LM.\r\n\r\nIntuitively, ||SYMBOLTOKEN|| can be interpreted as how probable a word ||SYMBOLTOKEN|| is given the ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| or ||SYMBOLTOKEN|| For example, let ||SYMBOLTOKEN|| He put an [MASK] into the fridge, ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| elephant and ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| turkey. ||SYMBOLTOKEN|| should have a relatively larger value since filling in the \"elephant\" in the first case results in an improper sentence, which is against commonsense.\r\n\r\nAs introduced earilier (Table ||SYMBOLTOKEN|| all CATs tasks consist of instances with positive and negative sentences. After we score each sample in a test instance, the models  redict the positive samp e simply by taking the highest sc re in the instance.\r\n\r\n\r\n", {}], "Commonsense Tests Results": ["\r\n\r\n\r\n\r\nTable ||SYMBOLTOKEN|| shows the model performances with random choices as the baseline. Take WSC for example, the random baseline is 0.5, the human is 0.920 and all the models range between 0.51  and 0.694 with RoBERTa-large giving the best result of 0.694. Except for the ARCT task, all tested models demonstrate stronger performances than RANDOM, which indicates that the models all have varying degrees of commonsense. However, for most of the tasks, all of  odels are well below human p rformance.\r\n\r\n\r\n", {"Uni-directional Vs Bi-directional LM": ["\r\n  e compare unidirectional (GPT, GPT2-base, GPT2-medium) and bi-directional models  Bertbase, Bert-large, XLNetbase, XLNet-large, RoBERTa-base, and RoBERTa-large). Picking the stronges  model from each group, RoBERTa-large outp rforms GPT2medium by a large margin for every task  As mentioned before, RoBERTa-large has the same parameter size as GPT2-medium. However, RoBERTa-large is trained with much more data than GPT2-medium.\r\n\r\n\r\n\r\nFrom Figure ||SYMBOLTOKEN|| we can see that except for the SM task, both BERT-large and XLNet-large outperform GPT2-medium while BERT-large is trained with a smaller dataset than GPT2-medium. This indicates that bi-directional context can be more useful for learning commonsense. Intuitively, the models with bi-directional context can make more sentence-level inference. \r\nWhile only the predecessing words receive sufficient context in a uni-directional model, every word has the full context for bi-directional models. Table ||SYMBOLTOKEN|| shows examples where RoBERTa-large makes the correct prediction but GPT2-medium does not, we can see that the key tokens, which are considered to be the most influential part in making the correct  rediction, lie in the middle of the sentence. This can be the main reason why bi-directional context is important for models' commonsense ability.\r\n\r\n\r\n\r\n\r\n\r\n", {}], "Scale of Training Data": ["\r\n\r\nA larger training dataset intuitively allows a model to have access to more commonsense knowledge, thus performs better in our tests. Trained with by far the most data, RoBERTa is the winner for every task. Most of the models are in fact trained on a subset of the dataset used to train RoBERTa. However, larger dataset do not always work when the model capacity is limited with regard to commonsense. For example, GPT2-base underperforms GPT for many tasks in our dataset, which suggests GPT2-base underfits the WebText dataset with regard to commonsense. The fact that RoBERTa-base has the same parameter size as GPT2-base, yet benefits from the larger dataset suggests that bi-directional models have larger representative power in commonsense ability.\r\n\r\n\r\n\r\n\r\n", {}], "Number of Inference Steps": ["\r\nSimilar to humans, the model performance can intuitively drop when commonsense inference becomes more complicated. To verify this i tuition, we pick 100 sentences randomly from each test dataset and annotate the number of required inference steps (IS) of each instance manually. The inference step of each test dataset is defined as the average of the number of the turns of reasoning necessary for the instances from the test dataset. We choose to answer the question by counting the logical operations that exist in an instance. For example, for the sentence\r\n\r\nThey add a lot to the piece and I look forward to reading comments, but since comments sections always distract me from my work, Comment sections have failed., the logic chain is (They add a lot to the piece \u2227 I look forward to reading comments) \u2227 comments sections always distract me from my work \u2192 Comment sections have failed. Thus, this instance needs three inference steps.\r\n \r\nIn this way, we obtain the Inference Step (IS) for seven test datasets. Each instance is labeled by two expert annotators, and the inter-annotator agreement is 93%. The final IS is the average from both annotators. Figure ||SYMBOLTOKEN|| shows the results [The performances on tasks with more than one negative sample are transformed to binary-choice scales.] on the test cases with different IS. There is a decrease of performances as IS increases. SWAG and HellaSwag fall out the trend, which may suggest that the models have stronger commonsense ability in temporal reasoning.\r\n\r\nGenerally speaking, all of our tested models outperform the random baselines except for the ARCT task, which suggests that despite of using different modeling schemas, language modeling stands as an effective objective for extracting commonsense knowledge from large, raw texts. For each task, the overall performance increases with a larger model parameter size, a more sophisticated model design, and larger training data.\r\n\r\n\r\n\r\n", {}]}], "Robustness Test": ["\r\nThe robustness of models in commonsense reasoning is an important perspective in evaluating deep commonsense ability. Intuitively, a person can reason whether a statement makes sense or not because he has consistent knowledge. If the statement changes slightly, for example, changing a key word, that person should still make the correct judgement.\r\n\r\nWe aim to test the robustness of the five models by making dual test samples. A dual instance to the original instance should test the same commonsense knowledge point or largely relevant to the original one. In this way, we expect that the model can demonstrate consistency in the decision. One example is shown in Table ||SYMBOLTOKEN|| which choosing A in the original instance should lead to choosing B in the dual case (See Figure ||SYMBOLTOKEN|| for more examples).\r\n\r\n\r\n\r\n\r\nWe consider multiple ways to construct a dual test instance. Particularly, a dual test instance is built by methods: adding, deleting and replacing words in a test sample, or swapping two words in the sample, thereby resulting in a closely related test instance. All of our dual test instances are constructed from the original commonsense test data.\r\n\r\nWe construct 75 dual instances for each method above over WSC, SM, and ARCT, keeping the instances from each dataset approximately equivalent in order to evaluate the influence of different duality methods to the models. We then pair each dual instance with the original instance to form a new test case. If the model gives the correct or wrong prediction for both of the instances in this case, we recognize it as a consistent case.\r\n\r\n\r\nPortion of consistent cases of each method for each contextualizer. Add stands for adding key words in the test sample; Del stands for deleting key words in the test sample; Swap stands for swapping the position of words in the test sample; Sub stands for replacing key words in the test sample. The best contextualizer for each method is bolded.\r\n\r\n\r\nThe results are shown in Table ||SYMBOLTOKEN|| In theory, a model equipped with relevant commonsense should give consistent predictions on a pair of dual test case. However, we find that none of the models reach consistency. In fact, their consistency is well below the random baselines except for the Swap method.\r\n\r\nTo better investigate the reason behind the  oor consistency, we look at inconsistent cases from the pre-trained model (i.e RoBERTa-large). Similar to Trinh-2018-a (2018), we investigate how the model makes decision between two candidate sentences ||SYMBOLTOKEN|| and ||SYMBOLTOKEN|| where they have the same number of words. In particular, we look at:\r\n\r\n", {}]}	Contextualized representations trained over large raw text data have given remarkable improvements for NLP tasks including question answering and reading comprehension. There have been works showing that syntactic, semantic and word sense knowledge are contained in such representations, which explains why they benefit such tasks. However, relatively little work has been done investigating commonsense knowledge contained in contextualized representations, which is crucial for human question answering and reading comprehension. We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models' co monsense ability while bi-directional context and larger training set are bonuses. We additionally find that current models do poorly on tasks require more necessary inference steps. Finally, we test the robustness of models by making dual test cases, which are correlated so that the correct prediction of one sample should lead to correct prediction of the other. Interestingly, the models show confusion on these test cases, which suggests that they learn commonsense at the surface rather than the deep level. We release a test set, named CATs publicly, for future research.
