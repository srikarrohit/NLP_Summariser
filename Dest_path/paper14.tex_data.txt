paper14.tex_data.txt	Multi-Vehicle MixedReality Reinforcement Learning f r Autonomous Multi-Lane Driving	{"Introduction": ["\r\n\r\nThe deployment of automated and autonomous vehicles presents us with transformational opportunities for road transport. To date, the number of companies workin  on this technology is substantive, and growing cbsreport. \r\nOpportunities reach beyond single-vehi le automation: by enabling groups of vehicles to jointly agree on maneuvers and navigation strategies, real-time coordination promises to improve ov ral  traffic throughput, road capacity, and passenger safety dressler:2014,ferreira2010self. However, drivi g in multivehicle and multi-lane settings still remains a challenging research problem, due to unpredictable vehicle interactions (e.g., non-cooperative cars, unreliable communication), hard workspace limitations (e.g., lane topographies), and constrained platform dynamics (e.g., steering kinematics, driver comfort).\r\n\r\n\r\n\r\nLe rningbased methods, such as deep reinforcement learning, have proven effective at designing robot control policies for an increasing number of tasks in single-vehicle systems, for applications such as navigation khan2019learning, flight molchanovSimto2019, and locomotion tan2018sim. \r\nLeveraging such methods for learning autonomous driving policies is emerging as a particularly promising approach pan2017virtual, shalev2016safe, kuderer2015learning.\r\nYet, the process of safely learning autonomous driving involves unique challenges, since the decision models often used in robotics do not lend themselves naturally to the multi-vehicle domain, due to the unpredictable behaviour of other agents. The unapologetic nature of the trial-and-error process in reinforcement learning compounds the difficulty of ensuring functional safety.\r\n\r\n\r\nThese adversities call for learning that first takes place in simulation, before transferring to the real world miglino1995evolving, shah2018airsim. \r\nThis transfer, often referred to as sim2real, is challenging due to discrepancies between conditions in simulation and the real world (such as vehicle dynamics and sensor data) peng2018sim, james2019sim, chebotar2019closing.\r\nDespite substantial advances in this field, the problem of executing immature policies directly  n an autonomous vehicle still raises considerable safety concerns. These concerns are exacerbated when multiple autonomous vehicle  share the same workspace, risking collisions and un-reparable damage \r\nSimultaneously, the act of colliding---or nearly-co lidingis essential to the learning process, enabling future policy roll-outs to incorporate these critical experiences. How are w  to provide safe multi-vehicle learning experiences, without forgoing the realism of high-fidelity training data?\r\nThere is a dearth of work that addresses this challenge.\r\n\r\n\r\n\r\n\r\n\r\n\r\nOur goal in this paper is t  develop a safe and efficient framework that allows us to learn driving policies for autonomous  ehicles operating in a shared workspace, where collision-freeness cann t be guaranteed.\r\nTowards this end, we learn an end-to-end policy for vehicle navigation on a multi-lane tr ck that is shared with other moving vehicles and static obstacles. The learning is based on a model-free method embedded in a distributed training mechanism that we tailor for mixed-reality compatibility. Key to our learning procedure is a sim2real approach that uses real-world online policy adaptation in a mixed-reality setup, where obstacles (vehicles and objects) exist in the virtual domain. This allows us to perform safe learning by simulating (and learning from) collisions between the learning agent(s) and other objects in virtual reality. \r\nWe apply our framework to a multi-vehicle setup consisting  f one real vehicle, and se eral simulated vehicles (as shown in Figure ||SYMBOLTOKEN|| Experiments show that a significant performance improvement can be obtained after just a few runs in mixed-reality, reducing the number of collisions and increasing reward collection.\r\nTo the best of our knowledge, this is the first demonstration of mixed-reality reinforcement learning for multi-vehicle applications.\r\n\r\n\r\n \r\n\r\n", {}], "Related Work": ["\r\n\r\nTraining in simulation before transferring learned policies to the real world provides the benefits of safety and facilitated data collection. Several methods alleviate the difficulty of bridging the reality gap: (i) parameter estimation, which estimates parameters of the real system to achieve a more realistic simulation lowrey2018reinforcement, tan2018sim, (ii) iterative data collection, which learns distributions of dynamics parameters in an iterative manner christiano2016transfer, chebotar2019closing, and (iii) domain randomization, which trains over a distribution of the system dynamics for policies that are more robust against simulator discrepancies from reality peng2018sim, muratore2018domain, james2019sim, ||SYMBOLTOKEN|| Although these met ods contribute significantly to closing the reality gap, the problem of guaranteeing safe policy execution still persists. \r\nMoreover, it often proves hard to accommodate all situations the robot may encounter in the real world, where unexpected conditions are the norm. To ease this challenge, researchers have proposed methods for continuous online adaptation in model-based reinforcement learning fu2016one, gu2016continuous. The aim of this approach is to  earn an approximate model and then adapt it at tes  time. However, this can still lead to safety concerns when there is a mismatch between what the model is trained for, and how it is used at test-time. More recent approaches, such as meta-learning, strive to overcome this challenge ||SYMBOLTOKEN|| The commonality of all these approaches, however, is their focus on single-robot systems in isolated work-spaces; guaranteeing safe online-learning in shared workspaces is still an open problem.\r\n\r\nThe idea of exploiting mixed (and augmented) reality for robotics applications was originally introduced as a tool to facilitate developm nt and prototyping. Early work experiments with virtual humanoids amongst real obstacles stilman2005augmented, leveraging the setup to rapidly prototype and test humanoid sub-components. Chen et al. chen2009mixed use augmented reality to obtain a coherent display of visual feedback during interactions between a real robot and virtual objects. More recently, mixed reality has gained importance in shared human-robot environments williams2018virtual, where combinations of physical and virtua  environments can provide safer ways to test interactions, \"... by also allowing a gradual\r\ntransition of the system components into shared physical environments\" hoenig2015mixed.\r\nThe introduction of mixed reality to support reinforcement learnin  has barely bee  considered. In mohammadi2019mixed, Mohammadi et al. present an approach for online continuous deep reinforcement learning for a reach-to-grasp task in a mixed-reality environm nt. Although targets exist in the physical world, the learning procedure is carried out in simulation (using real data), before actions are transferred and executed on the actual robot.\r\n\r\nThe particularity of our work is that we focus on multi-robot settings, where inter-robot interactions contribute significantly to the learning process, but  annot be executed directly on mult ple real platforms without incurring repeated damages.\r\nNot only does our mixed-reality framework help bridge the reality gap that still stymies progress in reinforcement learning for robotics, but also, it is especially significant for the specific application at hand in this work.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n \r\n\r\n", {}], "Problem Statement": ["\r\n\r\nWe consider a multi vehicle system composed of N vehicles on a multi-lane (closed) traffic circuit with M lanes. Each vehicle in the system has a unique target velocity, ||SYMBOLTOKEN|| i.e., vehicl s aim to travel at potentially different speeds. The circuit is obstructed by K obstacles (static vehicles).\r\nIn order to maintain target speeds and avoid collisions, vehicles must learn to change lanes and execute overtaking maneuvers (we do not enforce a ru e regarding which side a vehicle may overtake on). An image of our three-lane setup is shown in Figure ||SYMBOLTOKEN|| for 17 vehicles (one of which is real, those in blue are static).\r\n\r\nAssumptions.\r\nWe are especially interested in a vehicle's high-level decision-making process that involves lane changes and speed modulation. We, therefore, consider the availability of a low-level controller that executes reliable trajectory following, allowing the vehicle to remain in the centre of its current lane.\r\nTo facilitate the low-lev l control task, we represent a lane by a sequence of cubic Bezier curves, continuous up to their first derivative (i.e. having no sharp corners). Vehicles are provided reliable (essentially noise-free) positioning information (e.g., through a motion capture system).\r\nWe also assume the ability of basic local communication, such that the desired velocity of each neighboring vehicle is available to the high-level controller. This neighborhood includes the six nearest vehicles within a vision radius, ||SYMBOLTOKEN|| Our goal is to learn a high-level control policy that allows  vehicles to drive as closely as possible to their target velocities, while av iding collisions with other vehicles.\r\n\r\n\r\n\r\n\r\n\r\n \r\n\r\n", {}], "Multi-Vehicle System": ["\r\n\r\nOur multi-vehicle system is based on a physical v hicle, the DeepRacer robot balaji2019deepracer, for which we also develop a virtual counterpart. \r\nThis platform, its dynamics, and control model are detailed below.\r\n\r\n\r\n\r\n\r\n\r\n\r\n@ S!S\r\nThe DeepRacer Robot\r\n\r\nThe DeepRacer is a 1/18th scale car with a 4MP camera, 4-wheel drive and Ackermann steering. It sports an Intel Atom processor, 4GB of memory, and 32GB of storage. It runs Ubuntu 16.04 LTS and ROS Kinetic Kame. The on-board computer and motors are powered by 13600mAh and 1100mAh batteries, respectively.\r\n\r\n\r\nThe DeepRacer was originally designed as a platform for vision-based reinforcement learning, with training carried out in simulation only. \r\nThis is different to our aim---which includes online training an  but also only focuses on non-vision-based, high-level decision-making. Therefore, we modified the platform to make it more suited to our goal.\r\nThe default ROS launch script was replaced, so that the DeepRacer does not\r\nrun a ROS master but relies on one running on a different device---therefore allowing more than one DeepRacer to be controlled simultaneously. \r\nWe implemented a new ROS node to communicate with the DeepRacer's servo node to set turning and throttle values. Adding this node also meant that communication to the DeepRacer could be done via UDP, reducing latency. \r\nFinally,\r\na custom, non-reflective case was designed to allow the integration of the robot with a motion tracking system.\r\n\r\n\r\n", {"DEFAULT": ["", {}], "Vehicle Model": ["\r\nThe DeepRacer has Ackermann steering geometry. We approximate its kinematics by the bicycle model, with motion equations:\r\n\u1e8b ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYM OLTOKEN|| ||SYMBOLTOKEN|| \u03d5_s  s the steering angle, ||SYMBOLTOKEN|| is the forward spee , ||SYMBOLTOKEN|| is the heading, and L is the vehicle's wheel base.\r\nThese equations are numerically integrated in our simulation via the Euler met od to obtain the position of the DeepRacer at each time step.\r\nFor the purpose of collision detect on in mixed-reality, the DeepRacer was modeled by   bounding box of similar size to its physical dimensions (\u223c 30cm \u00d7 20cm). \r\nVirtual vehicles are also identically modeled.\r\n\r\n\r\n\r\n", {}], "Simulation Setup": [" \r\nIn our setup, a ||SYMBOLTOKEN|| simulation provides the environment in which reinforcement learning agents can act, observe, and learn. As such, it also contains the high-level IDM/MOBIL controllers of the background traffic vehicles.\r\nWe implemented the reinforcement learning approach described in the previous section using Python and the PyTorch library. An interface between the ||SYMBOLTOKEN|| simulation and the Python interpreter was created using the  BOOST.Python ||SYMBOLTOKEN|| library. This interface exposes the ability to create environments as either mixed-real or purely virtual. The simulation provides o servations and reward signals to the Python implementation, according to the state of the environment. Then, it updates its state to reflect the agents' actions, as received from the Python interpreter.\r\n\r\nThe simulated environment also contains (i) the specifications of the Bezier curves for all lanes in the track, (ii) the states of the vehicles controlled by either reinforcement learning agents or the IDM/MOBIL algorithms, and (iii) K static obstacles. These obstacles are placed far enough apart not to fully block the road, and so that there is at least one in each lane of the circuit. Their exact positions are otherwise randomised. The starting locations of the background traffic and agent vehicle are likewise randomised along with the desired velocities ||SYMBOLTOKEN|| of all vehicles.\r\nFor each of the vehicles in the environment, collision detection is accomplished using bounding boxes of the same shape and size of a DeepRacer.\r\n\r\nThe simulation was written in ||SYMBOLTOKEN|| in order to provide higher performance, especially when pre-training a network in a purely simulated environment. \r\nTo the same end, the simulation was designed to be capable of running several simultaneous virtual environments (Figure ||SYMBOLTOKEN|| in order to allow the reinforcement learning algorithm to submit multiple parallel actions and receive multiple parallel observations---thus making a more efficient use of our learning computing hardware.\r\n\r\n\r\n", {}], "Real-World Setup": ["\r\n\r\nAs shown in Figure ||SYMBOLTOKEN|| the physical DeepRacer must interface with the simulation while training in mixed-reality.\r\nThe location and pose of a real-life DeepRacer in the environment is tracked using six OptiTrack Prime 17W cameras and the Motive motion capture software. \r\nWhen multiple real DeepRacers are used, we distinguish them by using unique layouts of reflective markers. \r\nThe positions of each of the DeepRacers is broadcast by Motive, receive  by a VRPN client and published to a ROS topic, making the data available to all nodes in our ROS environment.\r\nIn order to reduce network load and increase reliability, the frequency at which poses were transmitted was restricted to 50Hz, since this was also the update rate of the physics engine in the simulation. \r\nFrom the perspective of t e tracking system, the centre of a vehicle was defined as the centre of its rear axle. This choice preserves consistency with the simulation's definition of the centre of a car---itself chosen for the sake of simplicity, while using an Ack rmann steering model.\r\nThe vehicles drive on a closed loop track made up of individual trajectories that contain no intersections and are ||SYMBOLTOKEN|| continuous.\r\n\r\n\r\n", {}], "Mixed-Reality": ["\r\n\r\nMixed-reality plays a two-fold role in our work: (i) i  fosters an agent's learning, allowing simultaneous real and simulated training, and (ii) it provides us with better evaluation tools, through the ability to visualise the virtual and real agents' interactions.\r\n\r\nLearning\r\nIn the mixed-reality environment, the simulation receives live updates on the pose of the DeepRacer through the motion capture system and updates its representation of the environment state accordingly. The simulation sends commands setting the steering angle and velocity of the DeepRacer according to the actions of the high-level controller and the lateral component of  he lowlevel controller.\r\n\r\nThe simulation is able to detect collisions between the DeepRacer and the virtual vehicles through a collision box identical to that of a virtual vehicle sharing the same pose as the real agent. From the point of view of the high-level controllers, including the reinforcement learning agent,\r\nthe situation is no different from a purely virtual scenario---with the exception of the world's physics affecting the real DeepRacer. Parallelisation of environments is unavailable when training in a mixed-real environment, but since our implementation of A3C uses trajectories of experience with explicitly calculated returns, we substantially in rease their length and generate only a small number of trajectories for each optimisation step. Each of these trajectories is created using a different random initialisation of the environment in order to provide a variety of experiences to the reinforcement learning algorithm, at each optimisation step.\r\n\r\nVisualisation\r\nTo vi ualise the interaction between the virtual cars and the DeepRacer, during our tests, we set up a fixed camera to record the entire full-length experiments. From the simulation environment, we collect pose data for both the virtual and real cars and compute whether any vehicle is currently experiencing collisions.\r\nThese data are processed through a Python script importing Blender's API.\r\nAt each timestep, we inser  an animation keyframe of a vehicle model in the pose specified by the previously recorded data and a colour determined by whether the vehicle is (i) a fixed obstacle (blue), (ii) a moving vehicle (beige), or (iii), a vehicle currently in collision (red). \r\nIn a separate scene, the DeepRacer alias is also animated using the same procedure.\r\nThese two scenes are then composited together using Z-buffer values so that---when the DeepRacer is in front of a virtual vehicle---the area obscured by the Deepracer is transparent. The output can then be overlayed on top of the test footage to create the effect that the real and virtual vehicles are interacting.\r\n \r\n\r\n", {}]}], "Experiments": ["\r\n\r\nTo demonstrate the effectiveness of our mixed-reality setup---to train agents capable of collision-free driving---we performed experiments \r\non a ||SYMBOLTOKEN|| 3-lane track (see Figure ||SYMBOLTOKEN|| with lanes ||MATHEQUATION|| wide. The track itself fits a 3.5m \u00d7 2.2m area, with a lap length of roughly 16.4 metres, i.e., \u223c50 times the size of a DeepRacer ||SYMBOLTOKEN|| experiments include ||SYMBOLTOKEN|| (1 real, 12 virtual) vehicles and ||SYMBOLTOKEN|| virtual obstacles.\r\nThe low-level control parameters g and d (see Subsection ||SYMBOLTOKEN|| were set to 3 and 0.4, respectively.\r\nFor the learning parameters (see Section ||SYMBOLT KEN|| selected\r\n\u03b3 ||SYMBOLTOKEN|| 0.9,\r\n\u03c4 ||SYMBOLTOKEN|| 0.7,\r\n\u03f5 ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| and ||SYMBOLTO EN|| the actor and critics, we used learning rates of 2e-4 and 2e-3.\r\nOur results are summarized in Figures ||SYMBOLTOKEN|| <ref>, and ||SYMBOLTOKEN|| as well as by additional footage available on the Prorok Lab YouTube ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| during training of (i) the number of collisions per minute (top plot, lower is better) and (ii) the average reward collected by the training agent, over a sliding window of 8000 frames (bottom plot, higher is better).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nFirst,  e want to assess the soundness of our approach by evaluating how well training fares---in terms of incurred collisions and co lected reward.\r\nThis is shown in Figure ||SY BOLTOKEN|| where the two plots describe the evolution over time (measured in frames, i.e., the steps in which an agent receives one set of observations and takes one action) of: (i) the number of collisions per minute (top plot of Fig re ||SYMBOLTOKEN|| and (ii) the average collected reward (bottom plot of Figure ||SYMBOLTOKEN|| training is reflected in a general downward slope of the top plot (fewer collisions) and, conversely, a general upward slope of the bottom plot (greater reward).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSecond, we want to quantify the effectiveness of mixed-reality training at test time.\r\nThis is shown in Figure ||SYMBOLTOKEN|| top and bottom plots refer, once more, to collisions and collected reward, re pectively.\r\nEach one of the two plots compares two density distributions of these perfo mance metrics: one before (in blue) and one after (in red) training in mixed-reality.\r\nAs our simulation environment is partially randomised, the word scenario refers to all the data gathered from a single instantiation.\r\nOn the top plot, we can observe a left-shift (from blue to red, i.e., before and after) of the collisions' density distribution, that is, fewer collisions occurring after mixed-real training.\r\nOn the bottom plot, conversely, a right-shift reflects the improved ability of the agent, trained in mixed-reality, to collect reward.\r\n\r\nFinally, Figure ||SYMBOLTOKEN|| presents a qualitative comparison of how a DeepRacer agent's behaviour changes before (top) and after (bottom) mixed-reality training.\r\nThe x axis in Figure ||SYMBOLTOKEN|| shows the passing of time (in seconds) while the y axis captures the position of a vehicle along the track (in metres).\r\nFour blue horizontal lines represent obstacles (i.e., static virtual vehicles) on the track.\r\nAll other (13) lines represent moving vehicles---the thicker one being the DeepRacer agent.\r\nA color map is used to encode the speed (in metres per second) of each vehicle.\r\nRed dots indicate collisions between the real-life DeepRacer and either a virtual obstacle or vehicle.\r\nIndeed, collisions are rarer after mix dreality training.\r\nFootage of the mixed-reality experiments in Figure ||SYMBOLTOKEN|| i  also available ||SYMBOLTOKEN|| \r\n\r\n", {}], "Discussion": ["\r\n\r\n\r\nThe training stability and effectiveness of the proposed approach is reported in Figure ||SYMBOLTOKEN|| the top plot, one can observe early improvements---i.e., a reduction---in the number of collisions during training. This is followed by two periods of worsening performance (around frames  20'000 and 30'000), and then a more consistent downward trend (from frame 35'000 on .\r\nThe early improvements and performance deterioration (until frame 25' 00) may be explained by the choice of hyper-parameters. Our  earning rates aimed at aggressive policy changes. That is, an agent would have been, at first, too eager to learn how to overly accelerate---and collect more reward---resulting into more early collisions.\r\nThe bottom plot, presenting the collection of reward during training, shows a distinct mirroring (x axis symmetry) of the top plot. This is consistent with what we would expect---that is, fewer collisions leading to higher reward.\r\n\r\n\r\n\r\n\r\nFigure ||SYMBOLTOKEN||  emonstrates the performance of our methodology at test time.\r\nIn the top plot, we observe that the density distribution of collisions is significantly shifted to the left after mixed-reality training---indicating that our learning approach can effectively reduc  collisions.\r\nThe after-training distribution is also narrower, suggesting reduced variance and uncertainty.\r\nThe bottom plot presents the slightly more trivial result that reinforcement learning training does, indeed, lead to improved reward collection. Nonetheless, at test time, this is evidence of the ability of our approach to generalize.\r\n\r\n\r\n\r\nThe qualitative results in Figure ||SYMBOLTOKEN|| \r\ndemonstrate how the learning agent's behaviour changes before and after mixed-reality training.\r\nIn the top plot, a DeepRacer that has not yet been trained in mixed-reality collides remarkably often, with nearly every obs acle.\r\nThis collision-prone behaviour may be d e to the reduced responsiveness of the real DeepRacer hardware---when compared to the simulated vehicle---making it harder for the agent to timely stop or a oid other vehicles.\r\nAfter training in mixed-reality, collisions are almost com letely amended.\r\nIn the bottom plot of Figure ||SYMBOLTOKEN|| we can also observe virtual agents (IDM/MOBIL background traffic) either (i) overtaking the learning agent in the longer gaps between obstacles or (ii) piling-up behind it in more constraine  regions of the road---e.g., when the agent is cautiously approaching two near obstacles. \r\nInterestingly enough, traffic (e.g., between 50\" and 80\" in the bottom plot of Figure ||SYMBOLTOKEN|| is lik ly exacerbated by the fact that IDM/MOBIL agents would be willing to give the agent room to accelerate instead of overtaking it---yet, the agent proceeds at a reduced speed. While the learning agent is less dangerous after training, its unexpected prudence can mislead the other driving agents---which are not capable of learning---and reduce throughp t.\r\n\r\n\r\n\r\n\r\n\r\nFinally, it is important to observe that the simulation performance\r\nof the agents we transferred into our framework was still\r\ncharacterised by relatively high entropy.\r\nThis choice was made to minimise the risk of overfitting to the \r\nsimulation environment and let agents adapt more quickly\r\nto the mixed-reality setup.\r\nWhile we cannot say whether additional simulation-only training\r\nwould have benefited or hurt the agents transferring to mixed-reality,\r\nour resu ts support the idea that this appro ch led to quick and effective real-world adaptation.\r\nIn future developments of our framework, we will investigate\r\nmore sample-efficient off-policy reinforcement learning methods---e.g., softac which might allow for better performance without the need for a substantial increase in data gathering---and continuous action spaces.\r\n\r\n \r\n\r\n", {}], "Conclusions": ["\r\n\r\n\r\nThis work presented a mixed-reality framework for safe and efficient reinforcement learning of driving policies in multi-vehicle systems.\r\nOur learning algorithm was trained using a distributed mechanism specifically tailored to suit the needs of our mixed-reality setup. \r\nWe demonstrated successful online policy adaptation in an experimental setup involving one real vehicle and sixteen virtual vehicles. Our results showed that mixed-reality learning is able to provide significant performance improvements, leading to a reduction of collisions in the learned policies.\r\n\r\nThe particularity of our system is that it focuses on multi-robot settings, where interactions with other dynamic objects contribute significantly to the learning process, but cannot be executed directly on multiple real platforms without incurring repeated damages.\r\nThe proposed framework is a first of its kind: beyond providing specific benefits to the application at hand, it also helps bridge the reality gap that still stymies progress in reinforcement learning for robotics at large.\r\nFuture work will consider multiple learning agents using on-board sensing (e.g., vision), and how our mixed-reality setup enables their gradual introduction into mutually shared spaces.\r\n\r\n\r\n\r\n\r\n", {}], "Acknowledgements": ["\r\nThis work was supported by the Engineering and Physical Sciences Research Council (grant EP/S015493/1). Their support is gratefully acknowledged.\r\nThe DeepRace  robots used in this work were a gift to Amanda Prorok from AWS. Their support is gratefully acknowledged. This article solely reflects the opinions and conclusions of its authors and not AWS or any other Amazon entity.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n     #1       #1#1     #1  #1      #1      #1      #1#1 #1#1[2]#2[2]#2[1]#1[2][]arXiv:#2\r\n\r\n[Balaji, Mallya, Genc, Gupta, Dirac, Khare,\r\n  Roy, Sun, Tao, Townsend, et al.Balaji et al.2019]balaji2019deepracerauthorpersonBharathan Balaji, personSunil\r\n  Mallya, personSahika Genc, personSaurabh Gupta,\r\n  personLeo Dirac, personVineet Khare,\r\n  personGourav Roy, personTao Sun,\r\n  personYunzhe Tao, personBrian Townsend,\r\n  et al.year2019.\r\nDeepRacer: Educational Autonomous Racing Platform\r\n  for Experimentation with Sim2Real Reinforcement Learning.\r\njournal arXiv preprint arXiv:1911.01562\r\n  (year2019).\r\n\r\n\r\n\r\n[CBSCBS2018]cbsreportauthorpersonCBS.year2018.\r\ntitleCBS Insights Research ||SYMBOLTOKEN||    (year2018).\r\n(Accessed August 15, 2018).\r\n\r\n\r\n[Chebotar, Handa, Makoviychuk, Macklin, Issac,\r\n  Ratliff, and FoxChebotar et al.2019]chebotar2019closingauthorpersonYevgen Chebotar, personAnkur\r\n  Handa, personViktor Makoviychuk, personMiles\r\n  Macklin, personJan Issac, personNathan Ratliff,\r\n  andpersonDieter Fox.year2019.\r\nClosing the sim-to-real loop: Adapting simulation\r\n  randomization with real world experience. In booktitle 2019\r\n  International Conference on Robotics and Automation (ICRA). IEEE,\r\n  pages8973--8979.\r\n\r\n\r\n\r\n[Chen, MacDonald, and WunscheChen\r\n  et al.2009]chen2009mixedauthorpersonIan Yen-Hung Chen, personBruce\r\n  MacDonald, andpersonBurkhard Wunsche.year2009.\r\nMixed reality simulation for mobile robots. In\r\n  booktitle 2009 IEEE International Conference on Robotics and\r\n  Automation. IEEE, pages232--237.\r\n\r\n\r\n\r\n[Christiano, Shah, Mordatch, Schneider,\r\n  Blackwell, Tobin, Abbeel, and ZarembaChristiano et al.2016]christiano2016transferauthorpersonPaul Christiano, personZain\r\n  Shah, personIgor Mordatch, personJ nas Schneider,\r\n  personTrevor Blackwell, personJoshua Tobin,\r\n  personPieter Abbeel, andpersonWojciech Zaremba.year2016.\r\nTransfer from simulation to real world through\r\n  learning deep inverse dynamics model.\r\njournal arXiv preprint arXiv:1610.03518\r\n  (year2016).\r\n\r\n\r\n\r\n[Clemente, M rt\u00ednez, and\r\n  ChandraClemente et al.2017]batch-a2cauthorpersonAlfredo V. Clemente,\r\n  personHumberto Nicol\u00e1s Castej\u00f3n Mart\u00ednez,\r\n  andpersonArjun Chandra.year2017.\r\nEfficient Parallel Methods for Deep Reinforcement\r\n  Learning.\r\njournal arXiv preprint arXiv:1705.04862\r\n  (year2017).\r\n\r\n\r\n\r\n[Dressler, Hartenstein, Altintas, and\r\n  TonguzDressler et al.2014]dressler:2014authorpersonFalko Dressler, personHannes\r\n  Hartenstein, personOnur Altintas, andpersonOzan\r\n  Tonguz.year2014.\r\nInter-v hicle communication: Quo vadis.\r\njournal IEEE Communications Magazinevolume52, number6 (year2014),\r\n  pages170--177.\r\n\r\n\r\n\r\n[Ferreira, Fernandes, Concei\u00e7\u00e3o,\r\n  Viriyasitavat, and TonguzFerreira et al.2010]ferreira2010selfauthorpersonMichel Ferreira, personRicardo\r\n  Fernandes, personHugo Concei\u00e7\u00e3o,\r\n  personWa tanee Viriyasitavat, andpersonOzan K\r\n  Tonguz.year2010.\r\nSelf-organized traffic control. In\r\n  booktitle Proceedings of the seventh ACM international\r\n  workshop on VehiculAr InterNETworking. ACM, pages85--90.\r\n\r\n\r\n\r\n[Fu, Levine, and AbbeelFu\r\n  et al.2016]fu2016oneauthorpersonJustin Fu, personSergey Levine,\r\n  andpersonPieter Abbeel.year2016.\r\nOne-shot learning of manipulation skills with\r\n  online dynamics adaptation and neural network priors. In\r\n  booktitle 2016 IEEE/RSJ International Conference on\r\n  Intelligent Robots and Systems (IROS). IEEE, pages4019--4026.\r\n\r\n\r\n\r\n[Fujimoto, van Hoof, and MegerFujimoto\r\n  et al.2018]td3authorpersonScott Fujimoto, personHerke van\r\n  Hoof  andpersonDavid Meger.year2018.\r\nAddressing Function Approximation Error in\r\n  Actor-Critic Methods. In booktitle Proceedings of the 35th\r\n  International Conference on Machine Learning\r\n  (seriesProceedings of Machine Learning Research),\r\n  editorpersonJennifer DyandpersonAndreas Krause (Eds.), Vol. volume80.\r\n  publisherPMLR, addressStockholmsm\u00e4ssan, Stockholm\r\n  Sweden, ||SYMBOLTOKEN|| Lillicrap, Sutskever, and LevineGu\r\n  et al.2016]gu2016continuousauthorpersonShixiang Gu, personTimothy\r\n  Lillicrap, personIlya Sutskever, andpersonSergey\r\n  Levine.year2016.\r\nContinuous deep q-learning with model-based\r\n  acceleration. In booktitle International Conference on\r\n  Machine Learning. pages2829--2838.\r\n\r\n\r\n\r\n[Haarnoja, Zhou, Abbeel, and LevineHaarnoja\r\n  et al.2018]s ftacauthorpersonTuomas Haarnoja, personAurick\r\n  Zhou, personPieter Abbeel, andpersonSergey\r\n  Levine.year2018.\r\nSoft Actor-Critic: Off-Policy Maximum Entropy Deep\r\n  Reinforcement Learning with a Stochastic Actor.\r\njournal arXiv preprint arXiv:abs/1801.01290\r\n  (year2018).\r\n\r\n\r\n\r\n[HasseltHasselt2010]hasselt2010authorpersonHado van Hasselt.year2010.\r\nD uble Qlearning. In booktitle\r\n  Proceedings of the 23rd International Conference on Neural Information\r\n  Processing Systems - Volume 2 (seriesNIPS'10).\r\n  publisherCurran Associates Inc., addressUSA,\r\n  ||SYMBOLTOKEN|| Milanes, Scaria, Phan, Bolas, and\r\n  AyanianHoenig et al.2015]hoenig2015mixedauthorpersonWolfgang Hoenig, personChristina\r\n  Milanes, personLisa Scaria, personThai Phan,\r\n  personMark Bolas, andpersonNora Ayanian.year2015.\r\nMixed reality for robotics. In\r\n  booktitle 2015 IEEE/RSJ International Conference on\r\n  Intelligent Robots and Systems (IROS). IEEE, pages5382--5387.\r\n\r\n\r\n\r\n[Hyldmar, He, and ProrokHyldmar\r\n  et ||SYMBOLTOKEN|| Hyldmar, personYijun\r\n  He, andpersonAmanda Prorok.year2019.\r\nA Fleet of MiniatureCars for Experiments\r\n  in CooperativeDriving.\r\njournal IEEE International Conference Robotics and\r\n  Automation (ICRA) ||SYMBOLTOKEN|| Wohlhart, Kalakrishnan, Kalashnikov,\r\n  Irpan, Ibarz, Levine, Hadsell, and BousmalisJames et al.2019]james2019simauthorpersonStephen James, personPaul\r\n  Wohlhart, personMrinal Kalakrishnan, personDmitry\r\n  Kalashnikov, personAlex Irpan, personJulian Ibarz,\r\n  personSergey Levine, personRaia Hadsell, andpersonKonstantinos Bousmalis.year2019.\r\nSim-to-real via sim-to-sim: Data-efficient robotic\r\n  grasping via randomized-to-canonical adaptation networks. In\r\n  booktitle Proceedings of the IEEE Conference on Computer\r\n  Vision and Pattern Recognition. pages12627--12637.\r\n\r\n\r\n\r\n[Kesting, Treiber, and HelbingKesting\r\n  et al.2007]kesting2007generalauthorpersonArne Kesting, personMartin\r\n  Treiber, andpersonDirk Helbing.year2007.\r\nGeneral Lane-Changing Model MOBIL for Car-Following\r\n  Models.\r\njournal Transportation Research Recordvolume1999, number1 (year2007),\r\n  ||SYMBOLTOKEN|| Zhang, Li, Wu, Schlotfeldt, Tang,\r\n  Ribeiro, Bastani, and KumarKhan et al.2019]khan2019learningauthorpersonArbaaz Khan, personChi Zhang,\r\n  personShuo Li, personJiayue Wu,\r\n  personBrent Schlotfeldt, personSarah Y Tang,\r\n  personAlejandro Ribeiro, personOsbert Bastani, andpersonVijay Kumar.year2019.\r\nLearning safe unlabeled multi-robot planning with\r\n  motion constraints.\r\njournal arXiv preprint arXiv:1907.05300\r\n  (year2019).\r\n\r\n\r\n\r\n[Kuderer, Gulati, and BurgardKuderer\r\n  et al.2015]kuderer2015learningauthorpersonMarkus Kuderer, personShilpa\r\n  Gulati, andpersonWolfram Burgard.year2015.\r\nLearning driving styles for autonomous vehicles\r\n  from demonstration. In booktitle 2015 IEEE International\r\n  Conference on Robotics and Automation (ICRA). IEEE,\r\n  pages2641--2646.\r\n\r\n\r\n\r\n[Lowrey, Kolev, Dao, Rajeswaran, and\r\n  TodorovLowrey et al.2018]lowrey2018reinforcementauthorpersonKendall Lowrey, personSvetoslav\r\n  Kolev, personJeremy Dao, personAravind Rajeswaran,\r\n  andpersonEman el Todorov.year2018.\r\nReinforcement learning for non-prehensile\r\n  manipulation: Transfer from simulation to physical system. In\r\n  booktitle 2018 IEEE International Conference on Simulation,\r\n  Modeling, and Programming for Autonomous Robots (SIMPAR). IEEE,\r\n  pages35--42.\r\n\r\n\r\n\r\n[Miglino, Lund, and NolfiMiglino\r\n  et al.1995]miglino1995evolvingauthorpersonOrazio Miglino,\r\n  personHenrik Hautop Lund, andpersonStefano Nolfi.year1995.\r\nEvolving mobile robots in simulated and real\r\n  environments.\r\njournal Artificial lifevolume2,\r\n  number4 (year1995), pages417--434.\r\n\r\n\r\n\r\n[Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\r\n  Silver, and KavukcuogluMnih et al.2016]a3cauthorpersonVolodymyr Mnih,\r\n  personAdri\u00e0 Puigdom\u00e8nech Badia,\r\n  personMehdi Mirza, personAlex Graves,\r\n  personTimothy P. Lillicrap, personTim Harley,\r\n  personDavid Silver, andpersonKoray Kavukcuoglu.year2016.\r\nAsynchronous Methods for Deep Reinforcement\r\n  Learning.\r\njournal arXiv preprint arXiv:1602.01783\r\n  (year2016).\r\n\r\n\r\n\r\n[Mohammadi, Zamani, Kerzel, and\r\n  WermterMohammadi et al.2019]mohammadi2019mixedauthorpersonHadi Beik Mohammadi,\r\n  personMohammad Ali Zamani, personMatthias Kerzel,\r\n  andpersonStefan Wermter.year2019.\r\nMixed-Reality Deep Reinforcement Learning for a\r\n  Reach-to-grasp Task. In booktitle International Conference on\r\n  Artificial Neural Networks. Springer, pages611--623.\r\n\r\n\r\n\r\n[Molchanov, Chen, H\u00f6nig, Preiss, Ayanian, and\r\n  SukhatmeMolchanov et al.2019]molchanovSimto2019authorpersonArtem Molchanov, personTao\r\n  Chen, personWolfgang H\u00f6nig, personJames A. Preiss,\r\n  personNora Ayanian, andpersonGaurav S. Sukhatme.year2019.\r\nSim-to-(Multi)-Real: Transfer of\r\n  Low-LevelRobustControlPolicies to MultipleQuadrotors.\r\njournal arXiv:1903.04628 [cs]\r\n  ||SYMBOLTOKEN|| 1903.04628.\r\n\r\n\r\n[Muratore, Treede, Gienger, and\r\n  PetersMuratore et al.2018]muratore2018domainauthorpersonFabio Muratore, personFelix\r\n  Treede, personMichael Gienger, andpersonJan\r\n  Peters.year2018.\r\nDomain r ndomization for simulation-based policy\r\n  optimization with transferability assessment. In booktitle\r\n  Conference on Robot Learning. pages700--713.\r\n\r\n\r\n\r\n[Nagabandi  Clavera, Liu, Fearing, Abbeel,\r\n  Levine, and FinnNagabandi et ||SYMBOLTOKEN|| Nagabandi, personIgnasi\r\n  Clavera, personSimin Liu, personRonald S. Fearing,\r\n  personPieter Abbeel, personSergey Levine, andpersonChelsea Finn.year2019.\r\nLearning to Adapt in Dynamic, Real-WorldEnvironmentsThroughMeta-ReinforcementLearning.\r\njournal arXiv:1803.11347 [cs, stat]\r\n  ||SYMBOLTOKEN|| 1803.11347.\r\n\r\n\r\n[Pan, You, Wang, and LuPan\r\n  et al.2017]pan2017virtualauthorpersonXinlei Pan, personYurong You,\r\n  personZiyan Wang, andpersonCewu Lu.year2017.\r\nVirtual to real reinforcement learning for\r\n  autonomous driving.\r\njournal arXiv preprint arXiv:1704.03952\r\n  (year2017).\r\n\r\n\r\n\r\n[Peng, Andrychowicz, Zaremba, and AbbeelPeng\r\n  et al.2018]peng2018simauthorpersonXue Bin Peng, personMarcin\r\n  Andrychowicz, personWojciech Zaremba, andpersonPieter Abbeel.year2018.\r\nSim-to-real transfer of robotic control with\r\n  dynamics randomization. In booktitle 2018 IEEE International\r\n  Conference on Robotics and Automation (ICRA). IEEE, pages1--8.\r\n\r\n\r\n\r\n[Schulman, Wolski, Dhariwal, Radford, and\r\n  KlimovSchulman et al.2017]ppoauthorpersonJohn Schulman, personFilip\r\n  Wolski, personPrafulla Dhariwal, personAlec Radford,\r\n  andpersonOleg Klimov.year2017.\r\nProximal Policy Optimization Algorithms.\r\njournal arXiv preprint arXiv:1707.06347\r\n  (year2017).\r\n\r\n\r\n\r\n[Shah, Dey, Lovett, and KapoorShah\r\n  et al.2018]shah2018airsimauthorpersonShital Shah, personDebadeepta\r\n  Dey, personChris Lovett, andpersonAshish Kapoor.year2018.\r\nAirsim: High-fidelity visual and physical\r\n  simulation for autonomous vehi les. In booktitle Field and\r\n  service robotics. Springer, pages621--635.\r\n\r\n\r\n\r\n[Shalev-Shwartz, Shammah, and\r\n  ShashuaShalev-Shwartz et al.2016]shalev2016safeauthorpersonShai Shalev-Shwartz,\r\n  personShaked Shammah, andpersonAmnon Shashua.year2016.\r\nSafe, multi-agent, reinforcement learning for\r\n  autonomous driving.\r\njournal arXiv preprint arXiv:1610.03295\r\n  (year2016).\r\n\r\n\r\n\r\n[Stilman, Michel, Chestnutt, Nishiwaki, Kagami,\r\n  and KuffnerStilman et al.2005]stilman2005augmentedauthorpersonMichael Stilman, personPhilipp\r\n  Michel, personJoel Chestnutt, personKoichi\r\n  Nishiwaki, personSatoshi Kagami, andpersonJa es\r\n  Kuffner.year2005.\r\nAugmented reality for rob t development and\r\n  experi entation.\r\njournal Robotics Institute, Carnegie Mellon\r\n  University, Pittsburgh, PA, Tech. Rep. CMU-RI-TR-05-55volume2, number3 (year2005).\r\n\r\n\r\n\r\n[Sutton and BartoSutton and Barto2011]sutton2011authorpersonRichard S SuttonandpersonAndrew G Barto.year2011.\r\nReinforcement learning: An introduction.\r\n  (year2011).\r\n\r\n\r\n\r\n[Tan, Zhang, Coumans, Iscen, Bai, Hafner,\r\n  Bohez, and VanhouckeTan et al.2018]tan2018simauthorpersonJie Tan, personTingnan Zhang,\r\n  personErwin Coumans, personAtil Iscen,\r\n  personYunfei Bai, personDanijar Hafner,\r\n  personSteven Bohez, andpersonVincent Vanhoucke.year2018.\r\nSim-to-real: Learning agile locomotion for\r\n  quadruped robots.\r\njournal arXiv preprint arXiv:1804.10332\r\n  (year2018).\r\n\r\n\r\n\r\n[Tobin, Fong, Ray, Schneider, Zaremba, and\r\n  AbbeelTobin et ||SYMBOLTOKEN|| Tobin, personR. Fong,\r\n  personA. Ray, personJ. Schneider, personW.\r\n  Zaremba, andpersonP. Abbeel.year2017.\r\nDomain randomization for transferring deep neural\r\n  networks from simulation to the real world. In booktitle 2017\r\n  IEEE/RSJInternationalConference on IntelligentRobots and\r\n  Systems (IROS). ||SYMBOLTOKEN|| Hennecke, and HelbingTreiber\r\n  et al.2000]treiber2000congestedauthorpersonMartin Treiber, personAnsgar\r\n  Hennecke, andpersonDirk Helbing.year2000.\r\nCongested traffic states in empirical observations\r\n  and microscopic simulations.\r\njournal Phys. Rev. Evolume62\r\n  (dateAugyear2000), pages1805--1824.\r\n\r\nIssue ||SYMBOLTOKEN|| Szafir, Chakraborti, and\r\n  Ben AmorWilliams et al.2018]williams2018virtualauthorpersonTom Williams, personDaniel\r\n  Szafir, personTathagata Chakraborti, andpersonHeni\r\n  Ben Amor.year2018.\r\nVirtual, augmented, and mixed reality for\r\n  human-robot interaction. In booktitle Companion of the 2018\r\n  ACM/IEEE International Conference on Human-Robot Interaction. ACM,\r\n  pages403--404.\r\n\r\n\r\n\r\n\r\n\r\n\n", {}]}	Autonomous driving promises to t ans orm road transport. Multi-vehicle and multi-lane scenarios, however, present unique challenges due to constrained navigation and unpredictable vehicle interactions. Learning-based methods---such as deep reinforcement learning---are emerging as a promising approach to automatically design intelligent driving policies that can cope with thes  challenges. Yet, the process of safely learning multi-vehicle driving behaviours is hard: while collisions---and their near-avoidance---are essential to the learning process, directly executing immature policies on autonomous vehicles raises considerable safety concerns. In this article, we present a safe and efficient framework that enables the learning of driving policies for autonomous vehicles operating in a shared workspace, where the absence of collisions cannot be guaranteed. Key to our learning procedure is a sim2real approach that uses real-world online policy adaptation in a mixed-reality setup, where other vehicles and static obstacles exist in the virtual domain. This allows us to perform safe learning by simulating (and learning from) collisions between the learning agent(s) and other objects in virtual reality. Our results demonstrate that, after only a few runs in mixed-reality, collisions are significantly reduced.
