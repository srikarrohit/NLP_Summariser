paper6.tex_data.txt	Large-Scale Noun Compound Interpretation	{"Introduction": ["\r\n\r\n\r\nNoun compounds (NCs) such as malaria mosquito and colon\r\ncancer tumor  suppressor protein are challenging  for text processing\r\nsince  the relationship  between the  nouns  they are  composed of  is\r\nimplicit.\r\nNCs are abundant in English and understanding their semantics is\r\nimportant in many natural language processing (NLP) applications.\r\nFor example,  a question answering  system might need to  know whether\r\nprotein acting as  a tumor suppressor is a  good paraphrase for\r\ntumor  suppressor  protein.  Similarly, a  machine  translation\r\nsystem  facing the  unknown noun  compound Geneva  headquarters\r\nmight  translate  it  better  if  it  could  first  paraphrase  it  as\r\nGeneva headquarters of the WTO.\r\nGiven  a  query for  \"migraine  treatment\",  an\r\ninformation  retrieval  system  could   use  paraphrasing  verbs  like\r\nrelieve  and  prevent  for query  expansion  and  result\r\nranking.\r\n\r\nMost work on noun compound interpretation has focused on twoword NCs.\r\nThere have been two general lines of research:\r\nthe first one derives the NC semantics from the semantics of the nouns it is made of Rosario:2002,Moldovan:2004,Kim:2005,Girju:2007a,Oseaghdha:2009,Tratz:2010,\r\nwhile the second one models the relationship between the nouns directly\r\nVanderwende:1994,Lapata:2002,Kim:2006,Nakov:2006:AIMSA,Nakov:2008,Butnariu:2008.\r\n\r\nIn either  case, the semantics of  an NC is typically  expressed by an\r\nabstract relation like Cause (e.g., malaria mosquito),\r\nSource (e.g.,  olive oil), or  Purpose (e.g.,\r\nmigraine  drug),  coming from  a  small  fixed inventory.  Some\r\nresearchers\r\nhowever, have argued for a more fine-grained, even infinite, inventory Finin:1980.\r\nVerbs are particularly useful in this respect and can capture elements\r\nof the semantics that the abstract relations cannot.\r\nFor  example,   while  most  NCs  expressing   Make,  can  be\r\nparaphrased by common patterns like be made of and be\r\ncomposed of,  some NCs allow  more specific patterns,  e.g., be\r\nsqueezed from for orange juice,  and be topped with for\r\nbacon pizza.\r\n\r\n\r\nRecently,    the    idea    of    sing    finegrained    paraphrasing\r\nverbs    for    NC    semantics   has    been    gaining    popularity\r\nButnariu:2008,Nakov:2008:AIMSA; there  has also been  a related\r\nshared  task   at  SemEval-2010   SemEval:2010Paraphrase.  This\r\ninterest is partly  driven by practicality: verbs  are directly usable\r\nas paraphrases.  Still, abstract relations remain  dominant since they\r\noffer  a more  natural generalization,  which is  useful for  many NLP\r\napplications.\r\n\r\nOne good  contribution to this debate  would be a direct  study of the\r\n elat onship between fine-grained and  coarse-grained relations for NC\r\ninterpretation. Unfortunately, the existing  datasets do not allow this\r\nsince they are tied to one particular granularity; moreover, they only\r\ncontain  a  few  hundred  NCs.  Thus, our  objective  is  to  build  a\r\nlarge-scale dataset of hundreds of thousands of NCs, each interpreted\r\n(1) by an abstract semantic relation and\r\n(2) by a set of paraphrasing verbs.\r\nHaving such a large dataset would also help the overall advancement of the field.\r\n\r\nSince there is no universally  accepted abstract relation inventory in\r\nNLP,  and  since  we  are  interested in  NC  semantics  from  both  a\r\ntheoretical and  a practical viewpoint,  we chose the set  of abstract\r\nrelations  proposed in  the  theory of  Levi:1978, which  is\r\ndominant  in theoretical  linguistics and  has  een  also used  in NLP\r\nNakov:2008.\r\n\r\nWe use a two-step algorithm to jointly harvest NCs and patterns (verbs\r\nand prepositions) that  interpret them for a  given abstract relation.\r\nFirst, we  extract NCs using  a small number  of seed patterns  from a\r\ngiven abstract  relation. Then,  using the  extracted NCs,  we harvest\r\nmore patterns. This  is repeated  ntil no new NCs  and patterns can be\r\nextracted or for a prespecified number of iterations.\r\nOur  approach  combines  pattern-based extraction  and  bootstrapping,\r\nwhich is novel for NC  interpretation; however, such combinations have\r\nbeen used in other areas, e.g., named entity recognition\r\nRiloff:1999,Thelen:2002,Curran:2007,McIntosh:2009.\r\n\r\n\r\n\r\nThe remainder of the paper is organized as follows:\r\nsec:relatedwork gives an overview of related work,\r\nsec:representation motivates our semantic representation,\r\nSections ||SYMBOLTOKEN|| <ref>, and ||SYMBOLTOKEN|| our method, dataset and experiments, respectively,\r\nsec:discussion discusses the results,\r\nsec:erroranalysis provides error analysis,\r\nand sec:conclusion concludes with suggestions for future work.\r\n\r\n\r\n\r\n\r\n", {}], "Related Work": ["\r\n\r\nAs we  mentioned above,  the implicit relation  between the  two nouns\r\nforming a  noun compound can  often be expressed overtly  using verbal\r\nand prepositional  paraphrases. For example, student  loan is\r\nloan given to a student, while morning\r\ntea can be paraphrased as tea in the morning. \n\r\nThus, many NLP approaches to NC semantics have used  erbs and prepositions\r\nas a fine-grained semantic representation\r\nor as features when predicting coarse-grained abstract relations.\r\nFor  example,  Vanderwende:1994 associated  verbs  extracted\r\nfrom definitions in an online dictionary with abstract relations.\r\nLauer:1995 expressed NC semantics  sing eight prepositions.\r\nKim:2006 predicted abstract relations using verbs as features.\r\nNakov:2008 proposed a fine-grained NC interpretati n\r\nusing a distribution over Web-derived verbs, prep sitions and coordinating conjunctions;\r\nthey also used this distribution to predict coarse-grained abstract relations.\r\nButnariu:2008 adopted a similar fine-grained verb-centered approach to NC semantics.\r\nUsing a distribution over verbs as a semantic interpretation was also carried out in  a recent challenge: SemEval-2010  Task 9\r\nbutnariu-EtAl:20 9:SEW,SemEval:2010Paraphrase.\r\n\r\n\r\nIn noun compound interpretation, verbs and prepositions can be seen as patterns connecting the two nouns in a paraphrase. Similar pattern-based approaches have been popular in  information extraction  and  ontology   learning.\r\nFor example,\r\nHearst:1992   extract d hyp nyms   using  patterns such as\r\nX, Y,  and/or other  Zs,\r\nwhere  Z is a hypernym of X and Y.\r\nBerland:1999 used similar patterns to extract meronymy (part-whole) relations,\r\ne.g., parts/NNS of/IN wholes/NNS matches basements of buildings.\r\nUnfortunately, matches  are rare,  which makes  it difficult  to build\r\nlarge  semantic inventories.  In  order to  overcome data  sparseness,\r\npattern-based approaches are often combined with bootstrapping.\r\nFor example, Riloff:1999 used a multi-level bootstrapping\r\nalgorithm to learn both a semantic lexicon and extraction patterns,\r\ne.g., owned by X extracts Company\r\nand facilities in X extracts Location.\r\nThat is, they learned semantic lexicons using extraction patterns, and\r\nthen, alternatively, they extracted new patterns using these lexicons.\r\nThey also introduced a second level of bootstrapping to retain\r\nthe  most  reliable  examples  only.  While  the  method  enables  the\r\nextraction of large lexicons,\r\nits quality  degrades rapidly,  which makes it impossible  to run\r\nfor  too many  iterations.  Recently, Curran:2007  and\r\nMcIntosh:2009  proposed ways  to  control degradation  using\r\nsimultaneous learning and weighting.\r\n\r\nBootstrapping has  been applied to  noun compound extraction  as well.\r\nFor example, Kim:2007  used it to produce a  large number of\r\nsemantically interpreted noun compounds from  a small number of seeds.\r\nIn each iteration, the method replaced one component of an NC with its\r\nsynonyms, hypernyms and  hyponyms to generate a new NC.  These new NCs\r\nwere  further filtered  based on  their semantic  similarity with  the\r\noriginal  NC.  While  the  method  acquired a  large  number  of  noun\r\ncompounds without significant semantic drifting, its accuracy degraded\r\nrapidly  after each  iteration.\r\nMore importantly, the variation of the sense pairs was limited\r\nsince new NCs had to be semantically similar to the original NCs.\r\n\r\nRecently,  Kozareva:2010 combined patterns and b otstrapping\r\nto learn the selectional restrictions for various semantic relations.\r\nThey used patterns involving the coordinating conjunction and,\r\ne.g., \"*  and John  fly to *\",  and learned  arguments such\r\nas  Mary/Tom  and  France/New  York.  Unlike  in  NC\r\ninterpretation, it  is not  necessary for their  arguments to  form an\r\nNC,  e.g.,  Mary  France  and France  Mary  are  not\r\nNCs. Rather,  they were interested in building a semantic  ontology with  a predefined\r\nset  of semantic  relations, similar to  YAGO  Suchanek:2007, where\r\nthe  pattern work  for  woul  have  arguments like  a\r\ncompany/UNICEF.\r\n\r\n\r\n\r\n", {}], "Semantic Representation": ["\r\n\r\nInspired by Finin:1980,\r\nNakov:2006:AIMSA and Nakov:2008:AIMSA\r\nproposed that NC semantics is  best expressible\r\nusing paraphrases  involving verbs  and/or prepositions.  For example,\r\nbronze  statue  is  a  statue tha   is  made  of,  is\r\ncomposed of, consists of, contains, is of, is, is handcrafted from, is\r\ndipped in, looks  like bronze. They further proposed that selecting one\r\nsuch  paraphrase is  not enough\r\nand that multiple paraphrases are needed for a fine-grained  representation.\r\nFinally, they observed that not all paraphrases are equally good\r\n(e.g.,  is made  of  is  arguably  better\r\nthan  looks like  or is  dipped in  for Make),\r\nand thus proposed that the  semantics of  a noun  compound should be expressed\r\nas  a  distribution over  multiple possible paraphrases.\r\nThis line  of  research  was later adopted  by  SemEval-2010  Task  9\r\nSemEval:2010Paraphrase.\r\n\r\nIt easily  follows that the  semantics of abstract  relations\r\nsuch as Make that can hold between  the nouns in an NC can be represented\r\nin  the  same way:  as  a  distribution  over paraphrasing  verbs  and\r\nprepositions.  Note, however, that some  NCs are paraphrasable  by\r\nmore  specific  verbs  that do not necessarily support the target abstract relation.\r\nFor  example, malaria  mosquito, which  expresses\r\nCause, can  be paraphrased  using verbs  like carry,\r\nwhich do not  imply direct causation. Thus, while we  will be focusing\r\non  extracting  NCs  for a  particular  abstract  relation,  we  are\r\ninterested in building semantic  representations that are specific for\r\nthese NCs  and do not  necessarily apply to all  instances of\r\nthat relation.\r\n\r\n\r\nTraditionally, the semantics  of a noun compound  have been represented\r\nas  an abstract  relation  drawn  from a  small  closed  set.\r\nUnfortunately, no such set is universally accepted, and mapping between sets has  proven challenging\r\nGirju:2005. Moreover, being both abstract and limited, such sets\r\ncapture  only  part of  the  semantics;  often multiple  meanings  are\r\npossible, and sometimes none of  the pre-defined ones suits\r\na  given example. Finally, it  is unclear how useful  these sets are\r\nsince  researchers have often fallen  short of demonstrating practical uses.\r\n\r\nArguably, verbs have more expressive power and are more suitable for semantic representation:\r\nthere is an infinite number of them downing:1977:nc:sem,\r\nand they can capture fine-grained aspects of the meaning. For example,\r\nwhile both  wrinkle  treatment and   migraine  treatment express\r\nthe  same abstract  relation Treatment-For-Disease,\r\nfine-grained differences can be revealed using verbs,\r\ne.g.,   smooth can paraphrase the former, but not the latter.\r\n\r\nIn many theories, verbs play an  important role in NC derivation  Levi:1978.\r\nMoreover, speakers often use verbs to make the hidden relation between the noun in a noun compound overt.\r\nThis allows for simple  extraction and for straightforward use\r\nin NLP tasks like textual entailment Tatu:Moldovan:2005:entailment\r\nand machine translation Nakov:2008:ECAI.\r\n\r\nFinally, a single verb is often not enough,\r\nand the meaning is better approximated by  a collection of verbs. For example,\r\nwhile  malaria  mosquito expresses Cause\r\n(and is paraphrasable using  cause),  further aspects of the meaning can  be captured\r\nwith more verbs, e.g.,  carry,  spread,  be responsible for,\r\n be infected  with,  transmit    pass on, etc.\r\n\r\n\r\n\r\n\r\n", {}], "Method": ["\r\n\r\nWe   harvest   noun   compounds  expressing   some   target   abstract\r\nsemantic  relation   (in  the  experiments  below,   this  is  ||SYMBOLTOKEN||  starting  from a  small  number  of initial  seed\r\npatterns: paraphrasing  verbs and/or prepositions. Optionally,  we might\r\nalso  be given  a  small  number of  noun  compounds that  instantiate\r\nthe  target   abstract relation.  We  then   learn  more  noun   compounds  and\r\npatterns  for  the  r lation  by  alternating  between  the  following\r\ntwo  bootstrapping  steps,  using  the  Web as  a  corpus.  First,  we\r\nextract more noun compounds that  are paraphrasable with the available\r\npatterns  (see  sec:extraction-nc).  We  then  look  for  new\r\npatterns that  can paraphrase the newly-extracted  noun compounds (see\r\nsec:extraction-pattern). These  two steps are  repeated until\r\nno  new noun  compounds can  be  extracted or  until a  pre-determined\r\n umber of iterations has been  reached. A schematic description of the\r\nalgorithm is shown in fig:process.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", {"Bootstrapping Step 1: Noun Compound Extraction": ["\r\n\r\nGiven a list of patterns (verbs and/or prepositions),\r\nwe mine the Web to extract noun compounds that match these patterns.\r\nWe experiment with the following three bootstrapping strategies for this step:\r\n\r\n\r\n  *Loose bootstrapping uses the available patterns and imposes no further restrictions.\r\n\r\n  *Strict bootstrapping requires that, in addition to the patterns themselves, some noun compounds matching each pattern be made available a  well. A pattern is only instantiated in the context of either the head or the modifier of a noun compound that is known to match it.\r\n\r\n  *NC-only strict bootstrapping is a strict r version of strict bootstrapping, where the list of patterns is limited to the initial seeds.\r\n\r\n\r\n\r\n Below we describe each of the sub-steps of the NC extraction process:\r\nquery generation, snippet harvesting, and noun compound acquisition & filtering.\r\n\r\n\r\n\r\n", {"Query Generation": ["\r\n\r\nWe generate generalized exact-phrase queries\r\nto be used in a Web search engine (we use Yahoo!):\r\n\r\n\r\n\r\n\r\n\r\n where  PATTERN is  an inflected  form of  a verb,\r\nMOD  and HEAD are inflected forms\r\nthe  modifier and the head of a noun compound that is paraphrasable by the pattern,\r\nthat is the  word that,\r\nand  * is  the  search engine's  star operator.\r\n\r\nWe use the first  pattern for loose bootstrapping and\r\nthe other  two for both strict  bootstrapping and NC-only\r\nstrict bootstrapping.\r\n\r\nNote that the above queries  are generalizations of the actual queries\r\nwe  use against  the  search  engine. In  order  to instantiate  these\r\ngeneralizations, we further generate  the possible inflections for the\r\nverbs  and the  nouns involved.  For nouns,  we produce singular and\r\nplural forms, while  for verbs, we vary not only  the number (singular\r\nand plural), but  also the tense (we allow present,  past, and present\r\nperfect). When  inflecting verbs,  we distinguish between  active verb\r\nforms  like consist  of  and passive  ones  like be  made\r\nfrom and  we treat  them accordingly.\r\nOverall, in the case of  loose bootstrapping, we generate about\r\n14  and  20 queries  per  pattern  for  active and  passive  patterns,\r\nrespectively, while for  strict bootstrapping and NC-only\r\nstrict  bootstrapping,  the  instantiations  yield about  28  and  40\r\nque ies for active and passive patterns, respectively.\r\n\r\nFor example, given the seed be made of,\r\nwe could generate\r\n\"* that were made of *\".\r\nIf we are further given the NC orange juice,\r\nwe could also produce\r\n\"juice that was made of *\" and\r\n\"* that is made of oranges\".\r\n\r\n\r\n\r\n\r\n", {}], "Snippet Extraction": ["\r\n\r\nWe  execute  the  above-described instantiations  of  the  generalized\r\nqueries against a search engine as exact phrase queries, and, for each\r\none, we collect the snippets for the top 1,000 returned results.\r\n\r\n\r\n\r\n", {}], "NC Extraction and Filtering": ["\r\n\r\nNext, we  process the snippets  returned by  the search engine  and we\r\nacquire p tential noun  compounds from them.\r\nThen, in each  snippet, we look for  an instantiation\r\nof the  pattern used  in the  query and  we try\r\nto  extract  suitable  noun(s)  that occupy  the  position(s)  of  the\r\n*.\r\n\r\nFor loose  bootstrapping, we extract  two nouns, one  from each\r\nend of the matched pattern,  while for strict bootstrapping and\r\nfor  NC-only strict  bootstrapping, we  only extract  one noun,\r\neither preceding  or following  the pattern, since  the other  noun is\r\nalready fixed. We then lemmatize the  extracted noun(s) and\r\n e  form  NC candidates  from  the  two arguments  of  the\r\ninstantiated  pattern,  taking into  account  whether  the pattern  is\r\nactive or passive.\r\n\r\nDue  to  the   vast  number  of  snippets  we  have   to  process,  we\r\ndecided not to use a  syntactic parser  or a  part-of-speech (POS)\r\ntagger[In  fact,  POS  taggers  and  parsers  are  unreliable\r\nfor  Web-derived snippets,  which often  represent parts  of sentences\r\nand  contain errors  in  spelling,  capitalization and  punctuation.];\r\nthus, we  use   heuristic  rules  instead.  We   extract  \"phrases\"  using\r\nsimple  indicators   such  as   punctuation  (e.g.,   comma,  period),\r\ncoordinating conjunctions[Note  that filtering  the arguments\r\nusing  such  indicators  indirectly subsumes  the  pattern  \"X\r\nPATTERN Y and\"  proposed in Kozareva:2010.] (e.g.,\r\nand,  or),   prepositions  (e.g.,  at,   of,  from),\r\nsubordinating conjunctions (e.g.,  because, since, although),\r\nand  relative  pronouns (e.g.,  that,  which,  who). We  then\r\nextract the nouns from these phrases, we lemmatize them using WordNet,\r\nand we form a list of NC candidates.\r\n\r\nWhile the above heuristics work reasonably well in practice,\r\nwe perform some further filtering, removing all NC candidates\r\nfor which one or more of the following conditions are met:\r\n\r\n\r\n  * the candidate NC is one of the seed examples or has been extracted on a previous iteration;\r\n\r\n  * the head and the modifier are the same;\r\n\r\n  * the head or the modifier are not both listed as nouns in WordNet Fellbaum:1998;\r\n\r\n  * the candidate NC occurs less than 100 times in the Google Web 1T 5-gram ||SYMBO TOKEN||  * the NC is extracted less than N times (we tried 5 and 10) in the cont xt of the pattern for all instantiations of the pattern.\r\n\r\n\r\n\r\n\r\n", {}]}], "Bootstrapping Step 2: Pattern Extraction": ["\r\n\r\n\r\nThis is  the second  tep  of our  bootstrapping algorithm as  shown on\r\nfig:process. Given a list of  noun compounds, we mine the Web\r\nto  extract patterns:  verbs and/or  prepositions that  can paraphrase\r\nea h  NC.  The  idea is  to  turn the  NC's  pre-modifier into  a\r\npost-modifying relative  clause and to  collect the verbs and prepositions\r\nthat  are used in such  clauses. Below we describe each of  the sub-steps of the\r\nNC extraction  process: query  generation, snippet harvesting,  and NC\r\nextraction & filtering.\r\n\r\n\r\n", {"Query Generation": ["\r\n\r\nThe  process of  extraction  starts with  exact-phrase queries  issued\r\nagainst a Web search engine  (again Yahoo!) using the following\r\ngeneralized pattern:\r\n\r\n\r\n\"HEAD THAT? * MOD\"\r\n\r\n\r\n where MOD and  HEAD are inflected forms of\r\nNC's  modifier  and  head,  respectively,  THAT?  stands  for\r\nthat,  which,  who  or   the  empty  string,  and\r\n* stands for ||MATHEQUATION|| instances of search engine's star operator.\r\n\r\nFor example, given orange juice,\r\nwe could generate queries like\r\n\"juice that * oranges\",\r\n\"juices which * * * * * * oranges\",\r\nand \"juices * * * orange\".\r\n\r\n\r\n\r\n", {}], "Pattern Extraction and Filtering": ["\r\n\r\nWe  split  the  extracted  snippets into  sentences,  and  filter  out\r\nall  incomplete  ones  and  those  that do  not  contain  (a  possibly\r\ninflected  version  of)  the  target   nouns.  We  further  make  sure\r\nthat  the   word  sequence  following  the   second  mentioned  target\r\nnoun  is   non-empty  and  contains   at  least  one   non-noun,  thus\r\nensuring  the  snippet  includes  the  entire  noun  phrase.  We  then\r\nperform\r\nshallow parsing,\r\nand we extract all verb  forms, and the following preposition, between\r\nthe  target nouns.  We allow  for adjectives  and participles  to fall\r\nbetween the verb and the preposition but not nouns; we further ignore\r\nmodal  verbs and  auxiliaries, but  we retain  the passive  be,\r\nand  we  make sure  there  is  exactly  one  verb phrase  between  the\r\ntarget nouns.  Finally, we  lemmatize the verbs  to form  the patterns\r\ncandidates, and we apply the following pattern selection rules:\r\n\r\n\r\n  * we filter out all patterns that were provided as initial seeds or were extracted previously;\r\n\r\n  * we select the top 20 most frequent patterns;\r\n\r\n  * we filter out all patterns that were extracted less than N times (we tried 5 and 10) and with less than M NCs per pattern (we tried 20 and 50).\r\n\r\n\r\n\r\n\r\n", {}]}]}], "Target Relation and Seed Examples": ["\r\n\r\n\r\n\r\nOur seed examples: 20 noun compoun s and 18 verb patterns.\r\n\r\n\r\n\r\n\r\n\r\nAs  we mentioned  above, we  use the  inventory of  abstract relations\r\nproposed   in   the   popular  theoretical   linguistics   theory   of\r\nLevi:1978. In  this theory, noun compounds  are derived from\r\nunderlying relative clauses or noun phrase complement constructions by\r\nmeans  of  two general  processes:  predicate  deletion and  predicate\r\nnominalization.  Given   a  two-argument  predicate,     predicate\r\ndeletion removes that predicate, but retains its arguments to form an\r\nNC, e.g.,   pie made of apples \u2192   apple pie. In\r\ncontrast,  predicate nominalization creates an NC whose head is a\r\nnominalization  of  the underlying  predicate  and  whose modifier  is\r\neither the  subject or the  object of  that predicate, e.g.,   The\r\nPresident  refused General  MacArthur's  request. \u2192  \r\npresidential refusal.\r\n\r\nAccording  to Levi,  predicate  deletion can  be  applied to  abstract\r\npredicates,  whose semantics  can be  roughly approximated  using five\r\nparaphrasing  verbs   (Cause,  Have,  Make,\r\nUse,  and Be)  and  four prepositions  (In,\r\nFor,  From,   and  About).\r\n\r\nTypically,  in predicate deletion,  the modifier  is derived from  the object  of the\r\nunderlying relative clause; however, the  first three verbs also allow\r\nfor it to be derived from  the subject. Levi expresses the distinction\r\nusing  indexes. For  example,  music  box is  ||SYMBOLTOKEN|| i.e., the  box makes music, while\r\nchocolate  bar  is ||SYMBOLTOKEN||  (subject-derived),  i.e.,\r\nthe bar is made of chocolate (note the passive).\r\n\r\nDue  to  time constraints,  we  focused  on  one relation  of  ||SYMBOLTOKEN|| which  is among the  most frequent relations\r\nan NC can express  and is present in some form in many relation inventories\r\nwarren:1978,Barker:1998,Rosario:2001,Nastase:Szpakowicz:2003,Girju:2005,Girju:2007:task4,girju09:lre,hendrickx-EtAl:2010:SemEval,Tratz:2010.\r\n\r\nIn Levi's theory, ||SYMBOLTOKEN|| means that the head of the noun compound is made up of\r\nor is  a product of  its modifier. There  are three subtypes  of this\r\nrelation (we do not attempt to distinguish between them):\r\n\r\n\r\n  (a) the modifier is a unit and the head is a configuration, e.g.,  root system;\r\n\r\n  (b) the modifier represents a material and the head is a mass or an artefact, e.g.,  chocolate bar;\r\n\r\n  (c) the head represents human collectives and the modifier specifies their membership, e.g.,  worker teams.\r\n\r\n\r\nThere  are  20  instances  of ||SYMBOLTOKEN||  in  the  appendix  of\r\nLevi:1978,  and  we  use  them   all  as  seed  NCs.  As\r\nseed  patterns,   we  use   a  subset  of   the  human-proposed\r\nparaphrasing verbs and  prepositions corresponding to these  20 NCs in\r\nthe dataset in Nakov:2008:AIMSA, where each NC is paraphrased by\r\n25-30 annotators. For  example, for chocolate bar,  we find the\r\nfollowing list  of verbs (the  number of annotators who  proposed each\r\nverb is shown in parentheses):\r\n\r\n be made of (16), contain (16), be made from (10), be composed of (7), taste like (7), consist of (5), be (3), have (2), melt into (2), be manufactured from (2), be formed from (2), smell of (2), be flavored with (1), sell (1), taste of (1), be constituted by (1), incorporate (1), serve (1), contain (1), store (1), be made with (1), be solidified from (1), be created from (1), be flavoured with (1), be comprised of (1).\r\n\r\n\r\n\r\nAs we can see, the most frequent patterns are of highest quality, e.g., be made of (16),\r\nwhile the less frequent ones can be wrong, e.g., serve (1).\r\nTherefore, we filtered out all verbs that were proposed less than five\r\ntimes with  the 20 seed  NCs. We  further removed the  verb be,\r\nwhich is too general, thus ending  up with 18 seed patterns. Note that\r\nsome patterns  can paraphrase multiple  NCs: the total number  of seed\r\nNC-pattern pairs is 84.\r\n\r\nThe seed NCs  and patterns are shown in  tab:seed. While some\r\npatterns, e.g., taste  like do not express  the target ||SYMBOLTOKEN|| we  kept them  anyway since  they were  proposed by\r\nseveral human  annotators and since  they do express  the fine-grained\r\nsemantics of some particular instances of that relation;\r\nthus, we thought they might be useful, even for the general relation.\r\nFor  example,  taste  like  has   been  proposed  8  times  for\r\ncandy cigarette, 7 times for  chocolate bar, and 2 times\r\nfor sugar cube,  and thus it clearly correlates  well with some\r\nseed  examples,   ven if  it  does  not express  ||SYMBOLTOKEN||  in\r\ngeneral.\r\n\r\n\r\n\r\n", {}], "Experiments and Evaluation": ["\r\n\r\nUsing  the  NCs and patterns in  tab:seed   as  initial  seeds,  we  ran\r\nour  algorithm  for  three iterations  of  loose  bootstrapping\r\nand   strict  bootstrapping,   and   for   two  iterations   of\r\nNC-only strict bootstrapping. We only performed up to three iterations\r\nbecause of the huge number of noun compounds extracted\r\nfor NC-only strict bootstrapping (which we only ran for two iterations)\r\nand because of the low number of new NCs extracted by loose bootstrapping on iteration 3.\r\nWhile we could have run strict bootstrapping for more iterations,\r\nwe opted for a comparable number of iterations for all three methods.\r\n\r\n\r\n\r\n\r\nTotal number and accuracy in %\r\nfor NCs, patterns and NC-pattern pairs extracted and retained\r\nfor each of the three methods over all iterations.\r\n\r\n\r\n\r\n\r\n\r\n\r\nEvaluation results  for up to  three iterations. For  NCs, we\r\nshow the number of unique NCs  extracted and their accuracy in %. For\r\npatterns, we  show the  number of  unique NC-pattern  pairs extracted,\r\ntheir accuracy in  %, and the number of unique  patterns retained and\r\nused  to extract  NCs on  the second  step of  the current  iteration.\r\nThe  first column  shows the  pattern filtering  thresholds used  (see\r\nsec:pattern:filtering for details).\r\n\r\n\r\n\r\n\r\n\r\nExamples of noun compounds that we have extracted are bronze bell  (be made of,  be made from)\r\nand child team  (be  composed  of,  include).\r\nExample patterns  are be fille  with (cotton bag, water cup)\r\nand use  (water sculpture, wood  statue).\r\n\r\n\r\nTables    ||SYMBOLTOKEN|| and ||SYMBOLTOKEN|| show\r\nthe    overall     results.    As    we    mentioned     in    ||SYMBOLTOKEN|| at  each iteration,  we filtered  out all\r\npatterns that were extracted less than N times or with less than M\r\nNCs.\r\nNote t at\r\nwe only used the 10 most frequent NCs per pattern as NC seeds\r\nfor NC extraction in the next iterati n\r\nof strict bootstrapping and NC-only strict bootstrapping.\r\ntab:results shows the results f r two value combinations of (N;M): (5;50) and (10;20).\r\nNote also that if some NC was extracted by  several different patterns, it was only counted once.\r\nPatterns are subject to particular NCs,\r\nand thus we show\r\n(1) the number of patterns extracted with all NCs, i.e., unique NC-pattern pairs,\r\n(2) the accuracy of these pairs,[One of the reviewers suggested that\r\nevaluating the accuracy of NC-pattern p irs could potentially conceal some of the drift of our algorithm.\r\nFor example, while water cup / be filled with is a correct NC-pattern pair,\r\nwater cup is incorrect for ||SYMBOLTOKEN|| is probably an instance of Levi's For.\r\nThus, the same bootstrapping technique evaluated against a fixed set of semantic relations\r\n(which is the more tradit onal approach)\r\ncould arguably show bootstrapping going \"off the rails\" more quickly\r\nthan what we observe here.\r\nHowever, our goal, as stated in Section ||SYMBOLTOKEN|| to find NC-specific paraphrases,\r\nand our evaluatio  methodology is more adequate with respect to this goal.]\r\nand\r\n(3) the number of unique patterns retained after filtering,\r\nwhich will be used to extract new noun compounds on the second step of the current iteration.\r\n\r\n\r\n\r\nThe above accuracies were calculated based on  human judgments\r\nby an experienced, well-trained annotator.\r\nWe also hired a second annotator for a small subset of the examples.\r\n\r\nFor NCs   the first annotator  judged whether  each NC is  an instance\r\nof  ||SYMBOLTOKEN||  All  NCs  were judged,  except  for  iteration\r\n2 of NC-only  strict  ootstrapping,  where their  number was\r\nprohibitively high and only the most frequent noun compounds extracted\r\nfor each modifier and for each  head were checked: 9,004 NCs for ||SYMBOLTOKEN|| 4,262 NCs for ||SYMBOLTOKEN|| patterns, our first annotator judged the correctness of the unique\r\nNC-pattern  pairs, i.e.,  whether the  NC is  paraphrasable with the\r\ntarget pattern.  Given the large  number of NC-pattern pairs,\r\nthe  annotator only  judged patterns with  their top  10 most\r\nfrequent NCs.  For example, if  there  ere 5 patterns  extracted, then\r\nthe NC-pattern pairs to be judged would  be no more than 5 \u00d7 ||SYMBOLTOKEN|| 50.\r\n\r\nOur  second annotator  judged  340  random examples:  100  NCs an   20\r\npatterns with their  top 10 NCs for each iteration.  The Cohen's kappa cohen:1960:kappa\r\nbetween the  two annotators is  .66 (85% initial  agreement), which\r\ncorresponds to substantial agreement Landis:1977:kappa.\r\n\r\n\r\n\r\n", {}], "Discussion": ["\r\n\r\nTables  ||SYMBOLTOKEN|| and ||SYMBOLTOKEN||  show  that\r\nfixing one of the two nouns in the pattern, as  in\r\nstrict  bootstrapping and  NC-only strict  boots rapping,\r\nyields significantly higher  accuracy ||SYMBOLTOKEN|| test) for  both NC and\r\nNC-pattern pair extraction compared to loose bootstrapping.\r\n\r\nThe  accuracy for NC-only strict bootstrapping  is a\r\nbit higher  than for strict bootstrapping,\r\nbut the actual differences are probably smaller\r\nsince the evaluation of the former on  iteration  2 was done for the  most frequent NCs, which are more accurate.\r\n\r\nNote that the number of extracted NCs is much higher with the strict\r\nmethods because of the higher number of possible instantiations of the\r\ngeneralized query  patterns. For NC-only  strict bootstrapping,\r\nthe number  of extracted NCs  grows exponentially since the  n mber of\r\npatterns does not diminish as in the other two methods.\r\nThe number of extracted patterns  is similar for the different methods\r\nsince we select no more than 20 of them per iteration.\r\n\r\nOverall, the accuracy for all  methods decreases from one iteration to\r\nthe next since errors \r\naccumulate;\r\nstill, the degradation is slow.\r\nNote also the exception of loose bootstrapping on iteration 3.\r\n\r\nComparing the results  for ||SYMBOLTOKEN|| and ||SYMBOLTOKEN|| we can  see that, for all\r\nthree  methods,  using  the  latter  yields  a  sizable  drop  in  the\r\nnumber  of  extracted NCs  and  NC-pattern  pairs;  it also  tends  to\r\nyield a  slightly improved accuracy.  Note, however, the  exception of\r\nloose  bootstrapping for  the first  two iterations,  where the\r\nless restrictive ||SYMBOLTOKEN|| is more accurate.\r\n\r\n\r\nNumber of extracted noun compounds and accuracy in % for the method of Kim:2007.\r\nThe abbreviations Syn., Hyp., and Sis.\r\nindicate using synonyms, hypernyms, and sister words, respectively.\r\n\r\n\r\n\r\n\r\nAs  a comparison,  we  implemented the  method of  Kim:2007,\r\nwhich generates  new semantically interpreted NCs  by replacing either\r\nthe  head  or the  modifier  of  a  seed  NC with  suitable  synonyms,\r\nhypernyms  and  sister  words  from WordNet,\r\nfollowed by similarity filtering using WordNet::SimilarityPedersen:2004:wordnetsim.\r\n\r\nThe  results  for  three\r\nbootstrapping iterations using the same list of 20 initial seed NCs as\r\nin our previous  experiments, are shown in  tab:kim07. We can\r\nsee that the overall accuracy of  their method is slightly better than\r\nours. Note, however, that our method  acquired a much larger number of\r\nNCs, while  allowing more variety  in the NC semantics.  Moreover, for\r\neach extracted noun compound, we also generated a list of fine-grained\r\nparaphrasing verbs.\r\n\r\n\r\n\r\n\r\n", {}], "Error Analysis": ["\r\n\r\nBelow we analyze the errors of our method.\r\n\r\nMany problems were  due to wrong POS assignment. For  example, on Step\r\n2, because  of the omission  of that in  \"the statue\r\nhas such  high quality gold  (that) demand is  ...\", demand\r\nwas tagged as a  noun and thus extracted as an  NC modifier instead of\r\ngold. The problem also arose on Step 1, where we used WordNet\r\nto check whether  the NC candidates were composed of  two nouns. Since\r\nwords like clear,  friendly, and single are\r\nlisted in  WordNet as nouns (which  is possible in some  contex s), we\r\nextracted  wrong NCs  such  as  clear cube,  friendly\r\nteam,  and  single chain.  There  were  similar issues  with\r\nverb-particle constructions since some particles  can be used as nouns\r\nas well, e.g., give back, break down.\r\n\r\nSome errors were due to semantic transparency issues,\r\nwhere the syntactic and the semantic head of a target NP were mismatched\r\nFillmore:2002:Seeing:arguments,Fontenelle:1999.\r\nFor  example, from  the  sentence  \"This wine  is  made from  a\r\nrange of  white grapes.\", we would  extract range\r\nrather than grapes as the potential modifier of wine.\r\n\r\nIn some  cases, the NC-pattern  pair was correct,  but the NC  did not\r\nexpress the  target relation, e.g.,  while contain is  a good\r\nparaphrase for  toy box, the  noun compound itself is  not an\r\ninstance of ||SYMBOLTOKEN|| were also cases where the pair of extracted nouns did not make a\r\ngood NC,  e.g., worker work  or year toy.  Note that\r\nthis is despite  our checking that the candidate NC  occurred at least\r\n100  times in  the Google  Web  1T 5-gram  corpus (see  ||SYMBOLTOKEN|| We hypothesized that such bad NCs would tend to\r\nhave  a low  collocation  strength. We  tested  this hypothesis  using\r\nthe  Dice  coefficient,  calculated  using  the  Google  Web  1T\r\n5-gram corpus. fig:compositionality2 shows  a plot of the NC\r\naccuracy vs. collocation strength for strict bootstrapping ||SYMBOLTOKEN||  ||SYMBOLTOKEN|| for  all three  iterations (the  results for  the other\r\nexperiments  show a  similar  trend).  We can  see  that the  accuracy\r\nimproves slightly as the collocation strength increases:\r\ncompare the left and the right ends of the graph\r\n(the results are mixed in the middle though).\r\n\r\n\r\n\r\n\r\n\r\n\r\n", {}], "Conclusion  nd Future Work": ["\r\n\r\nWe have  presented a framework  for building  a very large  dataset of\r\nnoun compounds  expressing a given target  abstract semantic relation.\r\nFor  each  extracted  noun  compound,  we  generated  a  corresponding\r\nfine-grained  semantic interpretation:  a frequency  distribution over\r\nsuitable paraphrasing verbs.\r\n\r\nIn  future work,  we  plan to  apply our  framework  to the  remaining\r\nrelations in  the inventory of Levi:1978,  and to release\r\nthe  resulting dataset  to  the research  community.  We believe  that\r\nhaving a large-scale  dataset of noun compounds  interpreted with both\r\nfine-  and coarse-grained  semantic  relations would  be an  important\r\ncontribution to  the debate  about which representation  is preferable\r\nfor different tasks. It should also help the overall advancement of the\r\nfield of noun compound interpretation.\r\n\r\n\r\n", {}], "Acknowledgments": ["\r\n\r\nThis research is partially supported (for the second author) by the SmartBook project, funded by the Bulgarian National Science Fund under Grant D002-111/15.12.2008.\r\n\r\nWe would like to thank the anonymous\r\nreviewers for their detailed and constructive\r\ncomments, which have helped us improve the paper.\r\n\r\naclncpattern\r\n\r\n\n", {}]}	Responding  to the  need  for semantic  lexical  resources in  natural language processing  applications, we examine methods  to acquire noun compounds  (NCs), e.g.,  orange juice,  together with  suitable fine-grained  semantic  interpretations, e.g.,  squeezed  from, which  are directly  usable  as paraphrases.  We employ  bootstrapping and  web statistics,  and  utilize the  relationship  between NCs  and paraphrasing  patterns to  jointly extract  NCs and  such patterns  in multiple alternating  iterations. In evaluation, we  found that having one compound  noun fixed yields  both a higher number  of semantically interpreted  NCs  and  improved  accuracy  due  to  stronger  semantic restrictions.
