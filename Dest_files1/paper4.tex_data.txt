

math_commands.tex

lightgreyrgb0.43,0.43,0.43crimsonrgb0.86,0.08,0.24




@#!T
Contrastive Learning of Structured 
@#^T

 World Models

Thomas Kipf 

University of Amsterdam 
t.n.kipf@uva.nl

Elise van der Pol 

University of Amsterdam 

UvA-Bosch Delta Lab 
e.e.vanderpol@uva.nl

Max Welling 

University of Amsterdam 

CIFAR
m.welling@uva.nl










@#!A

A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastivelytrained Structured W rld Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the l arning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by  n agent, simple Atari games, and a multi-object phys cs simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highl  structured environments, while learning interpretable object-based representations.

@#^A




@#!S
Introduction
@#^S

Compositional reasoning in terms of objects, relations, and actions is a central ability in human cognition spelke2007core. This ability serves as a core motivation behind a range of recent works that aim at enriching machine learning models with the ability to disentangle scenes into objects, their properties, and relations between them  chang2016compositional,battaglia2016interaction,watters2017visual,van2018relational,kipf2018neural,sun2018actor,sun2019relational,xu2019unsupervised. These structured neural models greatly facilitate predicting physical dynamics and the consequences of actions, and provide a strong inductive bias for generalization to novel environment situations, allowing models to answer counterfactual questions such as "What would happen if I pushed this block instead of pulling it?".

Arriving at a structured description of the world in terms of objects and relations in the first place, however, is a challenging problem. While most methods in this area require some form of human annotation for the extraction of objects or relations, several recent works study the problem  f object discovery from visual data in a completely unsupervised or self-supervised manner eslami2016attend,greff2017neural,nash2017multi,van2018relational,kosiorek2018sequential,janner2018reasoning,xu2019unsupervised,burgess2019monet,greff2019multi,engelcke2 19genesis. These methods follow a generative approach, i.e., they learn to discover object-based representations by performing visual predictions or reconstruction and by optimizing an objective in pixel space. Placing a loss in pixe  space requires carefully trading off structural constraints on latent variables vs. accuracy of pixel-based reconstruction. Typical failure modes include ignoring visually small, but relevant features for predicting the future, such as a bullet in an Atari game kaiser2019model, or wasting model capacity on visually rich, but otherwise potentially irrelevant features, such as static backgrounds.

To avoid such failure modes, we propose to adopt a discriminative approach using contrastive learning, which scores real against fake experiences in the form of state-action-state triples from an experience buffer lin1992self, in a similar fashion as typical graph embedding approaches score true facts in the form of entity-relation-entity triples against corrupted triples or fake facts.

We introduce Contrastively-trained S ructured World Models (C-SWMs), a class of models for learning abstract state representations from observations in an environment. C-SWMs learn a set of abstract state variables, one for each object in a particular observation. Environment transitions are modeled using a graph neural network scarselli2009graph,li2015gated,kipf20 6semi,gilmer2017neural,battaglia2018relationa  that operates on latent abstract representations.

This paper further introduces a novel object-level contrastive loss for unsupervised learning of object-based representations. We arrive at this formulation by adapting methods for learning translational graph embeddings bordes2013translating,wang2014knowledge to our use case. By establishing a connection between contrastive learning of state abstractions franccois2018combined,thomas2018disentangling and relational graph emb ddings nickel2016review, we hope to prov de inspiration and  uidance for future model improvements in both fields.

In a set of experiments, where we use a novel ranking-based evaluation strategy, we demonstrate that C-SWMs learn interpretable object-level state abstractions, accurately learn to predict state transitions many steps into the future, demonstra e combinatorial generalization to novel environment configurations and learn to identify objects fro  scenes without supervision.


@#!S
Structured World Models
@#^S

Our goal is to learn an object-oriented abstraction of a particular observation or environment state. In addition, we would like to learn an action-conditioned transition model of the environment that takes object representations and their relations and interactions into acco nt.

We start by introducing the general framework for contrastive learning of state abstractions and transition mo els without object factorization in Sections ||SYMBOLTOKEN|| and in the following describe a variant that utilizes object-factorized state representations, which we term a Structured World Model.


@#S!S
State Abstraction
@#S^S

We consider an o fpolicy setting, where we operate solely on a buffer of offline experience, e.g., obtained from an exploration policy. Formally, this experience buffer ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| contains T tuples of states ||SYMBOLTOKEN|| actions ||SYMBOLTOKEN|| and follow-up states ||SYMBOLTOKEN|| which are reached after taking action ||SYMBOLTOKEN|| We do n t consider rewards as part of our framework for simplicity.

Our goal is to learn abstract or latent representations ||SYMBOLTOKEN|| of environment states ||SYMBOLTOKEN|| that discard any information which is not necessary to predict the abstract representation of the follow-up state ||SYMBOLTOKEN|| after taking action ||SYMBOLTOKEN|| Formally, we have an encoder E: S→Z which maps observed states to abstract state representations and a transition model T: Z×A→Z operating solely on abstract state representations.


@#S!S
Contrastive Learning
@#S^S

Our starting point is the graph embedding method TransE bordes2013translating: TransE embeds facts from a knowledge base ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| which consists of entity-relation-entity triples ||SYMBOLTOKEN|| r_t, ||SYMBOLTOKEN|| where ||SYMBOLTOKEN|| is the subject entity (analogous to the source state ||SYMBOLTOKEN|| in our case), ||SYMBOLTOKEN|| is the relation (analogous to the action ||SYMBOLTOKEN|| in our experience buffer), and ||SYMBOLTOKEN|| is the object entity (analogous to the target state ||SYMBOLTOKEN|| defines the energy of a triple ||SYMBOLTOKEN|| r_t, ||SYMBOLTOKEN|| as H ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| F(o_t)), where F (and G) are embedding functions that map discrete entities (and relations) to ||SYMBOLTOKEN|| where D is the dimensionality of the embedding space, and d(·,·) denotes the squared Euclidean distance. Training is carried out with an energy-based hinge loss lecun2006tutorial, with negative samples obtained by replacing the entities in a fact with random entities from the knowledge base.

We can port TransE to our setting with only minor modifications. As the effect of an action is in general not independent of the source state, we replace ||SYMBOLTOKEN|| with ||SYMBOLTOKEN|| a_t), i.e., with the transition function, conditioned on both the action and the (embedded) source state via ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| The overall energy of a state-action-state triple then can be defined as follows: H ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| a_t), ||SYMBOLTOKEN|| additive form of the transition model provides a strong inductive bias for modeling effects of actions in the environment as translations in the abstract state space. Alternatively, one could m del effects as linear transformations or rotations in the abstract state space, which motivates the use of a graph embedding method such as RESCAL nickel2011three, CompleX trouillon2016complex, or HolE nickel2016holographic.

With the aforementioned modifications, we arrive at the following energy-based hinge loss:
L ||SYMBOLTOKEN||   ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| a_t), ||MATHEQUATION|| ||SYMBOLTOKEN|| - ||SYMBOLTOKEN|| ||SYMBOLTOKEN||  ,

defined for a single ||SYMBOLTOKEN|| a_t, ||SYMBOLTOKEN|| with a corrupted abstract state ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| s̃_t is sampled at random from the experience buffer. The margin ||SYMBOLTOKEN|| is a hyperparameter for which we found ||SYMBOLTOKEN|| to be a good choice. Unlike  bordes2013translating, we place the hinge only on the negative term instead of on the full loss and we do not constrain the norm of the abstract states ||SYMBOLTOKEN|| which we found to work better in our context (see Appendix ||S MBOLTOKEN|| The overall loss is to be understood as an expectation of the above over samples from the experience buffer B.


@#S!S
Object-oriented State Fa torization
@#S^S

Our goal is to take into account the compositional nature of visual scenes, and hence we would like to learn a relational and object-oriented model of the environment that operates on a factored abstract state space ||SYMBOLTOKEN|| where K is the number of available object slots. We further assume an object-factori ed action space ||SYMBOL OKEN|| This factorization ensures that each object is independently represented an  it allows for efficient sharing of model parameter  across objects in the  ransition model. Thi  serves as a strong inductive bias for better generalization to novel scenes and facilitates learning and object discovery. The overall C-SWM model architecture using object-factorized representations is shown in Figure || YMBOLTOKEN|| g r a p h i c s ||SY BOLTOKEN|| C-SWM model is composed of the following components: 1) a CNN-based o ject extractor, 2) an MLP-based object encoder, 3) a GNN-based relational transition model, and 4) an object-factorized contrastive loss. Colored blocks denote abstract states for a particular object. 
@@@FIGURE@@@


Encode  and Object Extractor
We split the encoder into two separate modules: 1) a CNN-based object extractor ||SYMBOLTOKEN|| and 2) an MLP-based object encoder ||SYMBOLTOKEN|| The object extractor module is a CNN operating directly on image-based observations from the environment with K feature maps in its last layer. Each feature map ||MATHEQUATION|| can be interpreted as an object mask corresponding to one particular object slot, where ||SYMBOLTOKEN|| denotes selection of the k-th feature map. For simplicity, we only assign a single feature map per object slot which sufficed for the experiments considered in this work (see Appendix ||SYMBOLTOKEN|| To allow for encoding of more complex object features (other than, e.g., position/velocity), the object extractor can be adapted to produce multiple feature maps per object slot. After the object extractor module, we flatten each feature map ||SYMBOLTOKEN|| (object mask) and feed it into the object encoder ||SYMBOLTOKEN|| The object encoder shares weights across objects and returns an abstract state representation: ||MATHEQUATION|| with ||SYMBOLTOKEN|| We set ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| R^D in the following, where D is a hyperparameter.

Relational Transition Model
We implement the transition model as a graph neural network scarselli2009graph,li2015gated,kipf2016semi,battaglia2016interaction,gilmer2017neural,battaglia2018relational, which allows us to model pairwise interactions between object states while being invariant to the order in which objects are represented. After the encoder stage, we have an abstract state description ||SYMBOLTOKEN|| and an action ||SYMBOLTOKEN|| for every object in the scene. We represent actions as one-hot vectors (or a vector of zeros if no action is applied to a particular object), but note that other choices are possible, e.g., for continuous action spaces. The transition function then takes as input the tuple of object representations ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ..., ||SYMBOLTOKEN|| and actions ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ..., ||SYMBOLTOKEN|| at a particular time ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| a_t) ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN||  ||SYMBOLTOKEN|| a_t) is implemented as a graph neural network (GNN) that takes ||SYMBOLTOKEN|| as input node features. The model predicts updates ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| The object representations for the next time step are obtained via ||MATHEQUATION|| + ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ..., ||MATHEQUATION|| ||SYMBOLTOKEN|| The GNN consists of node update functions ||SYMBOLTOKEN|| and edge update functions ||SYMBOLTOKEN|| with shared parameters across all nodes and edges. These functions are implemented as MLPs and we choose the following form of message passing ||SYMBOLTOKEN|| &= ||SYMBOLTOKEN|| 
Δ ||SYMBO TOKEN|| &= ||SYMBOLTOKEN|| j ||SYMBOLTOKEN|| j)])   ,

where ||SYMBOLTOKEN|| is an intermediate representation of the edge or interaction between nodes i and j. This corresponds to a single round of node-to-edge and edge-to-node message pa sing. Alternatively, one could apply multiple rounds of message passing, but we did not find this to be necessary for the experiments considered in this work. Note that this update rule corresponds to message passing on a fully-connected scene graph, which is ||SYMBOLTOKEN|| This can be reduced to linear complexity by reducing connectivity to nearest neighbors in the abstract state space, which we leave for future work. We denote the output of the transition function for  he kth object as ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| a_t) in the following.

Multi-object Contrastive Loss
We only need to change the energy function to take the factorization of the abstract state space into account, which yields the following energy H for positive triples and H̃ for negative samples:

H ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||MATHEQUATION|  ||SYMBOLTOKEN|| ||SYMBOLTOKEN||  ,   H̃ ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN||  ,

where ||SYMBOLTOKEN|| is the k-th object representation of the negative state sample ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| The overall contrastive loss for a si gle stateaction-state sample from the experience buffer then takes the form:
L ||SYMBOLTOKEN|| H ||SYMBOLTOKEN|| (0, ||SYMBOLTOKEN|| - H̃)  .




@#!S
Related Work
@#^S

For coverage of related work in the area of object discovery with autoencoder-based models, we refer the reader to the Introduction section. We further discuss related work on relational graph embeddings in Section ||SYMBOLTOKEN|| Models of Environments
Recent work on modeling structured environments such as interacting multi-object or multi-agent systems has made great strides in improving predictive accuracy by explicitly taking into account the structured nature of such systems sukhbaatar2016learning,chang2016compositional,battaglia2016interaction,watte s2017visual,hoshen2017vain,wang2018nervenet,van2018relational,kipf2018neural,sanchez2018graph,xu2019unsupervised  These methods generally make use of some form of graph neural network, where node update functions model the dynamics of individual objects, parts or agents and edge update functions model their int ractions and relations. Several recent works succeed in learning such structured models directly from pixels watters2017visual,van2018relational,xu2019unsupervised,watters2019cobra, but in contrast to our work rely on pixel-based loss functions. The latest example in this line of research is the COBRA model watters2019cobra, which learns an action-conditioned transition policy on object representations obtained from an unsupervised object discovery model burgess2019monet. Unlike C-SWM, COBRA does not model interactions between object slots and relies on a pixel-based loss for training. Our object encoder, however, is mo e limited and utilizing an iterative object encoding process such as in MONet burgess2019monet would be interesting for future work.

Contrastive Learning
Contrastive learning m thods are widely used in the field of graph representation learning bordes2013translating,perozzi2014deepwalk,grover2016node2vec,bordes2013translating,schlichtkrull2018modeling,velivckovic2018deep, and for learning word representations mnih2012fast,mikolov2013efficient. The main idea is to construct pairs of related data examples (positive examples, e.g., connected by an edge in a graph or co-occuring words in a sentence) and pairs of unrelated or corrupted data examples (negative examples), and use a loss function that scores positive and negative pairs in a different way. Most energy-based losses lecun2006tutorial are suitable for this task. Recent works oord2018representation,hjelm2018learning,henaff2019data,sun2019contrastive,anand2019unsupervised connect objectives of this kind to the principle of learning representations by maximizing mutual information between data and learned representations, and successfully apply these methods to image, speech, and video data. 

State Representation Learning State representation learning in environments similar to ours is often approached by models based on autoencoders corneil2018efficient, watter2015embed, ha2018world, hafner2018learning, laversanne2018curiosity or via adversarial learning kurutach2018learning, wang2019learning. Some recent methods learn state representations without requiring a decoder back into pixel space. Examples include the selectivity objective in thomas201 disentangling, the contrastive objective in franccois2018combined, the distribution matching objective in gelada2019deepmdp or using causality-based losses and physical priors in latent space jonschkowski2015learning,ehrhardt2018unsupervised. Most notably, eh hardt2018unsupervised propose a method to learn an object detection module and a physics module jointly from raw video data without pixel-based losses. This approach, however, can only track a single object at a  ime and requires careful balancing of multiple loss functions.


@#!S
Experiments
@#^S

Our goal of this experimental section is to verify whether C-SWMs can 1) learn to discover object representations from environment interactions without supervision, 2) learn an accurate transition model in latent space, and 3) generalize to novel, unseen scenes. Our implementation is available under ||SYMBOLTOKEN|| evaluate C-SWMs on two novel grid world environments (2D shapes and 3D blocks) involving multiple interacting objects that can be manipulated independently by an agent, two Atari 2600 games (Atari Pong and Space Invaders), and a multi-object physics simulation (3-body physics). See Figure ||SYMBOLTOKEN|| for example observations.

For all environments, we use a random policy to collect experience for both training and evaluation. Observations are provided as 50×50×3 color images for the grid world environments and as 50×50×6 tensors (two concatenated consecutive frames) for the Atari and 3-body physics environments. Additional details on environments and dataset creation can be found in Appendix ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| Shapes
   
  ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| Blocks
   
    ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN||   
    ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN||   
    ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| observations from block pushing environments (a--b),  tari 2600 games (c--d) and a 3-body gravitational physics simulation (e). In the grid worlds (a--b), each block can be independently moved into the four cardinal directions unless the target position is occupied by another block or outside of the scene. Best viewed in color.
@@@FIGURE@@@



@#S!S
Evaluation Metrics
@#S^S

In order to evaluate model performance directly in latent space, we make use of ranking metrics, which are commonly used for the evaluation of link prediction models, as in, e.g., bordes2013translating. This allows us to assess the quality of learned representations directly without relying on auxiliary metrics such as pixel-based reconstruction losses, or performance in downstream tasks such as planning.

Given an observation encoded by the model and an action, we use the model to predict the representation of the next state, reached after taking the action in the environment. This predicted state representation is then compared to the encoded true observation after taking the action in the environment and a set of reference states (observations encoded by the model) obtained from t e experience buffer. We measu e and report both Hits at Rank 1 (H@1) and Mean Reciprocal Rank (MRR). Additional details on these evaluation metrics can be found in Appendix ||SYMBOLTOKEN|| World Models The predominant method for state representation learning is based on autoencoders, and often on the VAE kingma2013auto,rezende 014stochastic model in particular. This World Model baseline is inspired by ha2018world and uses either a deterministic autoencoder (AE) or a VAE to learn state representations. Finally, an MLP is used to predict the next state after taking an action.

Physics As Inverse Graphics (PAIG) This model by jaques2019physics is based on an encoder-decoder architecture and trained with pixel-based reconstruction losses, but uses a differentiable physics engine in the latent space that operates on explicit position and velocity representations for each object. Thus, this model is only applicable to th  3body physics environment.


@#S!S
Training and Evaluation Setting
@#S^S

We train C-SWMs on an experience buffer obtained by running a random policy on the respective environment. We choose 1000 episodes with 100 environment steps each for the grid world environments, 1000 episodes with 10 steps each for the Atari environments and 5000 episodes with 10 steps each fo  the 3body physics environment.

For evaluation, we populate a separate experience buffer with 10 environment steps per episode and a total of 10.000 episodes for the grid world environments, 100 episodes for the Atari environments and 1000 episodes for the physics environment. For the Atari environments, we minimize train/test overlap by `warm-starting' experience collection in these environments with random actions before we start populating the experience buffer (see Appendix ||SYMBOLTOKEN|| and we ensure that not a single full test set episode coincides exactly with an episode from the training set. The state spaces of the grid world environments are large (approx. 6.4M unique states) and hence train/test coincidence of a full 10-step episode is unlikely. Overlap is similarly unlikely for the physics environment which has a continuous  tate space. Hence, performing well on these tasks will require some form of generalization to new environment configurations or an unseen sequence of s ates and actions. 
All models are trained for 100 epochs (200 for Atari games) using the Adam kingma2014adam optimizer with a learning rate of 5· ||SYMBOLTOKEN|| and a batch size of 1024 (512 for baselines with decoders due to higher memory demands,  nd 100 for PAIG as suggested by the authors). Model architecture details are provided in Appendix ||SYMBOLTOKEN|| Results
@#S^S


We present qualitative results for the grid world environments in Figure ||SYMBOLTOKEN|| and for the 3-body physics environment in Figure ||SYMBOLTOKEN|| All results are obtained on hold-out test data. Further qualitative results (incl. on Atari games) can be found in Appendix ||SYMBOLTOKEN|| the grid world environments, we can observe that C-SWM reliably discovers object-specific filters for a particular scene, without direct supervision. Further, each object is represented by two coordinates which correspond (up to a rando  linear transformation) to the true object position in t e scene. Although we choose a two-dimensional latent representation per object for easier visualization, we find that results remain unchanged if we increase the dimensionality of the latent representation. The edges in this learned abstract transition graph correspond to the effect of a particular action applied to the object. The structure of the learned latent representation accurately captures the underlying grid structure of the environment. We further find that the transition model, which only has access to latent representations, correctly captures whether an action has an effect or not, e.g., if a neighboring position is blocked by another object.

Similarly, we find that the model can learn object-specific encoders in the 3-body physics environment and can learn object-specific latent representations that track location and velocity of a particular object, while learning an accurate latent transition model that generalizes well to unseen environment instances.


###FIGURE###
0.47< g r a p h i c s || YMBOLTOKEN|| object masks in a scene from the 3D cubes (top) and 2D shapes (bottom) environments.  ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN||  Learned abstract state transition graph of the yellow cube (left) and the green square (right), while keeping all other object positions fixed at test time.Discovered object masks (left) and direct visualization of the 2D abstract state spaces and transition graphs for a single object (right) in the block pushing environments. Nodes denote state embeddings obtained from a test set experience buffer with random actions and edges are predicted transitions. The learned abstract state graph clearly captures the underlying grid structure of the environment both in terms of object-specific latent states and in terms of predicted transitions, but is randomly rotated and/or mirrored. The model further correctly captures that certain actions do not have an effect if a neighboring position is blocked by another object (shown as colored spheres), even though the transition model does not have access to visual inputs. 
@@@FIGURE@@@



###FIGURE###
0.59< g r a p h i c s ||SYMBOLTOKEN|| from 3-body gravitational physics simulation (bottom) and learned abstract state transition graph for a single object slot (top).  ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| state transition graph from 50 test episodes for single object slot.Qualitative results for 3-body physics environment for a single representative test set episode (left) and for a dataset of 50 test episodes (right). The model lea ns to smoothly embed object trajectories, with the circular motion represented in the latent space (projected from four to two dimensions via PCA). In the abstract state transition graph, orange nodes denote starting states for a particular episode, green links correspond to ground truth transitions and violet links correspond to transitions predicted by the model. One trajectory (in the center) strongly deviates from typical traj ctories seen during training, and the model struggles to predict the correct trans tion.
@@@FIGURE@@@



@#S!S
Quantitative Results
@#S^S

We set up quantitative experiments for evaluating the quality of both object discovery and the quality of the learned transition model. We compare against autoencoder baselines and model variants that do not represent the environment in an object-factorized manner, do not use a GNN, or do not make use of contrastive learning. Performing well under this evaluation setting requires some degree of (combinatorial) generalization to unseen environment instances.

We report ranking scores (in %) in latent space, after encoding source and target observations, and taking steps in the latent space using the learned model. Reported results are mean and standard error of scores over 4 runs on hold-out environment instances. Results are summarized in Table ||SYMBOLTOKEN|| results for multi-step prediction in latent space. Highest (mean) scores in bold.


###TABLE###

  & & & 2c1 Step & 2c5 Steps & 2c10 Steps
(lr)4-5(lr)6-7(lr)8-9
&&Model & H@1 & MRR & H@1 & MRR & H@1 & MRR      ||SYMBOLTOKEN|| SHAPES & C-SWM& 100lightgrey±0.0 &  100lightgrey±0.0 & 100lightgrey±0.0 & 100lightgrey±0.0 & 99.9lightgrey±0.0 & 100lightgrey±0.0

&&-- latent GNN & 99.9lightgrey±0.0 & 100lightgrey±0.0 & 97.4lightgrey±0.1 & 98.4lightgrey±0.0 & 89.7lightgrey±0.3 & 93.1lightgrey±0.2

&&-- factored states & 54.5lightgrey±18.1 & 65.0lightgrey±15.9 & 34.4lightgrey±16.0  & 47.4lightgrey±16.0 & 24.1lightgrey±11.2 & 37.0lightgrey±12.1

&&-- contrastive loss & 49.9lightgrey±0.9 & 55.2lightgrey±0.9 & 6.5lightgrey±0.5 & 9.3lightgrey±0.7 & 1.4lightgrey±0.1 & 2.6lightgrey±0.2



&&World Model (AE) & 98.7lightgrey±0.5  & 99.2lightgrey±0.3 & 36.1lightgrey±8.1  & 44.1lightgrey±8.1 & 6.5lightgrey±2.6  & 10.5lightgrey±3.6

&&World Model (VAE) & 94.2lightgrey±1.0  & 96.4lightgrey±0.6 & 14.1lightgrey±1.1  & 21.4lightgrey±1.4 & 1.4lightgrey±0.2  & ||SYMBOLTOKEN|| BLOCKS & CSWM& 99.9lightgrey±0.0 & 100lightgrey±0.0 & 99.9lightgrey±0.0 & 100lightgrey±0.0 & 99.9lightgrey±0.0 & 99.9lightgrey±0.0

&&-- latent G N & 99.9lightgrey±0.0 & 99.9lightgrey±0.0 & 96.3lightgrey±0.4 & 97.7lightgrey±0.3 & 86.0lightgrey±1.8 & 90.2lightgrey±1.5

&&-- factored st tes & 74.2lightgrey±9.3 & 82.5lightgrey±8.3 & 48.7lightgrey±12.9 & 62.6lightgrey±13.0 & 65.8lightgrey±14.0 & 49.6lightgrey±11.0

&&-- contrastive loss & 48.9lightgrey±16.8 & 52.5lightgrey±17.8 & 12.2lightgrey±5.8 & 16.3lightgrey±7.1 & 3.1lightgrey±1.9 & 5.3lightgrey±2.8

&&World Model (AE) & 93.5lightgrey±0.8  & 95.6lightgrey±0.6 & 26.7lightgrey±0.7  & 35.6lightgrey±0.8 & 4.0lightgrey±0.2  & 7.6lightgrey±0.3

&&World Model ( AE) & 90.9lightgrey±0.7  & 94.2lightgrey±0.6 & 31.3lightgrey±2.3  & 41.8lightgrey±2.3 & 7.2lightgrey±0.9  & ||SYMBOLTOKEN|| & ||SYMBOLTOKEN|| C-SWM  |SYMBOLTOKEN|| 20.5lightgrey±3.5  & 41.8lightgrey±2.9 & 9.5lightgrey±2.2  & 22.2lightgrey±3.3 & 5.3lightgrey±1.6  & 15.8lightgrey±2.8

&& C-S M ||SYMBOLTOKEN|| 34.8lightgrey±5.3  & 54.3lightgrey±5.2 & 12.8lightgrey±3.4  & 28.1lightgrey±4.2 & 9.5lightgrey±1.7  & 21.1lightgrey±2.8

&& C-SWM ||SYMBOLTOKEN|| 36.5lightgrey±5.6  & 56.2lightgrey±6.2 & 18.3lightgrey±1.9  & 35.7lightgrey±2.3 & 11.5lightgrey±1.0  & 26.0lightgrey±1.2

&&World Model (AE) & 23.8lightgrey±3.3  & 44.7lightgrey±2.4 & 1.7lightgrey±0.5  & 8.0lightgrey±0.5 & 1.2lightgrey±0.8  & 5.3lightgrey±0.8

&&World M del (VAE) & 1.0lightgrey±0.0  & 5.1lightgrey±0.1 & 1.0lightgrey±0.0  & 5.2lightgrey±0.0 & 1.0lightgrey±0.0  & ||SYMBOLTOKEN|| & ||SYMBOLTOKEN|| C-SWM ||SYMBOLTOKEN||  48.5lightgrey±7.0  &  66.1lightgrey±6.6 &  16.8lightgrey±2.7  &  35.7lightgrey±3.7 &  11.8lightgrey±3.0  &  26.0lightgrey±4.1

&& C-SWM ||SYMBOLTOKEN|| 46.2lightgrey±13.0  & 62.3lightgrey±11.5 & 10.8lightgrey±3.7  & 28.5lightgrey±5.8 & 6.0lightgrey±0.4  & 20.9lightgrey±0.9

&& C-SWM ||SYMBOLTOKEN|| 31.5lightgrey±13.1  & 48.6lightgrey±11.8 & 10.0lightgrey±2.3  & 23.9lightgrey±3.6 & 6.0lightgrey±1.7  & 19.8lightgrey±3.3

&&World Model (AE) & 40.2lightgrey±3.6  & 59.6lightgrey±3.5 & 5.2lightgrey±1.1  & 14.1lightgrey± .0 & 3.8lightgrey±0.8  & 10.4lightgrey±1.3

&&World Model (VAE) & 1.0lightgrey±0.0  & 5.3lightgrey±0.1 & 0.8lightgrey±0.2  & 5.2lightgrey±0.0 & 1.0lightgrey±0.0  & ||SYMBOLTOKEN|| & ||SYMBOLTOKEN|| &  C-SWM &  100lightgrey±0.0  &  100lightgrey±0.0 &  97.2lightgrey±0.9  & 98.5lightgrey±0.5 &  75.5lightgrey±4.7  &  85.2lightgrey±3.1

&&World Model (AE) &  1 0lightgrey±0.0  &  100lightgrey±0.0 &  97.7lightgrey±0.3  &  98.8lightgrey±0.2 & 67. lightgrey±2.4  & 78.4lightgrey±1.8

&&World Model (VAE) &  100lightgrey±0.0  &  100lightgrey±0.0 & 83.1lightgrey±2.5  & 90.3lightgrey±1.6 & 23.6lightgrey±4.2  & 37.5lightgrey±4.8

&& Physics WM (PAIG) & 89.2lightgrey±3.5  & 90.7lightgrey±3.4 & 57.7lightgrey±12.0  & 63.1lightgrey±11.1 & 25.1lightgrey±13.0  & 33.1lightgrey±13.4

@@@TABLE@@@

@@@TABLE@@@


We find that baselines that make use of reconstruction losses in pixel space (incl. the C-SWM model variant without contrastive loss) typically generalize less well to unseen scenes and learn a latent space configuration tha  makes it difficult for the transition model to learn the correct transition function. See Appendix ||SYMBOLTOKEN|| for a visual analysis of such latent state transition graphs. This effect appears to be even stronger when using a VAE-based World Model, where the prior puts further constraints on the latent representations. C-SWM recovers thi  structure well, see Fig re ||SYMBOLTOKEN|| the grid-world environments (2D shapes and 3D blocks), C-SWM models latent transitions almos  perfectly, which requires taking interactions between latent representations of objects into account. Removing the interaction component, i.e., replacing the latent GNN with an object-wise MLP, makes the model insensitive to pairwise interactions and hence the ability to predict future states deteriorates. Similarly, if we remove the state factorization, the model has difficulties generalizing to unseen environment configurations.  e furth r explore variants of the gri world environments in Appendix ||SYMBOLTOKEN|| wher  1) certain actions have no effect, and 2) one object moves randomly and independent of agent actions, to test the robustness of our approach.

For the Atari 2600 experiments, we find that results can have a high variance, and that the ta k is more difficult, as both the World Model baseline and C-SWM struggle to make perfect long-term predictions. While for Space Invaders, a large number of object slots || YMBOLTOKEN|| appears to be beneficial, C-SWM achieves best results with only a single object slot in Atari Pong. This suggests that one should determine the optimal value of K based on performance on a validation set if it is not known a-priori. Using an iterative object encoding mechanism, such as in MONet burgess2019monet, would enable the model to assign `empty' slots which could improve robustness w.r.t. the choice of K, which we leave for future work.

We find that both C-SWMs and the autoencoder-based World Model baseline excel at short-term predictions in the 3-body physics environment, with C-SWM having a slight edge in the 10 step prediction setting. Under our evaluation setting, the PAIG baseline jaques2019physics underperforms using the hyperparameter setting recommended by the authors. Note that we do not tune hyperparameters of C-SWM separately for this task and use the same settings as in other environments.


@#S!S
Limitations
 #S^S
Instance Disambiguation
In our experiments, we chose a simple feed-forward CNN architecture for the object extractor module. This type of architecture cannot disambiguate multiple instances of the same object present in one scene and relies on distinct visual features or labels (e.g., the green square) for object extraction. To better handle scenes which contain potentially multiple copies of the same object (e.g., in the Atari Space Invaders game), one would require some form of iterative disambiguation procedure to break symmetries and dynamically bind individual objects to slots or object filesKahnemanTreisman84,kahneman1992reviewing, such as in the style of dynamic routing sabour2017dynamic, iterative inference greff2019multi,engelcke2019genesis or sequential masking burgess2019monet,kipf2019compile.

Stochasticity & Markov Assumption
Our formulation of C-SWMs does not take into account  tochasticity in environment transitions or observations, and hence is limited to fully deterministic worlds. A probabilistic extension of C-S Ms is an interesting avenue for future work.
For simplicity, we make the Markov assumption: stat  an  action contain all the information necessary to predict  he next state. This allows us to look at single state-action-state triples in isolation. To go beyond this limitation, one would require some form of memory mechanism, such as an RNN as part of the m del architecture, which we leave for future work.


@#!S
Conclusions
@#^S

Structured world models offer compelling advantages over pure connectionist methods, by enabling stronger inductive biases for generalization, without necessarily constraining the generality of the model: for example, the contrastively trained model on the 3-body physics environment is free to store identical representations in each object slot and ignore pairwise interactions, i.e., an unstructured world model still exists as a special case. Experimentally, we find that C-SWMs make effective use of this additional structure, likely because it allows for a transition model of significantly lower complexity, and learn object-oriented models that generalize better to unseen situations.

We are excited about the prospect of using C-SWMs for model-based planning and reinforcement learning in future work, where object-oriented representations will likely allow for more accurate counterfactual reasoning about effects of actions and novel interactions in the environment. We further hope to inspire future work to think beyond autoencoder-based approaches for object-based, structured representation learning, and to address some of the limitations outlined in this paper.


@#!S
Acknowledgements
@#^S

We would like to thank Marco Federici and Adam Kosiorek
for helpful discussions. We would further like to thank the anonymous ICLR 2020 reviewers for valuable feedback. T.K. acknowledges funding by SAP ||SYMBOLTOKEN|| Results and Discussion
@#^S

@#S!S
Object-specific  epresentations
@#S^S

We visualize abstract state transition graphs separated by object slot for the 3D cubes environment in Figure ||SYMBOLTOKEN|| Discovered object representations in the 2D shapes dataset (not shown) are qualitatively very similar. We apply the same visualization technique to the model variant without contrastive loss, which is instead trained with a decoder model and a loss in pixel space. See Figure ||SYMBOLTOKEN|| for this baseline and note that the regular structure in the latent space is lost, which makes it difficult for the transition model to learn transitions which generalize to unseen environment instances.


Qualitative results for the 3-body physics dataset are summarized in Figures ||SYMBOLTOKEN|| and ||SYMBOLTOKEN|| for two different random seeds.


###FIGURE###
0.18< g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot 5.Abstract state transition graphs per object slot for a trained C- WM model on the 3D cubes environment (with all objects allowed to be moved, i.e., none are fixed in place). Edge color denotes action type. The abs ract state graph is nearly identical for each object, which illustrates that the model successfully represents objects in the same manner despite their visual differences.
@@@FIGURE@@@




###FIGURE###
0.18< g r a p h i c s ||SYMBOLTOKEN|| slot ||SYM OLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot 5.Abstract state transition graphs per object slot for a trained SWM model without contrastive loss, using instead a loss in pixel space, on the 3D cubes environment. Edge color denotes action type.
@@@FIGURE@@@



###FIGURE###
0.38< g r a p h i c s ||SYMBOLTOKEN|| object-specific ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot 3.Object filters (left) and abstract state transition graphs per object slot (right) for a trained C-SWM model on unseen test inst nces of the 3body physics environment (seed 1).
@@@FIGURE@@@



###FIGURE###
0.38< g r a p h i c s ||SYMBOLTOKEN|| object-specific ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|  g r a p h i c s ||SYMBOLTOKEN|| slot 3.Object filters (left) and abstract state transition graphs per object slot (right) for a trained C-SWM model on unseen test inst nces of the 3body physics environment (seed 2).
@@@FIGURE@@@


For the Atari 2600 environments, we generally found latent object representations to be less interpretable. We attribute this to the fact that a) objects have different roles and are in gene al not exchangeable (in contrast to the block pushing grid world environments and the 3-body physics environment), b) actions affect only one object directly, but many other objects indirectly in two conse utive frames, and c) due to multiple objects in one scene sharing the same visual features. See Figure | SYMBOLTOKEN|| for an example of learned representations in Atari Pong and Figur  ||SYMBOLTOKEN|| for an example in Space Invaders.


###FIGURE###
0.38< g r a p h i c s ||SYMBOLTOKEN|| object-specific ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot 3.Object filters (left) and abstract state transition graphs per object slot (right) for a trained C-SWM model with ||SYMBOLTOKEN|| object slots on unseen test instances of the Atari Pong environment.
@@@FIGURE@@@



###FIGURE###
0.38< g r a p h i   s ||SYMBOLTOKEN|| object-specific ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| slot 3.Obj ct filters (left) and abstract state transition graphs per object slot (right) for a trained C-SWM model with ||SYMBOLTOKEN|| object slots on unseen test instances of the Space Invaders environment.
@@@FIGURE@@@



###FIGURE###
< g r a p h i c s ||SYMBOLTOKEN|| model comparison in pixel space on a hold-out test instance of the 2D shapes environment. We train a separate decoder model for 100 epochs on both the C-SWM and the World Model baseline using all training environment instances to obtain pixel-based reconstructions for multiple prediction steps into the future. 
@@@FIGURE@@@



@#S!S
Model Comparison in Pixel Space
@#S^S

To supplement our model comparison in latent space using ranking metrics, we here show a direct comparison in pixel spac  at the example of the 2D shapes environment. This requires training a separate decoder model for C-SWM. For fairness of this comparison, we use the same protocol to train a separate decoder for the World Model (AE) baseline (discarding the one obtained from the original end-to end autoencoding training procedure). This decoder has the same architecture as in the other baseline models and is trained for 100 ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| model comparison in pixel space on a  oldout test set of the 2D shapes environment. The pl t shows mean squa ed reconstruction error (MSE) in pixel space for multiple transition model prediction steps into the future (lower is better), averaged over 4 runs. Shaded area denotes standard error. 

For a qualitative comparison, see Figure ||SYMBOLTOKEN|| The C-SWM model, as expected from our ranking analysis in latent space, performs almost perfectl  at this task. Although the World Model (AE) baseline makes clear mistakes which compound over time, it nonetheless often gets several object positions correct after many time steps. The ranking loss in latent space captures this behaviour well, and, for example, assigns an almost perfect score for 1-step prediction to the Worl  Model (AE) baseline. The typically used mean-squared error (MSE) in pixel space (see Figure ||SYMBOLTOKEN|| however, differs by several orders of magnitude between the two models and does not capture any of the nuanced differences in qualitative predictive behavior. We hence strongly encourage researchers in this area to consider ranking-based evaluations directly in latent space in addition to, or as a replacement for comparisons in pixel space.





@#S!S
Hinge Loss
@#S^S

In Table ||SYMBOLTOKEN|| we summarize results of a comparison between our multi-object contrastive loss from Eq. ||SYMBOLTOKEN|| denoted by C-SWM (original), and a full-hinge (or triplet) loss that places the hinge over both the positive and the negative energy term, i.e., L ||SYMBOLTOKEN|| (0,γ ||SYMBOLTOKEN|| H - H̃). The latter loss is similar to the loss employed in bordes2013translating, but without imposing a norm constraint on the embeddings ||SYMBOLTOKEN|| Reported results are mean and standard error over 4 runs on hold-out environment instances.


###TABLE###
Comparison of hinge loss variants on 2D shapes environment. Numbers denote ranking scores (in %) for multi-step prediction in latent space. Highest (mean) scores in bold.
###TABLE###

   & 2c1 Step & 2c5 Steps & 2c10 Steps
(lr)2-3(lr)4-5(lr)6-7
Model & H@1 & MRR & H@1 & MRR & H@1 & MRR      
 C-SWM (original)& 100lightgrey±0.0 &  100lightgrey±0.0 & 100lightgrey±0.0 & 100lightgrey±0.0 & 99.9lightgrey±0.0 & 100lightgrey±0.0

with full hinge ||SYMBOLTOKEN|| & 95.9lightgrey±1.0 & 97.8lightgrey±0.6 & 40.4lightgrey±5.9  & 54.4lightgrey±5.8 & 12.6lightgrey±4.0  & 21.8lightgrey±5.9

with full hinge ||SYMBOLTOKEN|| & 97.7lightgrey±0.9  & 98.8li htgrey±0.5 & 52.0lightgrey±2.5  & 65.7lightgrey±2.6 & 15.8lightgrey±2.9  & 26.8lightgrey±3.5

with full hinge ||SYMBOLTOKEN|| & 97.0lightgrey±0.4  & 98.5lightgrey±0.2 & 49.5lightgrey±1.5  & 63.3lightgrey±1.4 & 18.7lightgrey±1.2  & 30.3lightgrey±1.4

@ @TABLE@@@

@@@TABLE@@@


We find that placing the hinge only on the negative energy term significantly improves results, likely because the full hinge  oss can be minimized by simply increasing the norms of the embeddings ||SYMBOLTOKEN|| as outlined in bordes2013translating. This could be addressed by introducing additional constraints, such as by forcing the embeddings ||SYMBOLTOKEN|| to lie on the unit-hypersphere, as done in TransE bordes2013translating, which we do not consider in this work as most of the environment dynamics in our experiments obey a flat Euclidean geometry (e.g., translation on a grid).



@#S!S
Multiple Feature Maps per Object Slot
@#S^S

We experimented with assigning multiple feature maps per object slot to investigate whether this can improve performance on the Space Invaders environment (as an example of one of the more complex environments considered in this work). All other model settings are left unchanged. Results are summarized in Table ||SYMBOLTOKEN|| The original model variant with one feature map per object slot (using a total of ||SYMBOLTOKEN|| slots) is denoted by C-SWM (original . Reported results are mean and standard error over 4 runs o  holdout environment instances.


###TABLE###
Comparison of model variants with multiple feature maps per object on the Space Invaders environment. Numbers denote ranking score  (in %) for multi-step prediction in latent space.
###TABLE###

   & 2c1 Step & 2c5 Steps & 2c10 Steps
(lr)2-3(lr)4-5(lr)6-7
Model & H@1 & MRR & H@1 & MRR & H@1 & MRR      
 C-SWM (original)& 48.5lightgrey±7.0  &  66.1lightgrey±6.6 & 16.8lightgrey±2.7  &  35.7lightgrey±3.7 & 11.8lightgrey±3.0  &  26.0lightgrey±4.1

with 2 feature maps & 48.2lightgrey±7.1  & 65.7lightgrey±6.8 & 12.8lightgrey±3.0  & 27.7lightgrey±3.2 & 8.2lightgrey±2.9  & 21.2lightgrey±3.4

with 5 feature maps & 52.5lightgrey±3.4  & 71.0lightgrey±2.6 & 18.2lightgrey±4.3  & 36.3lightgrey±4.6 & 11.8lightgrey±2.5  & 25.0lightgrey±3.4

with 10 feature maps & 50.0lightgrey±0.9  & 68.6lightgrey±1.0 & 15.8lightgrey±1.4  & 32.4lightgrey±2.3 & 9.0lightgrey±1.4  & 22.7light rey±2.3

@@@TABLE@@@

@@@TABLE@@@

We  ind that there is no clear advantage in using multiple feature maps per object slot for the Space Invaders environment, but this might nonetheless prove useful for environments of more complex nature.


@#S!S
Grid-World Environment Variants
@#S^S

T  further evaluate the robustness of our approach, we experimented with two variant  of the 2D shapes grid-world environment with five objects.
In the first variant, termed no-op action, we include an additional `no-op' action for each obj ct that has no effect. This mirrors the setting present in many Atari 2600 benchmark environments that have an explicit no-op actio .
In the second variant, termed random object, we assign a special property to one out of the five objects: a random action is applied to this object at every turn, i.e., the agent only has control over the other four objects and in every environment step, a total of two objects receive an action (one by the agent, which only affects one out of the first four objects, and one random action on the last object). This is to test how the model behaves in the presence of other agents that act randomly. As our model is fully deterministic, it is to be expected th t adding a source of randomness in the environment will pose a challenge.

We report results in Table ||SYMBOLTOKEN|| Reported numbers are mean and standard error over 4 runs on hold-out environment instances. We find that adding a no-op action has little effect on our model, but adding a randomly moving object reduces predictive performance. Performance in the latter case could potentially be improved by explicitly modeling stochasticity in the environment.


###TABLE###
C mparison of C-SWM on variants of the 2D shapes environment. Nu bers denote ranking scores (in %) for multi-step predic ion in latent space.
###TABLE###

   & 2c1 Step & 2c5 Steps & 2c10 Steps
(lr)2-3(lr)4-5(lr)6-7
Model & H@1 & MRR & H@1 & MRR & H@1 & MRR      
 C-SWM (original env.)& 100lightgrey±0.0  & 100lightgrey±0.0 & 100lightgrey±0.0  & 100lightgrey±0.0 & 99.9lightgrey±0.0  & ||SYMBOLTOKEN|| no-op action & 100lightgrey±0.0  & 100lightgrey±0.0 & 100lightgrey±0.0  & 100lightgrey±0.0 & 99.9lightgrey±0.0  & ||SYMBOLTOKEN|| random object & 98.4lightgrey±0.1  & 99.2lightgrey±0.0 & 95.9lightgrey±0.2  & 97.6lightgrey±0.1 & 90.4lightgrey±0.3  & 93.7lightgrey±0.2

@@@TABLE@@@

@@@TABLE@@@



@#S!S
Stability
@#S^S

The discovered obj ct representations and identifications can vary between runs. While we found that this process is very stable for the simple grid world environments where acti ns only affect a single object, we found that results can be initialization-dependent on the Atari e vironments, where actions can have effects across all objects, and the 3-body physics simulation (see Figures ||SYMBOLTOKEN|| and ||SYMBOLTOKEN|| which does not have any actions. In some cases, the discovered representation can be less suitable for forward prediction in time or for  eneralization to novel scenes, which explains the variance in some of our results on these datasets.



@#S!S
Training Time
@#S^S

We found that the overall training time of the C-SWM model was comparable to that of the World Model baseline. Both C-SWM and the World Model baseline trained for approx. 1 hour wall-clock time on the 2D sha es dataset, approx. 2 hours on the 3D cubes dataset, and approx. 30min on the 3-body physics environment using a single NVIDIA GTX1080Ti GPU. The Atari Pong and Space Invaders models trained for typically less than 20 minutes. A notable exception is the PAIG baseline model jaques2019physics which trained for approx. 6 hours on a NVIDIA TitanX Pascal GPU using the recommended settings by the authors of the paper.


@#!S
Datasets
@#^S

@#S!S
Grid Worlds
@#S^S

To generate an experience buffer for training, we initialize the environment with random object placements and uniformly sample an object and an object-specific action at every time step.

We provide state observations as 50× 50× 3 tensors with RGB color channels, normalized to [0, 1]. Actions are provided as a 4-dim one-hot vector (if an action is applied) or a vector of zeros per object slot in the environment. The action one-hot vector encodes the directional movement action applied to a particular object, or is represented as a vector of zeros if no action is applied to a particular object. Note that only a single object receives an action per time step. For the Atari environments, we provide a copy of the one-hot encoded action vector to every object slot, and for the 3-body physics environment, which has no ac ions, we do not provide an action vector.

2D Shapes
This environment is a 5× 5 grid world with 5 different objects placed at random positions. Each location can only be occupied by at maximum one object. Each object is represented by a unique shape/color combination, occupying 10× 10 pixels on the overall 50 × 50 pixel grid. At each time step, one object can be selected and moved by one position along the four cardinal directions. See Figure ||SYMBOLTOKEN|| for an example. The action has no effect if the target location in a particular direction is occupied by another object or outside of the 5× 5 grid. Thus, a learned transition model needs to take pairwise interactions between object properties (i.e., their locations) into account, as it would otherwise be unable to predict the effect of an action correctly.

3D Blocks
To investigate to what degree our model is robust to partial occlusion and perspective changes, we implement a simple block pushing environment using Matplotlib hunter2007matplotlib as a rendering tool. The underlying environment dynamics are the same as in the 2D Shapes dataset, and we only change the rendering component to make for a visually more challenging task that involves a different perspective and partial occlusions. See Figure ||SYMBOLTOKEN|| for an example.


@#S!S
Atari 2600 Games
@#S^S
Atari Pong We make use of the Arcade Learning Environment bellemare2013arcade to create a small environment based on the Atari 2600 game Pong which is restricted to the first interaction between the ball and the player-controlled paddle, i.e., we discard the first couple of frames from the experience buffer where the opponent behaves completely independent of any player action. Specifically, we discard the first 58 (random) environment interactions. We use the  PongDeterministic-v4 variant of the environment in OpenAI Gym brockman2016openai. We use a random policy and populate an experience buffer with 10 environment interactions per episode, i.e., ||SYMBOLTOKEN|| An observation consists of two consecutive frames, cropped (to exclude the score) and resized to 50×50 pixels each. See Figure ||SYMBOLTOKEN|| for an example.

Space Invaders This environment is based on the Atari 2600 game Space Invaders, using the  SpaceInvadersDeterministic-v4 variant of the environment in OpenAI Gym brockman2016openai, and processed / restricted in a similar ma ner as the Pong environment. We discard the first 50 (random) environment interactions for each episode and only begin populating the experience buffer thereafter. See Figure ||SYMBOLTOKEN|| for an example observation.


@#S!S
3-Body Physics
@#S^S
 The 3-body physics simulation environment is an interacting system that evolves according to classical gravitational dynamics. Different from the other environments considered here, there are no actions. This environment is adapted from jaques2019physics using their publicly available ||SYMBOLTOKEN|| where we set the step size (dt) to 2.0 and the initial maximum x and y velocities to 0.5. We concatenate two consecutive frames of 50×50 pixels each to provide the model with (implicit) velocity information.  See Figure ||SYMBOLTOKEN|| for an example observation sequence.



@#!S
Evaluation Metrics
@#^S



@#S!S
Hits at Rank k (H@k)
@#S^S
 This score is 1 for a particular example if the pr dicted state representation is in the k-nearest neighbor set around the encoded true observation, where we define the neighborhood of a node to include the node itself. Otherwise this score is 0. In other words, this score measures whether the rank of the predicted state representation is smaller than or equal to k, when ranking all reference state representations by distance to the true state representation. We report the average of this score over a particular evaluation dataset.


@#S!S
Mean Reciprocal Rank (MRR)
@#S^S

This score is defined as the average inverse rank, i.e., ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| where ||SYMBOLTOKEN|| is the rank of the n-th sample.


@#!S
Architecture and Hyperparameters
@#^S



@#S!S
Object Extractor
@#S^S

For the 3D cubes environment, the object extractor is a 4-layer CNN with 3× 3 filters, zero-padding, and 16 feature maps per layer, with the exception of the last layer, which has ||SYMBOLTOKEN|| feature maps, i.e., one per object slot. After each laye , we apply BatchNorm ioffe2015batch a | SYMBOLTOKEN|| activation function. For the 2D shapes environment, we choose a simpler CNN architecture with only a single convolutional layer with 10× 10 filters and a stride of 10, followed by BatchNorm and a ReLU activation (LeakyReLU xu2015empirical for the Atari 2600 and physics environments). This layer has 16 feature maps and is followed by a channel-wise linear transformation (i.e., a 1×1 convolution), with 5 feature maps as output. For both models, we choose a sigmoid activation function after the last layer to obtain object masks with value  in (0, 1). We use the same two-layer architecture for the Atari 2600 environments and the 3-body physics environment, but wit  9×9 filters (and zero-padding) in the first layer, and 5×5 filters with a stride of 5 in the second layer.


@#S!S
Object Encoder
@#S^S

After reshaping/flattening the output of the object extractor, we obtain a vector representation per object (2500-dim for the 3D cubes environment, 25-dim for the 2D shapes environment, and 1000-dim for Atari 2600 and physics environments). The object encoder is an MLP with two hidden layers of 512 units and each, followed by ReLU activation functions. We further us  LayerNorm ba2016layer before the activation function of the second hidden layer.  The output of the final output layer is 2-dimensional (4-dimensional for Atari 2600 and physics environments), reflecting the ground truth object state, i.e., the object coordinates in 2D (although this is not provided to the model), and velocity (if applicable).


@#S!S
Transition Model
@#S^S

Both the node and the edge model in the GNN-based transition model are MLPs with the same architecture / number of hidden units as the object encoder model, i.e., two hidden layers of 512 units each, LayerNorm and ReLU activations.

 @#S!S
Loss function
@#S^S

Th  margin in the hinge loss is chosen as ||SYMBOLTOKEN|| We further multiply the squared Euclidean distance d(x,y) in the loss function with a factor of ||SYMBOLTOKEN|| with ||SYMBOLTOKEN|| to control the spread of the embeddings. We use the same setting in all experiments.


@#S!S
Baselines
@#S^S
World Model Baseline

The World Model baseline is trained in two stages: First, we train an auto-encoder or a VAE with a 32-dim latent space, where the encoder is a CNN with the same architecture as the object extractor used in the C-SWM model, followed by an MLP with the same architecture as the object encoder module in C-SWM on the flattened representation of the output of the encoder CNN. The decoder exactly mirrors this architecture where we replace convolutional layers with deconvolutional layers. We verified that this architecture can successfully build representations of single ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| Cubes
   
  ||SYMBOLTOKEN|| g r a p h i c s ||SYMBOLTOKEN|| ShapesReconstructions from the latent code of a trained VAE-based World Model baseline.
Example reconstructions from the latent code are shown in Figure ||SYMBOLTOKEN|| We experiment d both with mean squared error and binary cross entropy (using the continuous channel values in [0, 1] as targets) as reconstruction loss in the (V)AE models, both of which are typical choices in most practical implementations. We generally found binary cross entropy to be more stable to optimize and to produce better results, which is why we opted for this loss in all the baselines using decoders considered in the experimental section.


In the second stage, we freeze the model parameters of the auto-encoder and train a transition model with mean-squared error on the latent representations. For the VAE model, we use the predicted mean values of the latent representations. This transition model takes the form of an MLP with the same architecture and number of hidden units as the node model in C-SWM. We experimented both with a translational transition model (i.e., the transition model only predicts the latent state difference, instead of the full next state) and direct prediction of the next state. We generally found that the translational transition model performed better and used it throughout all the reported results.

The World Model baselines are trained with a smaller batch size of 512, which slightly improved performance and simplified memory management.

Ablations
We perform the following ablations: 1) we replace the latent GNN with an MLP (per object, i.e., we remove the edge update function) to investigate whether a structured transition model is necessary, 2) we remove the state factorization and embed the full scene into a single latent variable of higher dimensionality (original dimensionality × number of original object slots) with an MLP as transition model, and 3) we replace the contrastive loss with a pixel-based reconstruction loss on both the current state and the predicted next state (we add a decoder that mirrors the architecture of the encoder).

Physics-as-Inverse-Graphics (PAIG)
For this baseline, we train the PAIG model from jaques2019physics with the code provided by the ||SYMBOLTOKEN|| on our dataset with the standard settings recommended by the authors for this particular task, namely: ||SYMBOLTOKEN|| epochs=500, ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| and ||SYMBOLTOKEN|| We use the same input size as in C-SWM, i.e., frames are of shape 50×50×3. We further set ||SYMBOLTOKEN|| ||SYMBOLTOKEN|| and ||SYMBOLTOKEN|| to match our setting of predicting for a total of 10 frames while conditioning on a pair of 2 initial frames to obtain initial position and velocity information. We train this model with four different random seeds. For evaluation, we extract learned position representations for all frames of the test set and further run the latent physics prediction module (conditioned on the first two initial frames) to obtain model predictions. We further augment position representations with velocity information by taking the difference between two consecutive position representations and concatenating this representation with the 2-dim position representation, which we found to slightly improve results. In total, we obtain a 12-dim (4-dim × 3 objects) representation for each time step, i.e., the same as C-SWM. From this, we obtain ranking metrics. We found that one of the training runs ||SYMBOLTOKEN|| collapsed all latent representations to a single point. We exclude this run in the results reported in Table ||SYMBOLTOKEN|| i.e., we report average and standard error over the three other runs only.



