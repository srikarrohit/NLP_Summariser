% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{hyperref}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Structured Summarization of \\ Academic Publications}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Alexios Gidiotis \and
Grigorios Tsoumakas}
%
\authorrunning{A. Gidiotis and G. Tsoumakas}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{School of Informatics, Aristotle University of Thessaloniki}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
We propose SUSIE, a novel summarization method that can work with state-of-the-art summarization models in order to produce structured scientific summaries for academic articles. We also created PMC-SA, a new dataset of academic publications, suitable for the task of structured summarization with neural networks. We apply SUSIE combined with three different summarization models on the new PMC-SA dataset and we show that the proposed method improves the performance of all models by as much as 4 ROUGE points.
\keywords{Text summarization \and natural language processing \and deep learning.}
\end{abstract}
%
%
%
\section{Introduction}
Having informative summaries of scientific articles is crucial for dealing with the avalanche of academic publications in our times. Such summaries would allow researchers to quickly and accurately screen retrieved articles for relevance to their interests. More importantly, such summaries would lead to high quality indexing of the articles by (academic) search engines, leading to more relevant academic search results.     

%The increasing rate at which scientific articles are being published makes it very difficult for researchers to keep up with the developments in their area of research. This also makes the task of retrieving relevant articles increasingly difficult. 

Currently, the role of such summaries is played by the abstracts produced by the authors of the articles. However, authors usually include in the abstract only the contributions and information of the paper that they consider important and ignore others that might be equally important to the scientific community \cite{Elkiss2008BlindArticle}. 
%Instead, most of the time authors would simply copy parts of the paper and put them in the abstract.

%Automated summarization of academic articles can facilitate these problems by compressing the articles into short, informative summaries. 
%Typically an article is indexed by search engines at the title and abstract level. Also, human readers screen articles by looking at the abstracts. Τhis can be particularly problematic for articles that either have no abstract or the abstract is of poor quality.

%Automatically generating short yet informative summaries for published articles, allows search engines to index and effectively retrieve more relevant content. It is also much more helpful for the reader to be able to see a short, representation of the article that covers the key points, methods and findings. 

%First of all, re-writing the main aspects of the article requires a lot of effort from the writer. 
%On the contrary, a good, informative summary could be very helpful as a starting point for an author that is trying to write an abstract for his paper.

A solution to the above problem would be to employ state-of-the-art summarization approaches \cite{See2017GetNetworks,Paulus2017ASummarization,Dangovski2019RotationalApplications,Celikyilmaz2018DeepSummarization}, in order to automatically create short informative summaries of the articles to replace and/or accompany author abstracts for machine indexing and human inspection. These approaches however, have focused on the summarization of newswire articles while academic articles exhibit several differences and pose major challenges compared to news articles. 

First of all, news articles are much shorter than scientific articles and the news headlines that serve as summaries are much shorter than scientific abstracts. Secondly, scientific articles usually include several different key points that are scattered throughout the paper and need to be accurately included in a summary. These problems make it difficult to use summarization models that achieve state-of-the-art performance on newswire datasets for the summarization of academic articles.

We propose SUSIE (StrUctured SummarIzEr), a novel training method that allows us to effectively train existing summarization models on academic articles that have structured abstracts. Our method uses the XML structure of the articles and abstracts in order to split each article into multiple training examples and train summarization models that learn to summarize each section separately. We call such a task {\em structured summarization}. We further contribute a novel dataset consisting of open access PubMed Central articles along with their structured abstracts. SUSIE can easily be combined with different summarization models in order to address the problem of long articles and has been found to improve the performance of state-of-the-art summarization models by 4 ROUGE points.

We also created {\em PMC-SA} (PMC Structured Abstracts), a novel dataset that consists of academic articles from the biomedical domain The articles for this dataset were collected from the {\em PubMed Central Open Access} (PMC-OA) repository and follow the {\em IMRD} ((Introduction, Methods, Results, Discussion) structure. The abstracts in this dataset are also structured in a similar manner and each section of the full text can be paired with the corresponding section of the abstract.
% The resulting dataset consists of more than 700,000 articles that are processed and properly annotated for the task of summarization. 

\section{Related Work}

\subsection{Summarization Methods}
\label{sec:seq2seq_models}

State-of-the-art summarization methods use recurrent neural networks (RNNs) with the encoder-decoder architecture (or sequence-to-sequence architecture). These  methods usually treat the whole source text as an input sequence, encode it into their hidden state and generate a complete summary from that hidden state. 

Strong results have been achieved by such models when combined with an {\em attention} mechanism \cite{Nallapati2016AbstractiveBeyond,Chopra2016AbstractiveNetworks,Rush2015ASummarization}. Adding a {\em pointer-generator} mechanism has been shown to further improve results  \cite{See2017GetNetworks}. The pointer-generator mechanism gives the model the ability to copy important words from the source text in addition to generating words from a predefined vocabulary. Adding a {\em coverage} mechanism has been shown to lead to even better results. \cite{See2017GetNetworks}. The coverage mechanism prevents the model from repeating itself, which is a common problem with sequence-to-sequence models. In recent work, various approaches utilized reinforcement learning and policy gradient to further improve the performance of baseline models \cite{Paulus2017ASummarization,Celikyilmaz2018DeepSummarization}. Finally, \cite{Dangovski2019RotationalApplications} replaced the LSTM cells in the model of \cite{See2017GetNetworks} with a different type of RNN unit in order to overcome fundamental limitations of LSTM cells.
 
\subsection{Summarization Datasets}
\label{sec:other_data}
Most of the summarization datasets that are found in the literature such as {\em Newsroom}  \cite{Grusky2018Newsroom:Strategies}, {\em Gigaword}  \cite{Napoles2012AnnotatedGigaword} and {\em CNN / Daily Mail}  \cite{Hermann2015TeachingComprehend} are focused on newswire articles.
%With the exception of the DUC datasets, all other datasets are large, ranging from a few hundred thousand articles (CNN / Daily Mail) to a few million articles (Gigaword, Newsroom). 
%Several summarization datasets exist in the literature. Most of them are focused on newswire articles paired with one or several summaries. In some cases, the summaries are human-written like those in the datasets created for the {\em Document Understanding Conference} (DUC 2003-2004) and the {\em Newsroom} dataset \citep{Grusky2018Newsroom:Strategies}. In others, the  summaries are automatically generated from the headline of the article like in the {\em Gigaword} corpus \citep{Napoles2012AnnotatedGigaword} or from description bullet point like in the {\em CNN / Daily Mail} dataset \citep{Hermann2015TeachingComprehend}. With the exception of the DUC datasets, all other datasets are large, ranging from a few hundred thousand articles (CNN / Daily Mail) to a few million articles (Gigaword, Newsroom). 
The average article lengths are relatively small and range from 50 words (Gigaword) to a few hundred words (CNN / Daily Mail, Newsroom). The average summary lengths are also rather small and range from a single sentence (Gigaword, Newsroom) to a few sentences (CNN / Daily Mail). %also DUC

TAC 2014 (Text Analysis Conference 2014) is the only dataset that focuses on (biomedical) academic articles. The articles have an average of 9,759 words and the summaries an average of 235 words. However, as it consists of just 20 articles, it is not useful for training complex neural network summarization models. %and can only be used for the evaluation of such models. 

%One exception in the existing datasets is the {\em Text Analysis Conference} (TAC 2014) biomedical summarization dataset. This dataset contains 20 different topics with one biomedical reference article and an 15 citing articles for each topic. Also, each topic is paired with 4 scientific summaries written by experts in the biomedical domain.  

\section{Summarizing Academic Papers}
\subsection{Flat Abstract Summarization}
\label{sec:flat_sum}

A simple approach to summarizing academic papers would be to train sequence-to-sequence models using the full text of the article as source input and the abstract as reference summary. However, sequence-to-sequence models face multiple difficulties when given long input texts. A very long input sequence requires the encoder RNN to run for a lot of time steps. This greatly increases the computational complexity of the forward pass. To make things worse, the training of the encoder on very long input sequences becomes increasingly difficult due to the computational complexity of the backward pass. The training becomes increasingly slower and in many cases the vanishing gradients prevent the model from learning useful information.

A solution to this problem would be to truncate very long sequences (more than 600 words), but this can result in serious information loss which would severely affect the quality of the produced summaries.

Even harder is the training of a decoder with very long output sequences. In this case, the computational complexity and memory requirements of the decoder make it pointless to try and train a model with very long reference summaries.

Another problem of this straightforward approach, is that the different parts of an academic paper are not equally important for the task of summarization. Sections like the {\em introduction} include core information for the summary, while others like the {\em experiments} are noisy and usually include little useful information.

\subsection{SUSIE}
\label{sec:susie}

SUSIE (StrUctured SummarIzEr) is a novel summarization method that exploits structured abstracts in order to address the aforementioned problems. 

Many academic articles, especially in the life sciences domain follow the typical {\em IMRD} structure with sections like {\em introduction}, {\em background}, {\em methods}, {\em results} and {\em  conclusion}. When the abstract of the article is structured it usually  includes similar sections too. We employed a very simple method that looks for specific keywords in the header of each section in order to annotate both the article and abstract sections. For example, sections that include keywords like {\em methods}, {\em method}, {\em techniques} and {\em methodology} in their header are annotated as \textit{methods}. Table \ref{section_tags} presents the different section types and the keywords associated with them. 


Once the article and abstract sections are annotated, we pair each section of the full text with the corresponding section of the abstract and create one training example per section. We can then use one of the existing summarization methods and train a model for the summarization of single sections. Summarizing a single section of an article is a much easier task since the input and output sequences are a lot shorter and the information is more compact and focused on specific aspects of the article. In addition, section annotation allows us to filter out particular sections that are not useful for summarization.

At test time we extract the specified sections of the article and run the summarization model for each of them in order to produce section summaries. Then we combine those summaries in order to get the full summary of the article.


\section{PMC Structured Abstracts}

PubMed Central (PMC) is a free digital repository that archives publicly accessible full-text scholarly articles that have been published within the biomedical and life sciences journal literature. The PMC-SA (PMC Structured Abstracts) dataset was created from the open access subset of PMC, comprising approximately 2 million articles. We used the XML format downloaded from the PMC FTP server to create the dataset. Only the articles that have abstracts structured in sections were selected and included in the dataset. PMC-SA has a total of 712,911 full text articles along with their abstracts. The full texts of the articles have an average length of 2,514 words and are used as source texts for the summarization, while the abstracts have an average length of 260 words and are used as reference summaries. Code and instructions for the creation of the PMC-SA dataset will be made available online.\footnote{\url{https://github.com/AlexGidiotis/PMC-StructuredAbstracts-Dataset}} When compared with the existing datasets discussed in section \ref{sec:other_data} PMC-SA is clearly different in multiple ways. The articles and summaries are significantly longer compared to the different newswire datasets and this makes it a much harder task. Also, the new dataset is a lot larger than the TAC 2014 dataset which is the only other dataset that consists of academic publications. This makes it suitable for the training of state-of-the-art summarization models. 

We can easily apply SUSIE on PMC-SA since the XML format allows us to effectively split the full text and abstract into annotated sections. In table \ref{per_sec_exp_stats} we show detailed statistics about the source and abstract length for each section type.

\section{Experiments}


As we mentioned, SUSIE can be combined with a number of different summarization models. In order to evaluate the effectiveness of SUSIE the three different summarization models that were described in section \ref{sec:seq2seq_models} are trained and evaluated on PMC-SA using both the flat abstract method from section \ref{sec:flat_sum} and SUSIE.

%In order to evaluate the effectiveness of SUSIE, the three different summarization models that were described in section \ref{sec:seq2seq_models} are used and their performance is measured on PMC-SA. More specifically, we first train and evaluate the models using the flat abstract method from section \ref{sec:flat_sum}. Then we train the same models using SUSIE and compare the evaluation results.

The training set has 641,994 articles, the validation set has 35,309 articles and the test set 10,111 articles. In all experiments we included for summarization only the {\em introduction}, {\em methods} and {\em conclusion} sections because we have found that these particular section selection gives us the best performing models. For the flat abstract method, the selected sections are concatenated and used as source input paired with the concatenation of the corresponding abstract sections as reference summary. For SUSIE one example is created for each of the selected sections with the corresponding abstract section as reference summary. In Tables \ref{exp_stats} we provide detailed statistics about the training data used in the two different methods.


\subsection{Experimental Setup}

We used the implementation of the three models provided by \cite{See2017GetNetworks}\footnote{\url{https://github.com/abisee/pointer-generator}}. The hyperparameter setup used for the models is similar to that of \cite{See2017GetNetworks} and is detailed in the supplementary material. 

%All models use LSTM cells with 256 units and word embeddings of 128 dimensions which are are not pretrained. The decoders have a predefined vocabulary of 50k words. We train with the Adagrad optimizer \citep{Duchi2011AdaptiveOptimization} with a learning rate of 0.15 and a batch size of 16. We also clipped the gradients to have a maximum norm of 2. 

%No dropout or regularization was used. Instead, we used early stopping based on the validation loss to prevent the models from overfitting.

%to 250 and 50 words and finally to 500 and 100 words respectively.

In order to speed up the training process we start off with highly truncated input and output sequences. In more detail, we begin with input and output sequences truncated to 50 and 10 words respectively and train until convergence. Then we gradually increase the input and output sequences up to 500 and 100 words respectively.

When using the flat abstract method, we truncate each section to $\frac{L}{n}$ words before concatenating them to get the input and output sequences, where $L$ is the required article length and $n$ is the number of extracted sections from this article. 

The truncation of an academic article to a total of 500 words is definitely going to result in some severe information loss but we deemed it necessary due to the difficulties described in section \ref{sec:flat_sum}. To get the coverage model we simply add the coverage mechanism to the converged pointer-generator model and continue training.

% it becomes very difficult to train encoder-decoder models with more that 500 input words and 100 output words. When we attempted to increase the input size to 800 words, the training iterations became very slow and the final models were not able to focus on specific points in the text and generate meaningful summaries. On the other hand, increasing the output size to 200 words resulted in OOM errors and we were not able to train any models with that output size.

%For the attention sequence-to-sequence model and the pointer-generator model the end-to-end training takes approximately 4 days on two Nvidia GTX 1080 GPUs.

At test time, for the flat abstract method, we truncate each input section to $\frac{L}{n}$ with $L=500$ words and concatenate them to get an input sequence of 500 words. Then we run beam search for 120 decoding steps in order to generate a summary. For SUSIE each of the selected sections is truncated to 500 words before we run beam search for 120 decoding steps to get a summary for each one of them. Then we concatenate the individual summaries to get the summary of the full article.

\subsection{Results}

We evaluate the performance of all models with the ROUGE family of metrics~\cite{Lin2004Rouge:Summaries} using the {\em pyrouge} package\footnote{\url{https://pypi.org/project/pyrouge/0.1.3}}. In specific, we report F1 scores for ROUGE-1, ROUGE-2 and ROUGE-L. ROUGE-1 and ROUGE-2 measure the overlap, in unigrams and bigrams respectively, between the generated and the reference summary. ROUGE-L measures the longest common subsequence overlap.


Table \ref{exp_results} presents the results of our experiments. We can see that the pointer-generator model achieves higher scores than the simple attention sequence-to-sequence and adding the coverage mechanism further improves those scores which is in line with the experiments of \cite{See2017GetNetworks}. 

We also notice that SUSIE improves the scores of the flat summarization approach for all three models by as much as 4 ROUGE points. The performance of the best model, pointer-generator with coverage, is improved by approximately 13\%, 28\% and 14\% in terms of ROUGE-1, ROUGE-2 and ROUGE-L F1 score respectively. It is clear that the flat approach suffers from information loss due to the truncation of the source input.  
%Nevertheless, all three flat models perform poorly on the PMC-SA. That can mainly be attributed to the information loss that comes from the fact that we truncated the source input. On the other hand, SUSIE improves the scores of all three models by as much as 4 ROUGE points. 
In the appendix we illustrate the difference in the quality of the summaries produced by the two different methods by presenting generated examples for a real article. 

\section{Conclusion}

This work focused on the summarization of academic publications. We have shown that summarization models that perform well on smaller articles have difficulties when applied on longer articles with a lot of diverse information like academic articles. We proposed SUSIE, a novel approach that allowed us to successfully adapt existing summarization models to the task of structured summarization of academic articles. Also, we created PMC-SA, a new dataset of academic articles that is suitable for the training of summarization models using SUSIE. We found that training with SUSIE on the PMC-SA greatly improves the performance of summarization models and the quality of the generated summaries. 

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{references}

\section{Appendix}
\small
Here we will provide an example of summaries generated by the best performing model, namely pointer-generator with coverage, for a sample article from the test set. We provide two summaries, one generated from a model trained with the flat method and another generated from a model trained with SUSIE. We also provide the original abstract of the article for reference.  One can find the original article with {\em PMCID PMC5051331} at the PMC website\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5051331}}.

Comparing the two generated summaries, we can see that the one generated with SUSIE is superior to the flat one in terms of structure, readability and factual correctness. When compared with the original abstract, we can see that both summaries are not perfect but the one generated with SUSIE is in many cases acceptable. 

\subsection{Reference Summary}
\textbf{Objective} To examine the efficacy of psychological and psychosocial interventions for reductions in repeated self-harm. \\ 
\textbf{Design} We conducted a systematic review, meta-analysis and meta-regression to examine the efficacy of psychological and psychosocial interventions to reduce repeat self-harm in adults. We included a sensitivity analysis of studies with a low risk of bias for the meta-analysis. For the meta-regression, we examined whether the type, intensity (primary analyses) and other components of intervention or methodology (secondary analyses) modified the overall intervention effect. \\
\textbf{Data sources} A comprehensive search of medline, psycinfo and embase (from 1999 to june 2016) was performed. \\
\textbf{Eligibility criteria for selecting studies} Randomised controlled trials of psychological and psychosocial interventions for adult self-harm patients. \\
\textbf{Conclusions} Consideration of a psychological or psychosocial intervention over and above treatment as usual is worthwhile; with the public health benefits of ensuring that this practice is widely adopted potentially worth the investment. However, the specific type and nature of the intervention that should be delivered is not yet clear. Cognitive–behavioural therapy or interventions with an interpersonal focus and targeted on the precipitants to self-harm may be the best candidates on the current evidence. Further research is required.

\subsection{Flat P-Gen + Coverage}
Clinically treated non-fatal self-harm (we use the term self-harm henceforth) is common in terms of adverse outcomes such as repetition of self-harm, suicide and all-cause mortality; mental health morbidity, quality of life and physical, psychological and social functioning, mental health morbidity, quality of life and physical, psychological and social functioning. It is also costly in terms of immediate and ongoing treatment of self-harm of 16.3\% after 1 year and 7\% after 9 years. A more recent review of 177 studies from western and non-western countries indicated little change in these estimates, with reported repetition of self-harm of 16\%, while the suicide rate was 1.6

\subsection{SUSIE P-Gen + Coverage}
\textbf{Objective} To evaluate the efficacy of psychological and pharmacological interventions for reducing repetition of self-harm in unselected populations, but again highlight the poor quality of the evidence base in pooling data. In addition to these cochrane reviews, a number of trials have been published and several systematic reviews produced that aim to highlight what interventions are most often efficacious interventions (cbt) (and not problem-solving therapy). \\
\textbf{Methods} We searched medline, embase, and the cochrane central register of controlled controlled trials (rct) published up to february 2016 to identify randomized controlled trials evaluating the efficacy of psychological and psychosocial interventions to reduce repeat self-harm, (primary outcome) and to reduce suicidal ideation, depression and hopelessness (secondary outcomes) using meta-analysis; and (2) examine whether the type, intensity or other specific components of the interventions, or study methodology, modify the pooled intervention effect using meta-regression analysis. \\
\textbf{Conclusions} our study is consistent with the updated cochrane review, which in contrast to the original version showed no support for problem-focused (predominantly problem-solving therapy) interventions but a significant effect of interventions. Our study has shown that psychological or psychosocial interventions are effective overall, with cbt and psychodynamic interpersonal therapy currently the most promising for implementation.

\end{document}