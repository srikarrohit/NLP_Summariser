

Automated Theorem Provers -LRB- ATPs -RRB- DBLP : books/el/RobinsonV01 can be in principle
used to attempt the proof of any provable mathematical conjecture .
The standard ATP approaches have
so far relied primarily on fast implementation of manually designed search procedures and heuristics .
However using machine learning for guidance in the vast action spaces of the ATP calculi is a
natural choice that has been recently shown to significantly improve over the
unguided systems KaliszykUMO18 JakubuvU19 .

The common procedure of a firstorder ATP system -- saturation-style
or tableaux -- is the following . The ATP starts with a set of first
order axioms and a conjecture . The conjecture is negated and the
formulas are Skolemized and clausified .
The objective is then to derive a contradiction from the set of clauses
typically using some form of resolution and related inference rules .
The Skolemization as well as introduction of new definitions during
the clausification results in the introduction of many new function and predicate
symbols .

When guiding the proving process by statistical machine learning
the state of the prover and the
formulas literals and clauses are typically encoded to vectors of
real numbers . This has been so far mostl done with hand-crafted features resulting
in large sparse vectors DBLP : conf/ijcai/KaliszykUV15 hammers4qed abs-1108-3446 UrbanVS11 KaliszykU15 JakubuvU17a possibly reducing their dimension afterwards ChvalovskyJ0U19 .
Several experiments with neural
networks have been made recently in particular based on 1D convolutions
RNNs GollerK96 TreeRNNs ChvalovskyJ0U19 and GraphNNs DuvenaudMABHAA15 . Most of the approaches however can not
capture well the idea of a variable occurring multiple times in the
formula and to abstract from the names of the variables .
These issues were first
addressed in FormulaNet DBLP : conf/nips/WangTWD17 but even that
architecture relies on knowing the names of function and predicate symbols . This makes it
unsuitable for hand ing the large number of problem-specific function and predicate
symbols introduced during the cla sification . -LSB- The ratio of such symbols in real-world clausal datasets is around 40 % see Section | | SYMBOLTOKEN | | same holds for large datasets of ATP problems
where symbol names are not used consistently such as the TPTP library Sutcliffe10 .


In this paper we make further steps towards the abstraction of
mathematical clauses formulas and proof states . We present a network that is invariant not only
under renaming of variables but also under renaming of arbitrary function and predicate
symbols . It is also invariant under replacement of the symbols by their negated versions .
This is achieved by a novel conversion of the input formulas into a hypergraph followed by a
particularly designed
graph neural network -LRB- GNN -RRB- capable of maintaining the invariance under negation .
We experimentally demonstrate in three case studies
that the network works well on data
coming from automated theorem proving tasks .


The paper is structured as follows . We first formally describe our network architecture in
Section | | SYMBOLTOKEN | | and discuss its invariance properties
in Section | | SYMBOLTOKEN | | We describe an experiment using the
network for guiding in Section | | SYMBOLTOKEN | | and
two experiments done on a fixed dataset in
Section | | SYMBOLTOKEN | | Section | | SYMBOLTOKEN | | contains the
results of these three experiments .




We presented a neural network for processing mathematical formulae invariant under symbol names negation and ordering of clauses and their literals and we demonstrated its learning capabilities in
three automated reasoning tasks .
In particular the network improves over the previous version of
guided by XGBoost by 45.6 % on the test set in the first iteration of
learning-guided proving . It also outperforms earlier methods on the
premise-selection data and establishes strong baseline for symbol
guessing . One of its novel uses proposed here and allowed by this
neural architecture is creating new conjectures by detecting and
following alignments of various mathematical theories and
concepts . This task turns out to be a straigh forward application of the structural
learning performed by the network .

Possible future work includes for example integration with state-of-the-art saturation-style provers .
An interesting next step is also evaluation on a heterogeneous dataset such as TPTP where
symbols are not used consistently and learning on multiple libraries --
e.g. jointly on HOL and HOL Light as done previously
by DBLP : conf/lpar/GauthierK15 using a hand-crafted alignment
system .



@label
Automated reasoning and theorem proving have recently become major challenges for machine learning
@label
In other domains representations that are able to abstract over unimportant transformations such as abstraction over translations and rotations in vision are becoming more common
@label
Standard methods of embedding mathematical formulas for learning theorem proving are however yet unable to handle many important transformations
@label
In particular embedding previously unseen labels that often arise in definitional encodings and in Skolemization has been very weak so far
@label
Similar problems appear when transferring knowledge between known symbols
@label
We propose a novel encoding of formulas that extends existing graph neural network models
@label
This encoding represents symbols only by nodes in the graph without giving the network any knowledge of the original labels
@label
We provide additional links between such nodes that allow the network to recover the meaning and therefore correctly embed such nodes irrespective of the given labels
@label
We test the proposed encoding in an automated theorem prover reasoning system based on the tableaux connection calculus and show that it improves on the best characterizations used so far
@label
The encoding is further evaluated on the premise selection task and a newly introduced symbol guessing task and shown to correctly predict 65 of the symbol names
@label
show that the network can One of the main questions in the rapidly arising field of machine learning in automated theorem proving is how to embed a mathematical formula that is how to assign a vector of real numbers to it
@label
The standard methods often can not handle well labels in the formulas unseen before which can easily appear from skolemization or definitional encoding and also their capabilities of transferring knowledge between known symbol are mostly limited
@label
We propose an encoding based on graph neural networks where symbols are represented only by nodes in the graph without giving the network any knowledge of the original labels
@label
We tested the network on a tableaux connection prover on Mizar40 dataset with significantly better results in the first iteration .