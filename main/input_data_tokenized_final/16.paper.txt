In many real-world applications of reinforcement learning -LRB- RL -RRB- interactions with the environment are limited due to cost or feasibility . This presents a challenge to
traditional RL algorithms since the max-return objective involves an expectation over on-policy samples . We introduce a new formulation of max-return optimization
that allows the problem to be re-expressed by an expectation over an arbitrary behavior-agnostic and off-policy data distribution . We first derive this result by considering
a regularized version of the dual max-return objective before extending our findings to unregularized objectives through the use of a Lagrangian formulation
of the linear programming characterization of Q-values . We show that if auxiliary dual variables of the objective are optimized then the gradient of the off-policy
objective is exactly the on-policy policy gradient without any use of importance weighting . In addition to revealing the appealing theoretical properties of this
approach we also show that it delivers good practical performanceThe use of model-free reinforcement learning -LRB- RL -RRB- in conjunction with function approximation has proliferated in recent years demonstrating successful applications in fields
such as robotics -LRB- Andrychowicz et al. . 2018 ; Nachum et al. . 2019a -RRB- game playing -LRB- Mnih et al. . 2013 -RRB- and conversational systems -LRB- Gao et al. . 2019 -RRB- . These successes often rely
on-policy access to the environment ; i.e. during the learning process agents may collect new experience from the environment using policies they choose and these interactions are
effectively unlimited . By contrast in many real-world applications of RL interaction with the environment is costly if not impossible hence experience collection during learning
is limited necessitating the use of off-policy RL methods i.e. algorithms which are able to learn from logged experience collected by potentially multiple and possibly unknown
behavior policies . The off-policy nature of many practical applications presents a significant challenge for RL algorithms . The traditional max-return objective is in the form of an on-policy expectation
and thus policy gradient methods -LRB- Sutton et al. . 2000 ; Konda and Tsitsiklis 2000 -RRB- require samples from the on-policy distribution to estimate the gradient of this objective . The most
straightforward way to reconcile policy gradient with off-policy settings is via importance weighting -LRB- Precup et al. . 2000 -RRB- . However this approach is prone to high variance and
instability without appropriate damping -LRB- Munos et al. . 2016 ; Wang et al. . 2016 ; Gruslys et al. . 2017 ; Schulman et al. . 2017 -RRB- . The more common approach to the off-policy problem
is to simply ignore it which is exactly what has been proposed by many existing off-policy policy gradient methods -LRB- Degris et al. . 2012 ; Silver et al. . 2014 -RRB- . These algorithms simply
compute the gradients of the max-return objective with respect to samples from the offpolicy data ignoring distribution shift in the samples . The justification for this approach
is that the maximum return policy will be optimal regardless of the sampling distribution of states . However such a justification is unsound in function approximation settings
where models have limited expressiveness with potentially disastrous consequences on optimization and convergence -LRB- e.g. Lu et al. . 2018 -RRB- . Value-based methods provide an alternative
that may be more promising for the off-policy setting . In these methods a value function is learned either as a critic to a learned policy -LRB- as in actor-critic -RRB- or as the maximum return
value function itself -LRB- as in Q-learning -RRB- . This approach is based on dynamic programming in tabular settings which is inherently off-policy and independent of any underlying data
distribution . Nevertheless when using function approximation the objective is traditionally expressed as an expectation over single-step Bellman errors which re-raises the question
`` What should the expectation be ? '' Some theoretical work suggests that the ideal expectation is in fact the on-policy expectation -LRB- Sutton et al. . 2000 ; Silver et al. . 2014 ; Nachum et al. . 2018 -RRB- .
In practice this problem is usually ignored with the same justification as that made for off-policy policy gradient methods . It is telling that actor-critic or Q-learning algorithms
advertised as off-policy still require large amounts of online interaction with the environment -LRB- Haarnoja et al. . 2018 ; Hessel et al. . 2018 -RRB- . In this work we present an ALgorithm for
policy Gradient from Arbitrary Experience via DICE -LRB- AlgaeDICE -RRB- 1 as an alternative to policy gradient and value-based methods . We start with the dual formulation of the max-return objective
which is expressed in terms of normalized state-action occupancies rather than a policy or value function . Traditionally this objective is considered unattractive since access to the occupancies
either requires an on-policy expectation -LRB- similar to policy gradient methods -RRB- or learning a function approximator to satisfy single-step constraints -LRB- similar to value-based methods -RRB- . We
demonstrate how these problems can be remedied by adding a controllable regularizer and applying a carefully chosen change of variables obtaining a joint objective over a policy and an
auxiliary dual function -LRB- that can be interpreted as a critic -RRB- . Crucially this objective relies only on access to samples from an arbitrary off-policy data distribution collected by
potentially multiple and possibly unknown behavior policies -LRB- under some mild conditions -RRB- . Unlike traditional actor-critic methods which use a separate objective for actor and critic
this formulation trains the policy -LRB- actor -RRB- and dual function -LRB- critic -RRB- to optimize the same objective . Further illuminating the connection to policy gradient methods we show that if the
dual function is optimized the gradient of the proposed objective with respect to the policy parameters is exactly the on-policy policy gradient . This way our approach naturally avoids
issues of distribution mismatch without any explicit use of importance weights . We continue to provide an alternative derivation of the same results based on a primal-dual form of the
return-maximizing RL problem and notably this perspective extends the previous results to both undiscounted Î³ = 1 settings and unregularized max-return objectives . Finally we provide
empirical evidence that AlgaeDICE can perform well on benchmark RL tasks . We have introduced an ALgorithm for policy Gradient from Arbitrary Experience via
DICE or AlgaeDICE for behavior-agnostic off-policy policy improvement in reinforcement learning . Based on a linear programming characterization of the Q-function we derived
the new approach from a Lagrangian saddle-point formulation . The resulting algorithm AlgaeDICE automatically compensates for the distribution shift in collected off-policy
data and achieves an estimate of the on-policy policy gradient using this off-policy data .

@label
In many real-world applications of reinforcement learning -LRB- RL -RRB- interactions with the environment are limited due to cost or feasibility .
@label
This presents a challenge to traditional RL algorithms since the max-return objective involves an expectation over on-policy samples .
@label
We introduce a new formulation of max-return optimization that allows the problem to be re-expressed by an expectation over an arbitrary
behavior-agnostic and off-policy data distribution .
@label
We first derive this result by considering a regularized version of the dual max-return objective before extending our findings to unregularized objectives
through the use of a Lagrangian formulation of the linear programming characterization of Q-values .
@label
We show that if auxiliary dual variables of the objective are optimized then the gradient of the off-policy objective is exactly the on-policy policy gradient
without any use of importance weighting .
@label
In addition to revealing the appealing theoretical properties of this approach we also show that it delivers good practical performance .
