Large transformer-based language models -LRB- LMs -RRB- trained on huge text corpora
have shown unparalleled generation capabilities . However controlling attributes
of the generated language -LRB- e.g. switching topic or sentiment -RRB- is difficult without
modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining . We propose a simple alternative : the Plug
and Play Language Model -LRB- PPLM -RRB- for controllable language generation which
combines a pretrained LM with one or more simple attribute classifiers that guide
text generation without any further training of the LM . In the canonical scenario
we present the attribute models are simple classifiers consisting of a user-specified
bag of words or a single learned layer with 100000 times fewer parameters than
the LM . Sampling entails a forward and backward pass in which gradients from
the attribute model push the LM 's hidden activations and thus guide the generation . Model samples demonstrate control over a range of topics and sentiment
styles and extensive automated and human annotated evaluations show attribute
alignment and fluency . PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation which will allow for
diverse and creative applications beyond the examples given in this paper .

The Transformer architecture has enabled large-scale language models -LRB- LMs -RRB-
trained on a huge amount of data to
greatly improve the state-of-the-art on natural language processing tasks . These models are used to
extract contextualized word embeddings for transfer learning purposes and as
natural language generators . The latter can leverage large amounts of unannotated data and a simple
log-likelihood training objective . However once such models are trained controlling attributes of
Controllable generation entails modeling p -LRB- x | a -RRB- where a is some desired controllable attribute -LRB- s -RRB-
and x the generated sample . However generative models only learn p -LRB- x -RRB- . In computer vision
Plug & Play Generative Networks developed a mechanism for
generating images with different attributes by plugging a discriminator -LRB- attribute model -RRB- p -LRB- a | x -RRB-
together with a base generative model p -LRB- x -RRB- and sampling from the resulting p -LRB- x | a -RRB- p -LRB- a | x -RRB- p -LRB- x -RRB-
effectively creating a conditional generative model on the fly from any supplied attribute model . In
a similar manner we propose the Plug and Play Language Model -LRB- PPLM -RRB- for conditional language
generation that combines one or more simple attribute models p -LRB- a | x -RRB- -- either in the form of a bagof-words -LRB- BoW -RRB- or single layer classifiers -- with a pre-trained unconditional language model p -LRB- x -RRB- .
We sample from the resulting combined model by following gradients in the latent representation
space in a manner inspired by the approximate Metropolis-adjusted Langevin sampler deployed in Nguyen et al. .
Optimization is performed ex post facto in the activation space therefore no re-training or finetuning is needed . Control is fine-grained with a strength parameter determining how strong the
attribute influence should be ; a strength of 0 fully recovers the original model p -LRB- x -RRB- . This design
allows vast flexibility : users can combine a state-of-the-art generative model which may be large
and difficult to train with any number of attribute controllers . Attribute models may be easier to train
or untrained -LRB- in the case of BoW models -RRB- and multiple controllers may be combined flexibly during
inference . In this paper we demonstrate the PPLM approach using a model as the general-purpose LM p -LRB- x -RRB- but the method applies in any representation space
from any transformer-based text generator and allows combination with any attribute model p -LRB- a | x -RRB- .
We demonstrate controlled generation with a number of attribute controllers assembled and combined during generation each with a different strength acting as a set of `` control knobs '' that tune
generation towards the desired attribute .

@label
Large transformer-based language models -LRB- LMs -RRB- trained on huge text corpora
have shown unparalleled generation capabilities .
@label
However controlling attributes
of the generated language -LRB- e.g. switching topic or sentiment -RRB- is difficult without
modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining .
@label
We propose a simple alternative : the Plug
and Play Language Model -LRB- PPLM -RRB- for controllable language generation which
combines a pretrained LM with one or more simple attribute classifiers that guide
text generation without any further training of the LM
@label
In the canonical scenario
we present the attribute models are simple classifiers consisting of a user-specified
bag of words or a single learned layer with 100000 times fewer parameters than
the LM .
@label
Sampling entails a forward and backward pass in which gradients from
the attribute model push the LM 's hidden activations and thus guide the generation .
@label
Model samples demonstrate control over a range of topics and sentiment
styles and extensive automated and human annotated evaluations show attribute
alignment and fluency .
@label
PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation which will allow for
diverse and creative applications beyond the examples given in this paper .
