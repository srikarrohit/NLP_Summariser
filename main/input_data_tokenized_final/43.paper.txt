This article reviews recent advances in multi-agent reinforcement learning algorithms for largescale control systems and communication networks , which learn to communicate and cooperate . We
provide an overview of this emerging field , with an emphasis on the decentralized setting under different
coordination protocols . We highlight the evolution of reinforcement learning algorithms from single-agent
to multi-agent systems , from a distributed optimization perspective , and conclude with future directions
and challenges , in the hope to catalyze the growing synergy among distributed optimization , signal
processing , and reinforcement learning communities

Fueled with recent advances in deep neural networks , reinforcement learning -LRB- RL -RRB- has been in the
limelight for many recent breakthroughs in artificial intelligence , including defeating humans in games
-LRB- e.g. , chess , Go , StarCraft -RRB- , self-driving cars , smart home automation , service robots , among many others .
Despite these remarkable achievements , many basic tasks can still elude a single RL agent . Examples
abound from multi-player games , multi-robots , cellular antenna tilt control , traffic control systems , smart
power grids to network management .
Often , cooperation among multiple RL agents is much more critical : multiple agents must collaborate
to complete a common goal , expedite learning , protect privacy , offer resiliency against failures and
adversarial attacks , and overcome the physical limitations of a single RL agent behaving alone . These tasks
are studied under the umbrella of cooperative multi-agent RL -LRB- MARL -RRB- , where agents seek to learn optimal
policies to maximize a shared team reward , while interacting with an unknown stochastic environment
and with each other . Cooperative MARL is far more challenging than the single-agent case due to : i -RRB-
the exponentially growing search space , ii -RRB- the non-stationary and unpredictable environment caused by
the agents ' concurrent yet heterogeneous behaviors , and iii -RRB- the lack of central coordinators in many
applications . These difficulties can be alleviated by appropriate coordination among agents .
The cooperative MARL can be further categorized into subclasses depending on the information
structure and types of coordination , such as how much information -LRB- e.g. , state , action , reward , etc. -RRB-
is available for each agent , what kinds of information can be shared among the agents , and what kinds
of protocols -LRB- e.g. , communication networks , etc. -RRB- are used for coordination . When only local partial
state observation is available for each agent , the corresponding multi-agent systems are often described
through decentralized partially observable Markov decision processes -LRB- MDP -RRB- , or DEC-POMDP for short ,
for which the decision problem is known to be extremely challenging . In fact , even the planning problem of
DEC-POMDPs -LRB- with known models -RRB- is known to be NEXT-complete . Despite some recent empirical
successes , finding an exact solution of Dec-POMDPs using RLs with theoretical guarantees
remains an open question .
When full state information is available for each agent , we call agents joint action learners -LRB- JALs -RRB-
if they also know the joint actions of other agents , and independent learners -LRB- ILs -RRB- if agents only know
their own actions . Learning tasks for ILs are still very challenging , since each agent sees other agents
as parts of the environment , so without observing the internal states , including other agents actions , the
problem essentially becomes non-Markovian and a partially observable MDP -LRB- POMDP -RRB- . It turns out
that optimal policy can be found under restricted assumptions such as deterministic MDP , and for
general stochastic MDPs , several attempts have demonstrated empirical successes . For a more
comprehensive survey on independent MARLs , the reader is referred to the survey .
The form of rewards , either centralized or decentralized , also makes a huge difference in multi-agent
systems . If every agent receives a common reward , the situation becomes relatively easy to deal with .
For instance , JALs can perfectly learn exact optimal policies of the underlying decision problem even
without coordination among agents . The more interesting and practical scenario is when rewards
are decentralized , i.e. , each agent receives its own local reward while the global reward to be maximized
is the sum of local rewards . This decentralization is especially important when taking into account the
privacy and resiliency of the system .

Until now , we mainly focused on networked MARL and recent advances which combine tools in
consensus-based distributed optimization with MARL under decentralized rewards . There remain much
more challenging agendas to be studied . By bridging two domains in a synergistic way , these research
topics are expected to generate new results and enrich both fields .

@label
This article reviews recent advances in multi-agent reinforcement learning algorithms for largescale control systems and communication networks , which learn to communicate and cooperate .
@label
We
provide an overview of this emerging field , with an emphasis on the decentralized setting under different coordination protocols .
@label
We highlight the evolution of reinforcement learning algorithms from single-agent to multi-agent systems , from a distributed optimization perspective , and conclude with future directions and challenges , in the hope to catalyze the growing synergy among distributed optimization , signal processing , and reinforcement learning communities .

