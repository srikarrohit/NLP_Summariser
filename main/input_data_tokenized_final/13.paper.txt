

A system can defend itself against adversarial machine learners by randomly switching the feature mask it is using by detecting unusual distributions of system inputs and reacting defensively to them or by using other methods . Authorship attribution can be discovered using adversarial machine learning to learn the patterns between the character unigrams and authors . An adversarial machine learner can learn the mappings between system inputs and outputs and learn the Neural Network -LRB- NN -RRB- or Support Vector Machine -LRB- SVM -RRB- .

A defensive system can detect the system inputs from an adversarial machine learner by comparing the normal probability distribution of the inputs versus the current probability distribution
of
the inputs and detecting a difference between them . If a difference is detected between the normal inputs to the system and the current inputs to the system based on the probability then it can be assumed that the current inputs may come from an adversarial machine learner which
has learned
the inputoutput mapping to model the NN .
Also if it is discovered that the inputs are along a certain decision boundary that the system has then the defending system can become wary of the inputs and suspect them of being from an adversarial machine learning attacker system .
Since the adversarial machine learner can develop a NN or SVM to model the syste given the input-output mappings the adversarial machine learning system can then use knowledge of the decision boundaries in the model to inform creating inputs that will `` trick '' the system that the adversarial machine learning system is attacking .

Generative Adversarial Networks -LRB- GANs -RRB- are a type of adversarial artificial intelligence . The discriminator operates based on a neural network that has decision boundaries and makes decisions based on inputs . The generative network the adversary learns the distribution in order to `` fake out '' the discriminator . Generative Adversarial Networks learn the distribution create synthetic data and can then trick the discriminator system and its decision boundaries . Examples of GANs tricking systems are `` deep fakes '' where an attacker creates an input to trick the decision boundaries of the original system . The analysis of variance such as an F-test ANOVA test or Student t-Tes c n be done . Then the P-value and T-values can be analyzed to make sure that the guessed value is correct .

The seven phases in the Machine Learning Model Kill Chain are the :

* Reconnaissance -LRB- Recon -RRB- Phase

* Weaponization Phase

* Delivery Phase

* Exploitation

* Installation

* Command and Control

* Action


During the Reconnaissance -LRB- Recon -RRB- Phase the Machine Learning -LRB- ML -RRB- models are determined . The ML models are used to protect the defending system from the type of attacks which are to be launched by the attacking system . Then during the Weaponization Phase the results of probes are used in an effort to develop an attack on the defending system by learning the defending system 's decision boundary . During the Decision Phase the defending system 's decision boundary is attacked by the attacking adversarial machine learning system . During the Exploitation Phase the adversarial machine learning system gathers deeper information about the defending system 's underlying model . The attacking system learns how the defending system 's model will be tuned how fast new rules can be formed and how threats are ranked . During the Installation Phase new rules or features that will allow future attacks to happen are set up . During the Command and Control Phase a hidden command and control channel is set up to allow for expansion of the attack . Finally during the Action Phase the attackers act on their main objectiveNyugen 2017 .


feature masks are developed and the system switches between the
feature masks to get roughly the same results . If an attacker knows the coding language that the system is written in -LRB- Python in this case -RRB- the random generator and the seed then the attacker can figure out the
feature masks .

If the number of features is reduced the accuracy will increase . Feature reduction is done
times to develop the
feature masks . The cycle of attack by the Adversarial Author -LRB- Attacking System -RRB- and defense by the Authorship Attribution System -LRB- AAS -RRB- -LRB- Defending System -RRB- is shown in Figure | | SYMBOLTOKEN | |


Acceptable ranges of drops in accuracy by the AAS whilst under attack are less than
percent . The baseline accuracy of the AAS is between
to
percent depending on which algorithm is being used . Therefore an
percent drop in accuracy on
to
percent accuracy would result in
to
percent accuracy . Term Frequency-Inverse Document Frequency -LRB- TFIDF -RRB- is a numerical statistic that is intended to reflect how important a letter is in a character unigram . TFIDF is used as a weighting factor in searches and the value increases proportionally to the number of times a letter appears in the character unigram and is offset by the number of character unigrams in the set that contain the letter .

When the simple -LRB- unweighted -RRB- Euclidean distance is used normalization provides an equal weighting to all features whereas some features would have more importance than others when normalization is not used . Normalization of the training data is done by subtracting the mean and dividing the mean and standard deviation of the training data .

Standardization provides a way to scale the test data with the mean and standard deviation of the training data .

It is important for a defensive system to establish a sense of normalcy for inputs . A measure of normalcy is created by the algorithm knowing the distribution of normal inputs . If the distribution of the inputs has changed a notification is made by the defensive algorithm to alert the system that there is something that has changed and the machine learning algorithm is receiving abnormal inputs . This violates the stationarity assumption since the inputs are not normal and expected inputs . The inputs with abnormal distributions can trick the decision boundary and create bad results from the machine learning algorithm .

In machine learning Na ve Bayes classifiers are a family of simple `` probabilistic classifiers '' based on applying Bayes ' theorem with strong -LRB- na ve -RRB- independence assumptions between the features . They are among the simplest Bayesian network models .

Na ve Bayes classifiers are highly scalable requiring a number of parameters linear in the numb r of variables -LRB- features/predictors -RRB- in a learning problem . Maximum-Likelihood training can be done by evaluating a closed-form expression which takes linear time rather than by expensive iterative approximation as used for many other types of classifiers .

In the statistics and computer science literature Na ve Bayes models are known under a variety of names including simple Bayes and independent Bayes . All these names reference the use of Bayes ' theorem in the classifier 's decision rule but Na ve Bayes is not -LRB- necessarily -RRB- a Bayesian method .

The discussion so far has derived the independent feature model that is the Na ve Bayes probability model . The Na ve Bayes classifier combines this model with a decision rule . On common rule is to pick the hypothesis that is most probable ; this is known as the Maximum A-Posteriori -LRB- MAP -RRB- decision rule .

When dealing with continuous data a typical assumption is that the continuous values associated with each class are distributed according to a normal -LRB- or Gaussian -RRB- distribution . For example suppose the training data contains a continuous attribute
. The data is first segmented by the class and then compute the mean and variance of
in each class .

The algorithms are sorted into equivalence classes based n whether there are statistically significant differences between the algorithms . The ANOVA and Student t-Tests are used to determine statistical significance and to compare the algorithms .

@label
An Adversarial System to attack and an Authorship Attribution System -LRB- AAS -RRB- to defend itself against the attacks are analyzed
@label
Defending a system against attacks from an adversarial machine learner can be done by randomly switching between models for the system by detecting and reacting to changes in the distribution of normal inputs or by using other methods
@label
Adversarial machine learning is used to identify a system that is being used to map system inputs to outputs
@label
A GEFeS is developed to evolve 30 feature masks for the three baseline machine learning techniques
@label
A unigram feature extractor is developed
@label
The improvement of using feature selection over the baselines is explained .
@label
The machine learners that are used to model the system being attacked are a Radial Basis Function Support Vector Machine a Linear Support Vector Machine and a Feedforward Neural Network
@label
The system defends itself against adversarial machine learning attacks by identifying inputs that do not match the probability distribution of normal inputs .
@label
The system also defends itself against adversarial attacks by randomly switching between the feature masks being used to map system inputs to outputs .



